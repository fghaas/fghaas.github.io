<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>xahteiwi.eu - Kubernetes</title><link href="https://xahteiwi.eu/" rel="alternate"/><link href="https://xahteiwi.eu/feeds/tag/kubernetes.atom.xml" rel="self"/><id>https://xahteiwi.eu/</id><updated>2024-12-07T09:00:00+00:00</updated><entry><title>Exploding memory usage in Django/uWSGI containers</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/max-fd/" rel="alternate"/><published>2024-12-07T09:00:00+00:00</published><updated>2024-12-07T09:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2024-12-07:/resources/hints-and-kinks/max-fd/</id><summary type="html">&lt;p&gt;We recently came across an interesting problem at work while migrating from one flavor of Kubernetes to another. It&amp;rsquo;s sufficiently obscure to merit a brief write-up for reference.&lt;/p&gt;</summary><content type="html">&lt;p&gt;When running &lt;a href="https://openedx.org/"&gt;Open edX&lt;/a&gt; on &lt;a href="https://kubernetes.io"&gt;Kubernetes&lt;/a&gt; clusters, one of its &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/"&gt;Pods&lt;/a&gt; is the &lt;code&gt;lms&lt;/code&gt; Pod, which runs the core of the Open edX Learning Management System (&lt;a href="https://github.com/openedx/edx-platform/tree/master/lms"&gt;LMS&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This is a relatively complex &lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; application, which runs in the Pod’s sole container.
Said Django application is being launched with &lt;a href="https://uwsgi-docs.readthedocs.io/"&gt;uWSGI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;At work, we had previously run this platform on Kubernetes clusters managed with &lt;a href="https://docs.openstack.org/magnum/"&gt;OpenStack Magnum&lt;/a&gt;, and were in the process of migrating to &lt;a href="https://gardener.cloud/"&gt;Gardener&lt;/a&gt;.
Apart from the fact that we were upgrading to a newer Kubernetes release, this also meant that the base operating system of our Kubernetes worker nodes changed from &lt;a href="https://fedoraproject.org/coreos/"&gt;Fedora CoreOS&lt;/a&gt; to &lt;a href="https://github.com/gardenlinux/gardenlinux"&gt;Garden Linux&lt;/a&gt; (which is effectively a Kubernetes-optimised Debian).
The virtualisation platform underpinning the Kubernetes cluster remained the same (&lt;a href="https://docs.openstack.org/"&gt;OpenStack&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Mid-migration, we suddenly noticed that our cluster was &lt;a href="https://en.wikipedia.org/wiki/Out_of_memory#Recovery"&gt;oom-killing&lt;/a&gt; our &lt;code&gt;lms&lt;/code&gt; pods.
Now this shouldn’t happen, for the following reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Normally, Kubernetes only kills a Pod for excessive memory usage when a &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/"&gt;memory limit&lt;/a&gt; is set on that Pod, which wasn’t the case.&lt;/li&gt;
&lt;li&gt;Otherwise (that is, with no memory limit set), Pods get killed only by the “regular” kernel oom-killer, and that should only happen when the Pod is grossly misconfigured — that is, its actual memory usage far exceeds its configured &lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits"&gt;memory request&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We quickly found out (via &lt;code&gt;kubectl top pod&lt;/code&gt;) that we were dealing with the latter of these two: our &lt;code&gt;lms&lt;/code&gt; Pod was consuming a whopping 8 GiB of memory when running on the Gardener-managed cluster — nearly 4 times the memory request of 2 GiB.&lt;/p&gt;
&lt;p&gt;This had us scratching our heads, for on the Magnum-managed cluster it was previously running on, that same pod had typically consumed only 80-120 MiB of memory (with occasional spikes).
Thus, we were dealing with baseline memory usage that had suddenly increased by two orders of magnitude.&lt;/p&gt;
&lt;p&gt;Now to explain this memory usage jump, you’ll need this background information:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The &lt;code&gt;corerouter&lt;/code&gt; plugin in uWSGI maintains an array of file descriptor references.&lt;/li&gt;
&lt;li&gt;The size of this array, and with it its memory usage, is a multiple of the value set for uWSGI’s &lt;code&gt;max-fd&lt;/code&gt; configuration option.&lt;sup id="fnref:corerouter"&gt;&lt;a class="footnote-ref" href="#fn:corerouter"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;If &lt;code&gt;max-fd&lt;/code&gt; has not been set in the uWSGI configuration, its default is the maximum number of open file handles allowed for the process per the system-wide configuration.&lt;/li&gt;
&lt;li&gt;Said default can be defined by the &lt;code&gt;nofiles&lt;/code&gt; &lt;a href="https://www.man7.org/linux/man-pages/man5/limits.conf.5.html"&gt;ulimit&lt;/a&gt;, or a &lt;a href="https://wiki.archlinux.org/title/Cgroups"&gt;cgroups&lt;/a&gt; restriction.
   A cgroups restriction is also what systemd uses to implement &lt;a href="https://www.freedesktop.org/software/systemd/man/latest/systemd.exec.html#Process%20Properties"&gt;the &lt;code&gt;LimitNOFILE&lt;/code&gt; option&lt;/a&gt;, which can be set on any systemd unit.&lt;sup id="fnref:man-outdated"&gt;&lt;a class="footnote-ref" href="#fn:man-outdated"&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;If neither the ulimit nor a cgroups restriction is in place, &lt;a href="https://www.kernel.org/doc/Documentation/sysctl/fs.txt"&gt;the &lt;code&gt;fs.nr_open&lt;/code&gt; sysctl&lt;/a&gt;, if set, acts as a backstop. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Prior to release 256, systemd effectively &lt;a href="https://github.com/systemd/systemd/pull/29322"&gt;bumped the default&lt;/a&gt; for &lt;code&gt;LimitNOFILE&lt;/code&gt; from 1048576 (2²⁰) to &lt;code&gt;infinity&lt;/code&gt;, which meant that rather than setting its own cgroups limit, it would rely on &lt;code&gt;fs.nr_open&lt;/code&gt;.
And &lt;em&gt;that&lt;/em&gt; value was recently upped in some distributions to 1073741824 (2³⁰) — an increase by a factor of 2¹⁰ or 1024 over the previously applicable value.&lt;/p&gt;
&lt;p&gt;This change was also applied on Debian (which Garden Linux is based on), and it was even &lt;a href="https://lists.debian.org/debian-devel/2024/06/msg00041.html"&gt;discussed on the Debian mailing list&lt;/a&gt; — where ironically, concerns about raising this limit were pre-emptively quashed with the assertion that file descriptors are such an “extremely cheap resource” that it does not hurt to allow absurdly high numbers of them.&lt;/p&gt;
&lt;p&gt;In the uWSGI case, however, this had the somewhat devastating effect of increasing memory usage to insane levels.&lt;/p&gt;
&lt;p&gt;To their credit, the Garden Linux developers identified this flaw (which, to my knowledge was baked into their version 1592.2), and &lt;a href="https://github.com/gardenlinux/gardenlinux/pull/2442"&gt;fixed it&lt;/a&gt; in version 1592.3.
Still, to insulate ourselves from further such issues, we have opted to &lt;a href="https://uwsgi-docs.readthedocs.io/en/latest/Configuration.html"&gt;reconfigure our systems&lt;/a&gt; to run uWSGI with an explicitly defined &lt;code&gt;max-fd&lt;/code&gt; option, set to the prior system-wide default of 1048576 (although setting it to something as low as 1024 would &lt;em&gt;probably&lt;/em&gt; work too). &lt;/p&gt;
&lt;h3&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://github.com/lotharbach"&gt;Lothar Bach&lt;/a&gt;, &lt;a href="https://github.com/polarathene"&gt;Brennan Kinney&lt;/a&gt;, &lt;a href="https://github.com/pkdevpl"&gt;Piotr Kucułyma&lt;/a&gt;, &lt;a href="https://github.com/NamrataSitlani"&gt;Namrata Sitlani&lt;/a&gt;, and &lt;a href="https://github.com/mrtmm"&gt;Maari Tamm&lt;/a&gt; all contributed to the findings discussed in this article.&lt;sup id="fnref:order"&gt;&lt;a class="footnote-ref" href="#fn:order"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:order"&gt;
&lt;p&gt;I’ve listed these individuals in alphabetical order by surname. &lt;a class="footnote-backref" href="#fnref:order" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:corerouter"&gt;
&lt;p&gt;See &lt;a href="https://github.com/unbit/uwsgi/blob/master/plugins/corerouter/corerouter.c#L705"&gt;the source&lt;/a&gt;, which at the time of writing reads:
  &lt;code&gt;ucr-&amp;gt;cr_table = uwsgi_malloc(sizeof(struct corerouter_session *) * uwsgi.max_fd);&lt;/code&gt; &lt;a class="footnote-backref" href="#fnref:corerouter" title="Jump back to footnote 2 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:man-outdated"&gt;
&lt;p&gt;As far as I can tell, at the time of writing the table captioned “Resource limit directives” in the &lt;code&gt;systemd.exec&lt;/code&gt; man page is outdated and incorrect as far as &lt;code&gt;LimitNOFILE&lt;/code&gt;’s default is concerned, and also the “Don’t use” admonition seems misguided at this point. &lt;a class="footnote-backref" href="#fnref:man-outdated" title="Jump back to footnote 3 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="hints-and-kinks"/><category term="Kubernetes"/><category term="Django"/><category term="Containers"/><category term="systemd"/></entry><entry><title>One For All: Using Terraform to manage OpenStack and Kubernetes resources</title><link href="https://xahteiwi.eu/talk-submissions/oidn-2019-terraform/" rel="alternate"/><published>2019-05-07T00:00:00+00:00</published><updated>2019-05-07T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2019-05-07:/talk-submissions/oidn-2019-terraform/</id><summary type="html">&lt;p&gt;A workshop I submitted to Open Infra Days Nordics 2019.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a workshop I proposed&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; for &lt;a href="https://openinfranordics.com/"&gt;OpenInfra Days
Nordics&lt;/a&gt;, via a non-anonymized CfP
process using &lt;a href="https://www.papercall.io/"&gt;PaperCall&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Title&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;One For All: Using Terraform to manage OpenStack and Kubernetes
resources&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Elevator Pitch&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;You have 300 characters to sell your talk. This is known as the
“elevator pitch”. Make it as exciting and enticing as possible.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A hands-on introduction to Terraform in an OpenStack and Kubernetes
context. Get the basics (of Terraform), then spin up a Kubernetes
cluster in an OpenStack public cloud (with Terraform), and manage
resources on it (with Terraform).&lt;/p&gt;
&lt;h2&gt;Talk Format&lt;/h2&gt;
&lt;p&gt;Workshop (&amp;gt;60 minutes)&lt;/p&gt;
&lt;h2&gt;Audience Level&lt;/h2&gt;
&lt;p&gt;Intermediate&lt;/p&gt;
&lt;h2&gt;Description&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This field supports Markdown. The description will be seen by
reviewers during the CFP process and may eventually be seen by the
attendees of the event.&lt;/p&gt;
&lt;p&gt;You should make the description of your talk as compelling and
exciting as possible. Remember, you’re selling both the organizers
of the events to select your talk, as well as trying to convince
attendees your talk is the one they should see.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If you are interested in deployment automation for arbitrarily complex
containerized microservice applications, this is for you!&lt;/p&gt;
&lt;p&gt;In this workshop, you will&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;get to know the basics of &lt;a href="https://www.terraform.io/"&gt;Terraform&lt;/a&gt; and
  &lt;a href="https://www.terraform.io/docs/configuration/"&gt;Terraform
  configurations&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;spin up a Kubernetes cluster with Terraform, using the &lt;a href="https://www.terraform.io/docs/providers/openstack/"&gt;OpenStack
  provider&lt;/a&gt; and
  interfacing with &lt;a href="https://docs.openstack.org/magnum/latest/user/"&gt;OpenStack
  Magnum&lt;/a&gt; in a public
  cloud,&lt;/li&gt;
&lt;li&gt;start managing Kubernetes resources from Terraform, using the
  &lt;a href="https://www.terraform.io/docs/providers/kubernetes/"&gt;Kubernetes
  provider&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You’ll walk away with a solid understanding of Terraform’s
capabilities, enabling you to make an informed decision of whether
Terraform is a suitable deployment automation facility for your
organization’s needs.&lt;/p&gt;
&lt;p&gt;Prior Terraform knowledge is not required.&lt;/p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;This field supports Markdown. Notes will only be seen by reviewers
during the CFP process. This is where you should explain things such
as technical requirements, why you’re the best person to speak on
this subject, etc…&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are no technical requirements other than internet connectivity,
and a web browser (preferably on a laptop, though a reasonably-sized
tablet with a modern browser should work as well).&lt;/p&gt;
&lt;h2&gt;Tags&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Tag your talk to make it easier for event organizers to be able to
find. Examples are “ruby, javascript, rails”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Terraform, OpenStack, Kubernetes, Magnum&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr/&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;If you’re curious why this is here, please read
&lt;a href="https://xahteiwi.eu/blog/2019/04/23/talk-submissions/"&gt;this&lt;/a&gt;. &lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="talk-submissions"/><category term="Conference"/><category term="Terraform"/><category term="OpenStack"/><category term="Kubernetes"/></entry></feed>