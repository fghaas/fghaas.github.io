<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>xahteiwi.eu - Pacemaker</title><link href="https://xahteiwi.eu/" rel="alternate"></link><link href="https://xahteiwi.eu/feeds/tag/pacemaker.atom.xml" rel="self"></link><id>https://xahteiwi.eu/</id><updated>2016-02-03T00:00:00+00:00</updated><entry><title>Pacemaker's best-kept secret: crm_report</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/pacemakers-best-kept-secret/" rel="alternate"></link><published>2016-01-30T00:00:00+00:00</published><updated>2016-02-03T00:00:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2016-01-30:/resources/hints-and-kinks/pacemakers-best-kept-secret/</id><summary type="html">&lt;p&gt;Pacemaker has an excellent, but little-known, error reporting facility: crm_report.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Whenever things in Pacemaker go wrong (say, for example, resource
failover doesn’t work as expected, or your cluster didn’t properly
recover after a node shutdown), you’ll want to find out just exactly
&lt;em&gt;why&lt;/em&gt; that happened. Of course, the actual reason for the malfunction
may be buried somewhere deep in your cluster configuration or setup,
and so you might need to look at quite a few different sources to pin
it down.&lt;/p&gt;
&lt;p&gt;Sometimes, too, you want to enlist the help of a colleague, &lt;a href="/contact"&gt;or maybe
&lt;strong&gt;our&lt;/strong&gt; help even&lt;/a&gt;, to get to the bottom of the issue. And
sometimes it’s not practical to let someone access to system to just
trigger the problem and watch what breaks.&lt;/p&gt;
&lt;p&gt;Thankfully, Pacemaker ships with a utility that helps you collect
everything you or someone else might need to look at, in a simple,
compact format. Unfortunately few people, including even long-time
Pacemaker users, know that it exists: it’s called &lt;code&gt;crm_report&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Running &lt;code&gt;crm_report&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;crm_report&lt;/code&gt;‘s command syntax is rather quite simple. You just tell it
how far in the past you want the report to start, and which directory
you want to collect data in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;crm_report&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"2016-01-25 00:00:00"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;/tmp/crm_report
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The directory you specify must not exist. If it does, &lt;code&gt;crm_report&lt;/code&gt;
will refuse to run, rather than clobber or mess up your existing
report data.&lt;/p&gt;
&lt;p&gt;By analyzing your logs all the way back to a start date you specify,
&lt;strong&gt;&lt;code&gt;crm_report&lt;/code&gt; makes it unnecessary for you to actually try to
reproduce the problem.&lt;/strong&gt; All you need is a rough idea when the issue
occurred, and then you give &lt;code&gt;crm_report&lt;/code&gt; a timestamp a little earlier
than that as its start date.&lt;/p&gt;
&lt;p&gt;You can also specify the &lt;em&gt;end&lt;/em&gt; of the period you’re interested
in. Suppose you’re exactly aware of a 10-minute time window in which
the problem occurred. In that case, you could run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;crm_report&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"2016-01-25 01:15:00"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;-t&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;"2016-01-25 01:25:00"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;/tmp/crm_report
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Either way, &lt;code&gt;crm_report&lt;/code&gt; will collect relevant log data for the
specified time window on the host it is run on, and then connect to
the other cluster nodes (via &lt;code&gt;ssh&lt;/code&gt;) and do the same there. The latter
behavior can be disabled by adding the &lt;code&gt;-S&lt;/code&gt; or &lt;code&gt;--single-node&lt;/code&gt; option,
but there usually isn’t a good reason to do that. In the end,
everything will be rolled into one tarball at
&lt;code&gt;/tmp/crm_report.tar.bz2&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You can then pull the report tarball off the node (with &lt;code&gt;scp&lt;/code&gt;,
&lt;code&gt;rsync&lt;/code&gt;, whatever you prefer), and then share it with whom you need
to. &lt;strong&gt;Note that the tarball can contain sensitive information such as
passwords, so be careful whom you share it with.&lt;/strong&gt;&lt;/p&gt;
&lt;h2&gt;What’s in a &lt;code&gt;crm_report&lt;/code&gt; tarball?&lt;/h2&gt;
&lt;p&gt;There’s a bunch of truly helpful information in a &lt;code&gt;crm_report&lt;/code&gt;
generated tarball. Depending on how your cluster is configured and
what problems were detected, it will contain, among other things:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Your current Pacemaker Cluster Information Base (CIB),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Your Corosync configuration,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Corosync Blackbox output (if &lt;code&gt;qb-blackbox&lt;/code&gt; is installed on your
  cluster nodes; you can read more about blackbox support
  &lt;a href="http://blog.clusterlabs.org/blog/2013/pacemaker-logging/"&gt;here&lt;/a&gt;),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;drbd.conf&lt;/code&gt; and all your DRBD resource configuration files (if your
  cluster runs DRBD),&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;sysinfo.txt&lt;/code&gt;, a text file including your kernel, distro, Pacemaker
  version, and version information for all your installed packages,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;your Syslog, filtered for the time period you specified in your
  &lt;code&gt;crm_report&lt;/code&gt; command invocation,&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;diffs for critical system information, if &lt;code&gt;crm_report&lt;/code&gt; detected
  discrepancies between nodes.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, it contains pretty much everything that needs to be
shared in a critical troubleshooting situation.&lt;/p&gt;
&lt;h2&gt;Why isn’t this more widely known?&lt;/h2&gt;
&lt;p&gt;To be perfectly honest, we have no idea. &lt;code&gt;crm_report&lt;/code&gt; has been in
Pacemaker for years, and even prior to its existence, there was a
predecessor named &lt;code&gt;hb_report&lt;/code&gt;. It’s an extraordinarily useful utility,
yet when we ask customers to send a &lt;code&gt;crm_report&lt;/code&gt; tarball during a
Pacemaker troubleshooting engagement, the usual response is, “a
what?”&lt;/p&gt;
&lt;p&gt;We hope this post makes &lt;code&gt;crm_report&lt;/code&gt; known to a wider audience, so it
gets the love it deserves. &lt;i class="fa fa-smile-o"&gt;&lt;/i&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Pacemaker"></category></entry><entry><title>Adding MySQL/Galera resources to Pacemaker</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-adding-mysqlgalera-resources-pacemaker/" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-12-04:/resources/hints-and-kinks/mysqlgalera-pacemaker-adding-mysqlgalera-resources-pacemaker/</id><summary type="html">&lt;p&gt;Once you have one instance of Galera running, and it is running on the
same node that holds the temporarily-configured cluster IP
(192.168.122.99 in our example), you can add your resources to the
Pacemaker cluster configuration.&lt;/p&gt;
&lt;p&gt;Create a temporary file, such as &lt;code&gt;/tmp/galera.crm&lt;/code&gt;, with the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Once you have one instance of Galera running, and it is running on the
same node that holds the temporarily-configured cluster IP
(192.168.122.99 in our example), you can add your resources to the
Pacemaker cluster configuration.&lt;/p&gt;
&lt;p&gt;Create a temporary file, such as &lt;code&gt;/tmp/galera.crm&lt;/code&gt;, with the following
contents:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;primitive&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ocf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;heartbeat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;IPaddr2&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;nic&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"eth1"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;iflabel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"galera"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;ip&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"192.168.122.99"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cidr_netmask&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"24"&lt;/span&gt;
&lt;span class="n"&gt;primitive&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ocf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;heartbeat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;config&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"/etc/mysql/my.cnf"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;pid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"/var/run/mysqld/mysqld.pid"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;socket&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"/var/run/mysqld/mysqld.sock"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;binary&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"/usr/sbin/mysqld"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;monitor&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"30s"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"0"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"60s"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;op&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;stop&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"0"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"60s"&lt;/span&gt;
&lt;span class="n"&gt;clone&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cl_mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;meta&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;interleave&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"true"&lt;/span&gt;
&lt;span class="n"&gt;colocation&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;c_ip_galera_on_mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;\
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="n"&gt;inf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cl_mysql&lt;/span&gt;
&lt;span class="n"&gt;property&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;stonith&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;enabled&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"false"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, import this into your Pacemaker configuration:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;crm&lt;span class="w"&gt; &lt;/span&gt;configure&lt;span class="w"&gt; &lt;/span&gt;load&lt;span class="w"&gt; &lt;/span&gt;update&lt;span class="w"&gt; &lt;/span&gt;/tmp/galera.crm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What this creates are a couple of Pacemaker resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The cluster IP address, 192.168.122.99
  (&lt;code&gt;p_ip_mysql_galera&lt;/code&gt;). Throughout the lifetime of the cluster, this
  will always be available on one of the nodes where any MySQL/Galera
  instance is running. This is the IP address new Galera nodes use
  when joining the cluster.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The MySQL server itself (&lt;code&gt;cl_mysql&lt;/code&gt;), which will be automatically
  recovered in-place if it ever fails.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Bootstrapping the Galera cluster</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-bootstrapping-galera-cluster/" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-12-04:/resources/hints-and-kinks/mysqlgalera-pacemaker-bootstrapping-galera-cluster/</id><summary type="html">&lt;p&gt;In order to bootstrap your Galera cluster, manually bring up the
cluster IP address on the desired interface. In this example, we’ll
use 192.168.122.99 and eth1:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ip&lt;span class="w"&gt; &lt;/span&gt;address&lt;span class="w"&gt; &lt;/span&gt;add&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.99/24&lt;span class="w"&gt; &lt;/span&gt;dev&lt;span class="w"&gt; &lt;/span&gt;eth1&lt;span class="w"&gt; &lt;/span&gt;label&lt;span class="w"&gt; &lt;/span&gt;eth1:galera
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And initialize the Galera cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;mysqld&lt;span class="w"&gt; &lt;/span&gt;--wsrep_cluster_address …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;In order to bootstrap your Galera cluster, manually bring up the
cluster IP address on the desired interface. In this example, we’ll
use 192.168.122.99 and eth1:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ip&lt;span class="w"&gt; &lt;/span&gt;address&lt;span class="w"&gt; &lt;/span&gt;add&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.99/24&lt;span class="w"&gt; &lt;/span&gt;dev&lt;span class="w"&gt; &lt;/span&gt;eth1&lt;span class="w"&gt; &lt;/span&gt;label&lt;span class="w"&gt; &lt;/span&gt;eth1:galera
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And initialize the Galera cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;mysqld&lt;span class="w"&gt; &lt;/span&gt;--wsrep_cluster_address&lt;span class="o"&gt;=&lt;/span&gt;gcomm://&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the empty &lt;code&gt;gcomm://&lt;/code&gt; address.&lt;/p&gt;
&lt;p&gt;An avalanche of output is likely to follow. Near the end, you should
see entries similar to these:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Note&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;WSREP&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Synchronized&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;group&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;connections&lt;/span&gt;
&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Note&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;mysqld&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;ready&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;connections&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point, your MySQL/Galera cluster is properly initialized. It
only has one node, and it is not under cluster management yet, but
it’s already a working Galera installation.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Configuring Corosync</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-configuring-corosync/" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-12-04:/resources/hints-and-kinks/mysqlgalera-pacemaker-configuring-corosync/</id><summary type="html">&lt;p&gt;You now need configure Corosync. The following example configuration
file assumes that your cluster nodes have two network interfaces,
using the 192.168.122.0/24 and 192.168.133.0/24 networks. You will
need to adjust this to your own network configuration.&lt;/p&gt;
&lt;p&gt;Set the contents of &lt;code&gt;/etc/corosync …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;You now need configure Corosync. The following example configuration
file assumes that your cluster nodes have two network interfaces,
using the 192.168.122.0/24 and 192.168.133.0/24 networks. You will
need to adjust this to your own network configuration.&lt;/p&gt;
&lt;p&gt;Set the contents of &lt;code&gt;/etc/corosync/corosync.conf&lt;/code&gt; as follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;compatibility&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;whitetank&lt;/span&gt;

&lt;span class="n"&gt;totem&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;version&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;secauth&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;on&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;threads&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;rrp_mode&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;active&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="kd"&gt;interface&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;ringnumber&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;bindnetaddr&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;192.168&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;122.0&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;mcastaddr&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;239.255&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;42.0&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;mcastport&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;5405&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;ttl&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="kd"&gt;interface&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;ringnumber&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;bindnetaddr&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;192.168&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;133.0&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;mcastaddr&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;239.255&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="mf"&gt;42.1&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;mcastport&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;5405&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;ttl&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="n"&gt;logging&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;fileline&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;off&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;to_stderr&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;no&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;to_logfile&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;no&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;to_syslog&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;yes&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;off&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;timestamp&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;on&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="n"&gt;logger_subsys&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;subsys&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;AMF&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;&lt;span class="n"&gt;debug&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;off&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also, create an authkey file for node authentication:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;dd&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/dev/urandom&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/etc/corosync/authkey&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;bs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;128&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
chmod&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0400&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;/etc/corosync/authkey
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And create &lt;code&gt;/etc/corosync/service.d/pacemaker&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;service&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;pacemaker&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="n"&gt;ver&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;1&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Finally, distribute the configuration across your cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;n&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;in&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;bob&lt;span class="w"&gt; &lt;/span&gt;charlie&lt;span class="p"&gt;;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;do&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;rsync&lt;span class="w"&gt; &lt;/span&gt;-av&lt;span class="w"&gt; &lt;/span&gt;/etc/corosync/*&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;$n&lt;/span&gt;:/etc/corosync
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And start Corosync on all cluster nodes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;service&lt;span class="w"&gt; &lt;/span&gt;corosync&lt;span class="w"&gt; &lt;/span&gt;start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once Corosync has started on all nodes, you should be able to check its status with the &lt;code&gt;corosync-cfgtool&lt;/code&gt; and &lt;code&gt;corosync-objctl&lt;/code&gt; commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# corosync-cfgtool -s&lt;/span&gt;
Printing&lt;span class="w"&gt; &lt;/span&gt;ring&lt;span class="w"&gt; &lt;/span&gt;status.
Local&lt;span class="w"&gt; &lt;/span&gt;node&lt;span class="w"&gt; &lt;/span&gt;ID&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1870309568&lt;/span&gt;
RING&lt;span class="w"&gt; &lt;/span&gt;ID&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nv"&gt;id&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.111
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nv"&gt;status&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;ring&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;active&lt;span class="w"&gt; &lt;/span&gt;with&lt;span class="w"&gt; &lt;/span&gt;no&lt;span class="w"&gt; &lt;/span&gt;faults
RING&lt;span class="w"&gt; &lt;/span&gt;ID&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nv"&gt;id&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.133.111
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nv"&gt;status&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;ring&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;active&lt;span class="w"&gt; &lt;/span&gt;with&lt;span class="w"&gt; &lt;/span&gt;no&lt;span class="w"&gt; &lt;/span&gt;faults
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Both rings should be in the &lt;code&gt;active with no faults&lt;/code&gt; state.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# corosync-objctl runtime.totem.pg.mrp.srp.members&lt;/span&gt;
runtime.totem.pg.mrp.srp.1870309568.ip&lt;span class="o"&gt;=&lt;/span&gt;r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.111&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.133.111&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
runtime.totem.pg.mrp.srp.1870309568.join_count&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
runtime.totem.pg.mrp.srp.1870309568.status&lt;span class="o"&gt;=&lt;/span&gt;joined
runtime.totem.pg.mrp.srp.1887086784.ip&lt;span class="o"&gt;=&lt;/span&gt;r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.112&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.133.112&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
runtime.totem.pg.mrp.srp.1887086784.join_count&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
runtime.totem.pg.mrp.srp.1887086784.status&lt;span class="o"&gt;=&lt;/span&gt;joined
runtime.totem.pg.mrp.srp.1903864000.ip&lt;span class="o"&gt;=&lt;/span&gt;r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.113&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;r&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;ip&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.133.113&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
runtime.totem.pg.mrp.srp.1903864000.join_count&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
runtime.totem.pg.mrp.srp.1903864000.status&lt;span class="o"&gt;=&lt;/span&gt;joined
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;All three nodes members should be in the membership with both of their
interfaces, and their status should be &lt;code&gt;joined&lt;/code&gt;.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Dealing with node failure</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-dealing-node-failure/" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-12-04:/resources/hints-and-kinks/mysqlgalera-pacemaker-dealing-node-failure/</id><summary type="html">&lt;p&gt;If an entire node happens to get killed, and that node currently does
not hold the Galera IP (192.168.122.99 in our example), then the other
nodes simply continue to function normally, and you can connect to and
use them without interruption. In the example below, &lt;code&gt;alice&lt;/code&gt; has …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If an entire node happens to get killed, and that node currently does
not hold the Galera IP (192.168.122.99 in our example), then the other
nodes simply continue to function normally, and you can connect to and
use them without interruption. In the example below, &lt;code&gt;alice&lt;/code&gt; has left
the cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;============&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;updated&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;24&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;55&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;change&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;via&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;crmd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;
&lt;span class="nl"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;openais&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DC&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;partition&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;quorum&lt;/span&gt;
&lt;span class="nl"&gt;Version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1.1.7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ee0730e13d124c3d58f00016c3376a1de5323cff&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nodes&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;expected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;votes&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Resources&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="o"&gt;============&lt;/span&gt;

&lt;span class="nl"&gt;Online&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nl"&gt;OFFLINE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alice&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Full&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;resources&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

&lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;&lt;span class="w"&gt;       &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ocf&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;heartbeat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;IPaddr2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt;       &lt;/span&gt;&lt;span class="n"&gt;Started&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Clone&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Set&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cl_mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="nl"&gt;Started&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="nl"&gt;Stopped&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If the node dies that does currently hold the Galera IP
(192.168.122.99 in our example), then the cluster IP shifts to a
different node, and when the failed node returns, it can re-fetch the
cluster state from the node that took over the IP address. In the
example below, in a healthy cluster the IP happens to be running on
&lt;code&gt;bob&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;============&lt;/span&gt;
&lt;span class="k"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;updated&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;
&lt;span class="k"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;change&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;via&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;crmd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;
&lt;span class="nl"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;openais&lt;/span&gt;
&lt;span class="k"&gt;Current&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;DC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;partition&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;quorum&lt;/span&gt;
&lt;span class="nl"&gt;Version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1.1.7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ee0730e13d124c3d58f00016c3376a1de5323cff&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nodes&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;expected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;votes&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Resources&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="o"&gt;============&lt;/span&gt;

&lt;span class="nl"&gt;Online&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt; bob alice charlie &lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;Full&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;resources&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

&lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;&lt;span class="w"&gt;       &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nl"&gt;ocf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;heartbeat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;IPaddr2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;       &lt;/span&gt;&lt;span class="n"&gt;Started&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Clone&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Set&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cl_mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="nl"&gt;Started&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt; alice bob charlie &lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Subsequently, &lt;code&gt;bob&lt;/code&gt; is affected by a failure, and the IP address
shifts to &lt;code&gt;alice&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;============&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;updated&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;33&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;change&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;via&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;crmd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;
&lt;span class="nl"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;openais&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DC&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;partition&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;quorum&lt;/span&gt;
&lt;span class="nl"&gt;Version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1.1.7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ee0730e13d124c3d58f00016c3376a1de5323cff&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nodes&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;expected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;votes&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Resources&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="o"&gt;============&lt;/span&gt;

&lt;span class="nl"&gt;Online&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alice&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nl"&gt;OFFLINE&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Full&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;resources&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

&lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;&lt;span class="w"&gt;       &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ocf&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;heartbeat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;IPaddr2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt;       &lt;/span&gt;&lt;span class="n"&gt;Started&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alice&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Clone&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Set&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cl_mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="nl"&gt;Started&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alice&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="nl"&gt;Stopped&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When &lt;code&gt;bob&lt;/code&gt; returns, it simply connects to &lt;code&gt;alice&lt;/code&gt; (which now hosts the
cluster IP), fetches the database state from there, and continues to
run:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;============&lt;/span&gt;
&lt;span class="k"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;updated&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;46&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;
&lt;span class="k"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;change&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;22&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;23&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;via&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;crmd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;
&lt;span class="nl"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;openais&lt;/span&gt;
&lt;span class="k"&gt;Current&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;DC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;partition&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;quorum&lt;/span&gt;
&lt;span class="nl"&gt;Version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1.1.7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ee0730e13d124c3d58f00016c3376a1de5323cff&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nodes&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;expected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;votes&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Resources&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="o"&gt;============&lt;/span&gt;

&lt;span class="nl"&gt;Online&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt; bob alice charlie &lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;Full&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;resources&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

&lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;&lt;span class="w"&gt;       &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nl"&gt;ocf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;heartbeat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;IPaddr2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;       &lt;/span&gt;&lt;span class="n"&gt;Started&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alice&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Clone&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Set&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cl_mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="nl"&gt;Started&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt; alice bob charlie &lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>MySQL/Galera in Pacemaker High Availability Clusters</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-high-availability-clusters/" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-12-04:/resources/hints-and-kinks/mysqlgalera-pacemaker-high-availability-clusters/</id><summary type="html">&lt;p&gt;In this walkthrough, you will create a Pacemaker managed MySQL/Galera
cluster. It assumes that you are running on a Debian 6.0 (squeeze)
box, but the concepts should be equally applicable to other platforms
with minimal modifications.&lt;/p&gt;
&lt;p&gt;It also assumes that your Galera cluster will consist of three nodes …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this walkthrough, you will create a Pacemaker managed MySQL/Galera
cluster. It assumes that you are running on a Debian 6.0 (squeeze)
box, but the concepts should be equally applicable to other platforms
with minimal modifications.&lt;/p&gt;
&lt;p&gt;It also assumes that your Galera cluster will consist of three nodes,
named alice, bob and charlie. Furthermore, all cluster nodes can
resolve each other’s hostnames.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Please note: All commands in this walkthrough require that you are
logged into your system as root.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, make sure you have the required packages installed. One of the
easiest ways to get your hands on MySQL/Galera binaries is to install
Percona XtraDB Cluster, which our friends at Percona make available in
their public software repository.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;/etc/apt/sources.list.d/percona.list&lt;/code&gt; with the following
content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;deb&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;http://repo.percona.com/apt&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kp"&gt;squeeze&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kp"&gt;main&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Fetch the Percona repository signing key:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;apt-key&lt;span class="w"&gt; &lt;/span&gt;adv&lt;span class="w"&gt; &lt;/span&gt;--keyserver&lt;span class="w"&gt; &lt;/span&gt;hkp://keys.gnupg.net&lt;span class="w"&gt; &lt;/span&gt;--recv-keys&lt;span class="w"&gt; &lt;/span&gt;1C4CBDCDCD2EFD2A
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You also require Pacemaker packages from the Debian backports
repository. Do do so, create &lt;code&gt;/etc/apt/sources.list.d/backports.list&lt;/code&gt;
with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;deb&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;http://backports.debian.org/debian-backports&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kp"&gt;squeeze-backports&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kp"&gt;main&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, update your package lists:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;apt-get&lt;span class="w"&gt; &lt;/span&gt;update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once that is completed, you are able to install the
&lt;code&gt;percona-xtradb-cluster-server-5.5&lt;/code&gt; package:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;apt-get -y install percona-xtradb-cluster-server-5.5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note that &lt;code&gt;percona-xtradb-cluster-server-5.5&lt;/code&gt; conflicts with the
standard Debian &lt;code&gt;mysql-server&lt;/code&gt; packages, so if you have any of those
installed, they will be removed in the process of installing XtraDB
Cluster.&lt;/p&gt;
&lt;p&gt;Stop the MySQL server services for the time being:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;service&lt;span class="w"&gt; &lt;/span&gt;mysql&lt;span class="w"&gt; &lt;/span&gt;stop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also required is the pacemaker package (and its dependencies) from
squeeze-backports:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;apt-get&lt;span class="w"&gt; &lt;/span&gt;-t&lt;span class="w"&gt; &lt;/span&gt;squeeze-backports&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;pacemaker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And finally rsync is required for one of the supported Snapshot State
Transfer (SST) methods for Galera:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;apt-get&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;rsync
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, all required packages are installed and you’re ready to configure
XtraDB Cluster.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Recovering from full cluster shutdown</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-recovering-full-cluster-shutdown/" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-12-04:/resources/hints-and-kinks/mysqlgalera-pacemaker-recovering-full-cluster-shutdown/</id><summary type="html">&lt;p&gt;If at any time &lt;em&gt;all&lt;/em&gt; of the nodes in your cluster have been taken
down, it is necessary to re-initialize the Galera replication
state. In effect, this is identical to bootstrapping the cluster.&lt;/p&gt;
&lt;p&gt;Start by manually bringing up the cluster IP on one of your nodes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ip&lt;span class="w"&gt; &lt;/span&gt;address&lt;span class="w"&gt; &lt;/span&gt;add&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;192 …&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;If at any time &lt;em&gt;all&lt;/em&gt; of the nodes in your cluster have been taken
down, it is necessary to re-initialize the Galera replication
state. In effect, this is identical to bootstrapping the cluster.&lt;/p&gt;
&lt;p&gt;Start by manually bringing up the cluster IP on one of your nodes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;ip&lt;span class="w"&gt; &lt;/span&gt;address&lt;span class="w"&gt; &lt;/span&gt;add&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;192&lt;/span&gt;.168.122.99/24&lt;span class="w"&gt; &lt;/span&gt;dev&lt;span class="w"&gt; &lt;/span&gt;eth1&lt;span class="w"&gt; &lt;/span&gt;label&lt;span class="w"&gt; &lt;/span&gt;eth1:galera
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Re-initialize the Galera cluster:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;mysqld&lt;span class="w"&gt; &lt;/span&gt;--wsrep_cluster_address&lt;span class="o"&gt;=&lt;/span&gt;gcomm://&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;&amp;amp;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Note the empty &lt;code&gt;gcomm://&lt;/code&gt; address.&lt;/p&gt;
&lt;p&gt;Finally, clear your resource state with &lt;code&gt;crm resource cleanup
cl_mysql&lt;/code&gt;. Pacemaker will leave the running IP address and MySQL
instance untouched, and bring up the additional MySQL instances.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Setting Galera-specific MySQL options</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-setting-galera-specific-mysql-options/" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-12-04:/resources/hints-and-kinks/mysqlgalera-pacemaker-setting-galera-specific-mysql-options/</id><summary type="html">&lt;p&gt;Now you can proceed with setting Galera specifics in your MySQL
configurations.&lt;/p&gt;
&lt;p&gt;Create a configuration file, &lt;strong&gt;identical on all cluster nodes,&lt;/strong&gt; named
&lt;code&gt;/etc/mysql/conf.d/galera.cnf&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[mysqld]&lt;/span&gt;
&lt;span class="na"&gt;bind_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;0.0.0.0&lt;/span&gt;
&lt;span class="na"&gt;binlog_format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;ROW&lt;/span&gt;
&lt;span class="na"&gt;default_storage_engine&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;InnoDB&lt;/span&gt;
&lt;span class="na"&gt;innodb_autoinc_lock_mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;2&lt;/span&gt;
&lt;span class="na"&gt;innodb_locks_unsafe_for_binlog&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Create another configuration file …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Now you can proceed with setting Galera specifics in your MySQL
configurations.&lt;/p&gt;
&lt;p&gt;Create a configuration file, &lt;strong&gt;identical on all cluster nodes,&lt;/strong&gt; named
&lt;code&gt;/etc/mysql/conf.d/galera.cnf&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[mysqld]&lt;/span&gt;
&lt;span class="na"&gt;bind_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;0.0.0.0&lt;/span&gt;
&lt;span class="na"&gt;binlog_format&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;ROW&lt;/span&gt;
&lt;span class="na"&gt;default_storage_engine&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;InnoDB&lt;/span&gt;
&lt;span class="na"&gt;innodb_autoinc_lock_mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;2&lt;/span&gt;
&lt;span class="na"&gt;innodb_locks_unsafe_for_binlog&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Create another configuration file, &lt;strong&gt;specific to each cluster node,&lt;/strong&gt;
named &lt;code&gt;/etc/mysql/conf.d/wsrep.cnf&lt;/code&gt; with the following content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[mysqld]&lt;/span&gt;
&lt;span class="c1"&gt;# node alice has address 192.168.122.111&lt;/span&gt;
&lt;span class="na"&gt;wsrep_node_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;192.168.122.111&lt;/span&gt;
&lt;span class="na"&gt;wsrep_provider&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/usr/lib/libgalera_smm.so&lt;/span&gt;
&lt;span class="na"&gt;wsrep_slave_threads&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;8&lt;/span&gt;
&lt;span class="na"&gt;wsrep_sst_method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;rsync&lt;/span&gt;
&lt;span class="na"&gt;wsrep_cluster_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;gcomm://192.168.122.99&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[mysqld]&lt;/span&gt;
&lt;span class="c1"&gt;# node bob has address 192.168.122.112&lt;/span&gt;
&lt;span class="na"&gt;wsrep_node_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;192.168.122.112&lt;/span&gt;
&lt;span class="na"&gt;wsrep_provider&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/usr/lib/libgalera_smm.so&lt;/span&gt;
&lt;span class="na"&gt;wsrep_slave_threads&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;8&lt;/span&gt;
&lt;span class="na"&gt;wsrep_sst_method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;rsync&lt;/span&gt;
&lt;span class="na"&gt;wsrep_cluster_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;gcomm://192.168.122.99&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;[mysqld]&lt;/span&gt;
&lt;span class="c1"&gt;# node charlie has address 192.168.122.111&lt;/span&gt;
&lt;span class="na"&gt;wsrep_node_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;192.168.122.113&lt;/span&gt;
&lt;span class="na"&gt;wsrep_provider&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;/usr/lib/libgalera_smm.so&lt;/span&gt;
&lt;span class="na"&gt;wsrep_slave_threads&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;8&lt;/span&gt;
&lt;span class="na"&gt;wsrep_sst_method&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;rsync&lt;/span&gt;
&lt;span class="na"&gt;wsrep_cluster_address&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s"&gt;gcomm://192.168.122.99&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can now proceed with bootstrapping your cluster.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Starting Pacemaker</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-starting-pacemaker/" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-12-04:/resources/hints-and-kinks/mysqlgalera-pacemaker-starting-pacemaker/</id><summary type="html">&lt;p&gt;Once Corosync is running, you are able to start the Pacemaker cluster
resource manager on all cluster nodes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;service&lt;span class="w"&gt; &lt;/span&gt;pacemaker&lt;span class="w"&gt; &lt;/span&gt;start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once cluster startup is completed, you should see output similar to
the following when invoking the &lt;code&gt;crm_mon&lt;/code&gt; utility:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;============
Last updated: Mon Dec  3 15:37:59 2012
Last change …&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Once Corosync is running, you are able to start the Pacemaker cluster
resource manager on all cluster nodes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;service&lt;span class="w"&gt; &lt;/span&gt;pacemaker&lt;span class="w"&gt; &lt;/span&gt;start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once cluster startup is completed, you should see output similar to
the following when invoking the &lt;code&gt;crm_mon&lt;/code&gt; utility:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;============
Last updated: Mon Dec  3 15:37:59 2012
Last change: Mon Dec  3 15:37:58 2012 via crmd on alice
Stack: openais
Current DC: alice - partition with quorum
Version: 1.1.7-ee0730e13d124c3d58f00016c3376a1de5323cff
3 Nodes configured, 3 expected votes
0 Resources configured.
============

Online: [ bob alice charlie ]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Testing resource recovery</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/mysqlgalera-pacemaker-testing-resource-recovery/" rel="alternate"></link><published>2012-12-04T10:53:27+01:00</published><updated>2012-12-04T10:53:27+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-12-04:/resources/hints-and-kinks/mysqlgalera-pacemaker-testing-resource-recovery/</id><summary type="html">&lt;p&gt;If MySQL happens to die in your cluster, Pacemaker will automatically
recover the service in place. To test this, select any node on your
cluster and send the &lt;code&gt;mysqld&lt;/code&gt; process a &lt;code&gt;KILL&lt;/code&gt; signal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;killall&lt;span class="w"&gt; &lt;/span&gt;-KILL&lt;span class="w"&gt; &lt;/span&gt;mysqld
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, monitor your cluster status with &lt;code&gt;crm_mon -rf&lt;/code&gt;. After a few
seconds, you should …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If MySQL happens to die in your cluster, Pacemaker will automatically
recover the service in place. To test this, select any node on your
cluster and send the &lt;code&gt;mysqld&lt;/code&gt; process a &lt;code&gt;KILL&lt;/code&gt; signal:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;killall&lt;span class="w"&gt; &lt;/span&gt;-KILL&lt;span class="w"&gt; &lt;/span&gt;mysqld
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, monitor your cluster status with &lt;code&gt;crm_mon -rf&lt;/code&gt;. After a few
seconds, you should see one of your &lt;code&gt;p_mysql&lt;/code&gt; clones entering the
&lt;code&gt;FAILED&lt;/code&gt; state:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;============&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;updated&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;03&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;
&lt;span class="n"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;change&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;54&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;via&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;crmd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;
&lt;span class="nl"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;openais&lt;/span&gt;
&lt;span class="n"&gt;Current&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;DC&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;partition&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;quorum&lt;/span&gt;
&lt;span class="nl"&gt;Version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1.1.7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ee0730e13d124c3d58f00016c3376a1de5323cff&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nodes&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;expected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;votes&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Resources&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="o"&gt;============&lt;/span&gt;

&lt;span class="nl"&gt;Online&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alice&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Full&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;resources&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ocf&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;heartbeat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;IPaddr2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;Started&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alice&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Clone&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Set&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cl_mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="nl"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ocf&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;heartbeat&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="n"&gt;mysql&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Started&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;FAILED&lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="nl"&gt;Started&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alice&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Migration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;summary&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Node&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alice&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Node&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Node&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;

&lt;span class="n"&gt;Failed&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;actions&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nl"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_monitor_30000&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;complete&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;not&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;running&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, after a few seconds, the resource will automatically recover:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;============&lt;/span&gt;
&lt;span class="k"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;updated&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;19&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;03&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;
&lt;span class="k"&gt;Last&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;change&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Mon&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Dec&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;54&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;44&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;2012&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;via&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;crmd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;on&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;
&lt;span class="nl"&gt;Stack&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;openais&lt;/span&gt;
&lt;span class="k"&gt;Current&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;DC&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;charlie&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;partition&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;quorum&lt;/span&gt;
&lt;span class="nl"&gt;Version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mf"&gt;1.1.7&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;ee0730e13d124c3d58f00016c3376a1de5323cff&lt;/span&gt;
&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Nodes&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;expected&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;votes&lt;/span&gt;
&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Resources&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;configured&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;
&lt;span class="o"&gt;============&lt;/span&gt;

&lt;span class="nl"&gt;Online&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt; bob alice charlie &lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;Full&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;resources&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;p_ip_mysql_galera&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nl"&gt;ocf&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="nl"&gt;heartbeat&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;IPaddr2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="n"&gt;Started&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;alice&lt;/span&gt;
&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Clone&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;Set&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;cl_mysql&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p_mysql&lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;     &lt;/span&gt;&lt;span class="nl"&gt;Started&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="n"&gt; alice bob charlie &lt;/span&gt;&lt;span class="o"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Migration&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Node&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;alice&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Node&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;bob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;
&lt;span class="w"&gt;   &lt;/span&gt;&lt;span class="nl"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;migration&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000000&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;fail&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nf"&gt;count&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;Node&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;charlie&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;

&lt;span class="n"&gt;Failed&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nl"&gt;actions&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nl"&gt;p_mysql&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="n"&gt;_monitor_30000&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;node&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;call&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;rc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;status&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;complete&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ow"&gt;not&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;running&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To subsequently get rid of the entry in the &lt;code&gt;Failed actions&lt;/code&gt; list, use
&lt;code&gt;crm resource cleanup cl_mysql&lt;/code&gt;.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Galera"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>MySQL High Availability Deep Dive</title><link href="https://xahteiwi.eu/resources/presentations/mysql-high-availability-deep-dive/" rel="alternate"></link><published>2012-12-03T13:50:00+00:00</published><updated>2012-12-03T13:50:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-12-03:/resources/presentations/mysql-high-availability-deep-dive/</id><content type="html">&lt;p&gt;This is a tutorial that Yves Trudeau and I presented at the
Percona Live UK 2012 conference in London. It covers Pacemaker
integration with DRBD, MySQL Replication, and Galera.&lt;/p&gt;
&lt;!--break--&gt;
&lt;ul&gt;
&lt;li&gt;Slides: &lt;a href="https://docs.google.com/presentation/d/12CzmvBOUpbIOrS2CGbG4PU6v74g99C0gGlh0Zf0Qh7g/embed"&gt;Google Slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="presentations"></category><category term="Conference"></category><category term="MySQL"></category><category term="Pacemaker"></category><category term="Galera"></category></entry><entry><title>Pacemaker and the recent GitHub service interruption</title><link href="https://xahteiwi.eu/blog/2012/09/26/pacemaker-and-the-recent-github-service-interruption/" rel="alternate"></link><published>2012-09-26T11:32:00+00:00</published><updated>2012-09-26T11:32:00+00:00</updated><author><name>florian</name></author><id>tag:xahteiwi.eu,2012-09-26:/blog/2012/09/26/pacemaker-and-the-recent-github-service-interruption/</id><summary type="html">&lt;p&gt;It never fails. Someone manages to break their Pacemaker cluster, and
&lt;a href="http://openlife.cc/author"&gt;Henrik&lt;/a&gt; starts preaching &lt;a href="http://openlife.cc/blogs/2012/september/failover-evil"&gt;his usual sermon
of why Pacemaker is
terrible&lt;/a&gt; and why
you should never-ever use it. And when that someone is
&lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt;, which we all know, use and love, then
that sermon gets a bit of excess …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It never fails. Someone manages to break their Pacemaker cluster, and
&lt;a href="http://openlife.cc/author"&gt;Henrik&lt;/a&gt; starts preaching &lt;a href="http://openlife.cc/blogs/2012/september/failover-evil"&gt;his usual sermon
of why Pacemaker is
terrible&lt;/a&gt; and why
you should never-ever use it. And when that someone is
&lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt;, which we all know, use and love, then
that sermon gets a bit of excess attention. Let’s take a quick look at
the facts.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;The week of September 10, GitHub suffered a couple of outages which
caused a total downtime of 1 hour and 46 minutes, as
&lt;a href="https://github.com/jnewland"&gt;Jesse&lt;/a&gt; precisely pointed out &lt;a href="https://github.com/blog/1261-github-availability-this-week"&gt;in a blog
post&lt;/a&gt;.
Exhibiting the excellent transparency that GitHub always offers at any
time its infrastructure is affected by issues (remember &lt;a href="https://github.com/blog/1068-public-key-security-vulnerability-and-mitigation"&gt;their
role-model behavior in an SSH security
incident&lt;/a&gt;
a few months back), Jesse explains, in a very detailed way, what
happened on one of their Pacemaker clusters.&lt;/p&gt;
&lt;p&gt;Now, all of what follows is based exclusively on the information in that
blog post of Jesse’s. I have no inside knowledge of the incident, so my
picture may be incomplete or skewed. But here’s my take on it anyway. I
do encourage you to read Jesse’s post full-length, as the rest of this
post otherwise won’t make much sense. I’ll just quote certain pieces of
it and comment on them here.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Please note:&lt;/strong&gt; nothing in this post should be construed as a put-down
of GitHub’s excellent staff. They run a fantastic service and do an
awesome job. It’s just that their post-mortem seems to have created some
misconceptions in the MySQL community about the Pacemaker stack as a
whole, and those I’d like to help rectify. Also, I’m posting this in the
hope that it provides useful insight to both the GitHub folks, and to
anyone else facing similar issues.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Enable Maintenance Mode when you should&lt;/h2&gt;
&lt;p&gt;From the original post:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Monday’s migration caused higher load on the database than our
operations team has previously seen during these sorts of migrations.
So high, in fact, that they caused Percona Replication Manager’s
health checks to fail on the master. In response to the failed master
health check, Percona Replication manager moved the ‘active’ role and
the master database to another server in the cluster and stopped MySQL
on the node it perceived as failed.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
At the time of this failover, the new database selected for the
‘active’ role had a cold InnoDB buffer pool and performed rather
poorly. The system load generated by the site’s query load on a cold
cache soon caused Percona Replication Manager’s health checks to fail
again, and the ‘active’ role failed back to the server it was on
originally.
&lt;p&gt;&lt;/p&gt;
At this point, I decided to disable all health checks by enabling
Pacemaker’s &lt;code&gt;maintenance-mode&lt;/code&gt;; an operating mode in which no health
checks or automatic failover actions are performed. Performance on the
site slowly recovered as the buffer pool slowly reached normal levels.
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now there’s actually several issues in there even in this early stage.
Maintenance mode is generally the right thing to do here, but you enable
it &lt;em&gt;before&lt;/em&gt; making large changes to the configuration, and you disable
it when done. If you’re uncomfortable with the cluster manager taking
its hands off the entire cluster, and you know what you’re doing, you
could also just disable cluster management and monitoring on a specific
resource. Both approaches are explained
&lt;a href="https://www.hastexo.com/resources/hints-and-kinks/maintenance-active-pacemaker-clusters"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Also, as far as “health checks failing” on the master is concerned,
pretty much the only thing that is likely to cause such a failure in
this instance is a timeout, and you can adjust those even on a
per-operation basis in Pacemaker. But even that is unnecessary if you
enable maintenance mode at the right time.&lt;/p&gt;
&lt;h2&gt;“Maintenance mode” really means maintenance mode&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The following morning, our operations team was notified by a developer
of incorrect query results returning from the node providing the
‘standby’ role. I investigated the situation and determined that when
the cluster was placed into maintenance-mode the day before, actions
that should have caused the node elected to serve the ‘standby’ role
to change its replication master and start replicating were prevented
from occurring.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well, of course. In maintenance mode, Pacemaker takes its hands off your
resources. If you’re enabling maintenance mode right in the middle of a
failover, then that’s not exactly a stellar idea. If you do, then it’s
your job to complete those actions manually.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I determined that the best course of action was to
disable &lt;code&gt;maintenance-mode&lt;/code&gt; to allow Pacemaker and the Percona
Replication Manager to rectify the situation.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;“Best” might be an exaggeration, if I may say so.&lt;/p&gt;
&lt;h2&gt;A segfault and rejected cluster messages&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Upon attempting to disable &lt;code&gt;maintenance-mode&lt;/code&gt;, a Pacemaker segfault
occurred that resulted in a cluster state partition.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;OK, that’s bad, but what exactly segfaulted? crmd? attrd? pengine? Or
the master Heartbeat process? But the next piece of information would
have me believe that the segfault really isn’t the root cause of the
cluster partition:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;After this update, two nodes (I’ll call them ‘a’ and ‘b’) rejected
most messages from the third node (‘c’), while the third node rejected
most messages from the other two.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now it’s a pity that we don’t have any version information and logs, but
this looks very much like the “not in our membership” issue present up
to Pacemaker 1.1.6. This is a known issue, the fix is to update to a
more recent version (&lt;a href="https://github.com/ClusterLabs/pacemaker/commit/03f6105592281901cc10550b8ad19af4beb5f72f"&gt;here’s the
commit,&lt;/a&gt; on
GitHub of course), and the workaround is to just restart the Pacemaker
services on the affected node(s) while in maintenance mode.&lt;/p&gt;
&lt;h2&gt;A non-quorate partition running MySQL?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Despite having configured the cluster to require a majority of
machines to agree on the state of the cluster before taking action,
two simultaneous master election decisions were attempted without
proper coordination. In the first cluster, master election was
interrupted by messages from the second cluster and MySQL was stopped.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now this is an example of me being tempted to say, “logs or it didn’t
happen.” If you’ve got the default no-quorum-policy of “block”, and
you’re getting a non-quorate partition, and you don’t have any resources
with operations &lt;em&gt;explicitly&lt;/em&gt; configured to ignore quorum, then “two
simultaneous master election decisions” can only refer to the Designated
Coordinator (DC) election, which has no bearing whatsoever on MySQL
master status. Luckily, Pacemaker allows us to take a meaningful
snapshot of all cluster logs and status after the fact with crm_report.
It would be quite interesting to see a tarball from that.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In the second, single-node cluster, node ‘c’ was elected at 8:19 AM,
and any subsequent messages from the other two-node cluster were
discarded. As luck would have it, the ‘c’ node was the node that our
operations team previously determined to be out of date. We detected
this fact and powered off this out-of-date node at 8:26 AM to end the
partition and prevent further data drift, taking down all production
database access and thus all access to github.com.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That’s obviously a bummer, but really, if that partition is non-quorate,
and Pacemaker hasn’t explicitly been configured to ignore that, no
cluster resources would start there. Needless to say a working fencing
configuration would have helped oodles, too.&lt;/p&gt;
&lt;h2&gt;Your cluster has no crystal ball, but it does have a command line&lt;/h2&gt;
&lt;p&gt;I’ll skip over most of the rest of the GitHub post, because it’s an
explanation of how these backend issues affected GitHub users. I’ll just
hop on down to this piece:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The automated failover of our main production database could be
described as the root cause of both of these downtime events. In each
situation in which that occurred, if any member of our operations team
had been asked if the failover should have been performed, the answer
would have been a resounding no.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Well, you could have told your Pacemaker of that fact beforehand. Enable
maintenance mode and you’re good to go.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;There are many situations in which automated failover is an excellent
strategy for ensuring the availability of a service. After careful
consideration, we’ve determined that ensuring the availability of our
primary production database is not one of these situations. To this
end, we’ve made changes to our Pacemaker configuration to ensure
failover of the ‘active’ database role will only occur when initiated
by a member of our operations team.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That splash you just heard was the bath water. The scream was the baby
being tossed out with it.&lt;/p&gt;
&lt;p&gt;Automated failover is a pretty poor strategy &lt;em&gt;in the middle of a large
configuration change.&lt;/em&gt; And Pacemaker gives you a simple and easy
interface to disable it, by changing a single cluster property. Failure
to do so may result in problems, and in this case it did.&lt;/p&gt;
&lt;p&gt;When you put a baby seat on the passenger side of your car, you disable
the air bag to prevent major injury. But if you take that baby seat out
and an adult passenger rides with you, are you seriously saying you’re
going to manually initiate the air bag in case of a crash? I hope you’re
not.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Finally, our operations team is performing a full audit of our
Pacemaker and Heartbeat stack focusing on the code path that triggered
the segfault on Tuesday.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That’s probably a really good idea. For anyone planning to do the same,
&lt;a href="https://www.hastexo.com/services/checkup"&gt;we can help.&lt;/a&gt;&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on my blog on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;
&lt;!--break--&gt;</content><category term="blog"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry><entry><title>Maintenance in active Pacemaker clusters</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/maintenance-active-pacemaker-clusters/" rel="alternate"></link><published>2012-09-24T19:49:31+01:00</published><updated>2012-09-24T19:49:31+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-09-24:/resources/hints-and-kinks/maintenance-active-pacemaker-clusters/</id><summary type="html">&lt;p&gt;In a Pacemaker cluster, as in a standalone system, operators must
complete maintenance tasks such as software upgrades and configuration
changes. Here’s what you need to keep Pacemaker’s built-in monitoring
features from creating unwanted side effects.&lt;/p&gt;
&lt;h2&gt;Maintenance mode&lt;/h2&gt;
&lt;p&gt;This is quite possibly Pacemaker’s single most useful feature …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In a Pacemaker cluster, as in a standalone system, operators must
complete maintenance tasks such as software upgrades and configuration
changes. Here’s what you need to keep Pacemaker’s built-in monitoring
features from creating unwanted side effects.&lt;/p&gt;
&lt;h2&gt;Maintenance mode&lt;/h2&gt;
&lt;p&gt;This is quite possibly Pacemaker’s single most useful feature for
cluster maintenance. In maintenance mode, Pacemaker essentially takes
a “hands-off” approach to your cluster. Enabling Pacemaker maintenance
mode is very easy using the Pacemaker &lt;code&gt;crm&lt;/code&gt; shell:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;crm&lt;span class="w"&gt; &lt;/span&gt;configure&lt;span class="w"&gt; &lt;/span&gt;property&lt;span class="w"&gt; &lt;/span&gt;maintenance-mode&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;In maintenance mode, you can stop or restart cluster resources at
will. Pacemaker will not attempt to restart them. All resources
automatically become unmanaged, that is, Pacemaker will cease
monitoring them and hence be oblivious about their status. You can
even stop all Pacemaker services on a node, and all the daemons and
processes originally started as Pacemaker managed cluster resources
will continue to run.&lt;/p&gt;
&lt;p&gt;You should know that when you start Pacemaker services on a node while
the cluster in maintenance mode, Pacemaker will initiate a single
one-shot monitor operation (a “probe”) for every resource just so it
has an understanding of what resources are currently running on that
node. It will, however, take no further action other than determining
the resources’ status.&lt;/p&gt;
&lt;p&gt;You disable maintenance mode with the crm shell, as well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;crm&lt;span class="w"&gt; &lt;/span&gt;configure&lt;span class="w"&gt; &lt;/span&gt;property&lt;span class="w"&gt; &lt;/span&gt;maintenance-mode&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Maintenance mode is something you enable before running other
maintenance actions, not when you’re already half-way through
them. And unless you’re very well versed in the interdependencies of
resources running on the cluster you’re working on, it’s usually the
very safest option.&lt;/p&gt;
&lt;p&gt;In short: when doing maintenance on your Pacemaker cluster, by
default, enable maintenance mode before you start, and disable it
after you’re done.&lt;/p&gt;
&lt;h2&gt;Disabling monitoring and error recovery on specific resources&lt;/h2&gt;
&lt;p&gt;For any configuration changes that take no more than a few minutes,
involving an admin that is potentially watching a console window the
whole time, maintenance mode is highly recommended. However, enabling
maintenance mode can be a bit hard to argue for large configuration
changes lasting, say, several hours. Think of a massive database
rebuild, for example. In such a case, you may want to put only your
database resource in something like maintenance mode, and have
Pacemaker continue to monitor other resources like normal.&lt;/p&gt;
&lt;p&gt;You can do so by switching the resource to unmanaged mode and disable
its monitor operation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;crm&lt;span class="w"&gt; &lt;/span&gt;configure&lt;span class="w"&gt; &lt;/span&gt;edit&lt;span class="w"&gt; &lt;/span&gt;p_database
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then change the &lt;code&gt;is-managed&lt;/code&gt; meta  attribute and disable the &lt;code&gt;monitor&lt;/code&gt;
operation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;meta is-managed=false
op monitor interval=&amp;lt;interval&amp;gt; enabled=false
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once you’ve done that, you’ll effectively have enabled something akin
to maintenance mode for a single resource. You can reverse this as you
would expect:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;crm&lt;span class="w"&gt; &lt;/span&gt;configure&lt;span class="w"&gt; &lt;/span&gt;edit&lt;span class="w"&gt; &lt;/span&gt;p_database
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then change the &lt;code&gt;is-managed&lt;/code&gt; meta attribute and re-enable the
&lt;code&gt;monitor&lt;/code&gt; operation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;meta is-managed=true
op monitor interval=&amp;lt;interval&amp;gt; enabled=true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When using this approach, all other resources will be monitored and
automatically recovered as they normally would. Thus, you’ll have to
be acutely aware of any side effects your maintenance activities have
on other resources. If you’re unsure, you should use the global
maintenance mode instead.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Pacemaker"></category></entry><entry><title>Highly Available Cloud: Pacemaker integration with OpenStack</title><link href="https://xahteiwi.eu/resources/presentations/highly-available-cloud-pacemaker-integration-openstack/" rel="alternate"></link><published>2012-07-17T21:12:00+00:00</published><updated>2012-07-17T21:12:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-07-17:/resources/presentations/highly-available-cloud-pacemaker-integration-openstack/</id><summary type="html">&lt;p&gt;This presentation was delivered July 17, 2012 at OSCON in
Portland, Oregon.&lt;/p&gt;
&lt;p&gt;I summarize high availability in OpenStack Folsom, particularly
OpenStack integration with the Pacemaker high availability cluster
stack.&lt;/p&gt;
&lt;p&gt;I talk about high availability shortcomings in OpenStack Essex,
comparing OpenStack to some of its important competitors. I then
explain how …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This presentation was delivered July 17, 2012 at OSCON in
Portland, Oregon.&lt;/p&gt;
&lt;p&gt;I summarize high availability in OpenStack Folsom, particularly
OpenStack integration with the Pacemaker high availability cluster
stack.&lt;/p&gt;
&lt;p&gt;I talk about high availability shortcomings in OpenStack Essex,
comparing OpenStack to some of its important competitors. I then
explain how these shortcomings are being addressed in Folsom, and give
an overview of the current progress in view of current OpenStack Folsom
development.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slides: &lt;a href='https://prezi.com/embed/p4jstawrfqwh/"&amp;gt;&amp;lt;/'&gt;Prezi&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="presentations"></category><category term="Conference"></category><category term="OpenStack"></category><category term="Pacemaker"></category></entry><entry><title>Fencing in VMware virtualized Pacemaker nodes</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/fencing-vmware-virtualized-pacemaker-nodes/" rel="alternate"></link><published>2012-05-18T09:43:28+01:00</published><updated>2012-05-18T09:43:28+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-05-18:/resources/hints-and-kinks/fencing-vmware-virtualized-pacemaker-nodes/</id><summary type="html">&lt;p&gt;For users of VMware virtualization, it’s becoming increasingly common
to deploy Pacemaker clusters within the virtual infrastructure. Doing
this requires that you set up fencing via ESX Server or, more
commonly, vCenter. Here’s how to do that.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;cluster-glue&lt;/code&gt; package contains node Pacemaker’s fencing (STONITH)
plugins, one …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For users of VMware virtualization, it’s becoming increasingly common
to deploy Pacemaker clusters within the virtual infrastructure. Doing
this requires that you set up fencing via ESX Server or, more
commonly, vCenter. Here’s how to do that.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;cluster-glue&lt;/code&gt; package contains node Pacemaker’s fencing (STONITH)
plugins, one of which is the &lt;code&gt;external/vcenter&lt;/code&gt; plugin. It enables
Pacemaker to interface with an ESX Server host or vCenter server. When
a Pacemaker node needs to be fenced, the fencing node contacts the
vCenter host and instructs it to knock out the offending node.&lt;/p&gt;
&lt;p&gt;For this to work, your configuration needs to satisfy a couple of prerequisites:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Your setup needs a reasonably recent cluster-glue package (the one
  that ships in Debian squeeze-backports and Ubuntu precise is fine).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You need to install the &lt;a href="http://www.vmware.com/support/developer/vc-sdk/"&gt;vSphere Web Services
  SDK&lt;/a&gt; on your
  nodes. This itself has a number of Perl prerequisites. On
  Debian/Ubuntu systems, you should be able to install them with:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;aptitude install libarchive-zip-perl libcrypt-ssleay-perl \
  libclass-methodmaker-perl libuuid-perl \
  libsoap-lite-perl libxml-libxml-perl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, create a set of vCenter credentials with the &lt;code&gt;credstore_admin.pl&lt;/code&gt;
utility that comes bundled with the SDK:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;/usr/lib/vmware-vcli/apps/general/credstore_admin.pl \
  -s &amp;lt;vCenter server IP or hostname&amp;gt; \
  -u &amp;lt;vCenter username&amp;gt; \
  -p &amp;lt;vCenter password&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This creates a credentials file in
&lt;code&gt;.vmware/credstore/vicredentials.xml&lt;/code&gt; relative to your home
directory. Copy this file into a location where Pacemaker can find it,
say &lt;code&gt;/etc/vicredentials.xml&lt;/code&gt;, and make sure it gets 0600
permissions. Also, remember to copy it to all your cluster nodes. Once
your credentials are properly set up, you can test the STONITH agent’s
functionality by invoking it directly, like so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nv"&gt;VI_SERVER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&amp;lt;vCenter&lt;span class="w"&gt; &lt;/span&gt;server&lt;span class="w"&gt; &lt;/span&gt;IP&lt;span class="w"&gt; &lt;/span&gt;or&lt;span class="w"&gt; &lt;/span&gt;hostname&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nv"&gt;VI_CREDSTORE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/etc/vicredentials.xml&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nv"&gt;HOSTLIST&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"&amp;lt;pacemaker hostname&amp;gt;=&amp;lt;vCenter virtual machine name&amp;gt;"&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nv"&gt;RESETPOWERON&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;/usr/lib/stonith/plugins/external/vcenter&lt;span class="w"&gt; &lt;/span&gt;gethosts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;pacemaker hostname=""&gt; is the name of one of your cluster nodes as per
uname -n, and &lt;vcenter machine="" name="" virtual=""&gt; is the corresponding
machine name in your vCenter inventory. If everything is working fine,
the gethosts command should return the Pacemaker hostname again.&lt;/vcenter&gt;&lt;/pacemaker&gt;&lt;/p&gt;
&lt;p&gt;Now, on to adding this to the Pacemaker configuration. The example
below is for two hosts named alice and bob, which in the inventory
happen to be listed by their FQDN in the example.com domain:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;primitive p_fence_alice stonith:external/vcenter \&lt;/span&gt;
&lt;span class="n"&gt;  params VI_SERVER="vcenter.example.com" \&lt;/span&gt;
&lt;span class="n"&gt;    VI_CREDSTORE="/etc/vicredentials.xml" \&lt;/span&gt;
&lt;span class="n"&gt;    HOSTLIST="alice=alice.example.com" \&lt;/span&gt;
&lt;span class="n"&gt;    RESETPOWERON="0" \&lt;/span&gt;
&lt;span class="n"&gt;    pcmk_host_check="static-list" \&lt;/span&gt;
&lt;span class="n"&gt;    pcmk_host_list="alice" \&lt;/span&gt;
&lt;span class="n"&gt;  op monitor interval="60"&lt;/span&gt;
&lt;span class="n"&gt;primitive p_fence_bob stonith:external/vcenter \&lt;/span&gt;
&lt;span class="n"&gt;  params VI_SERVER="vcenter.example.com" \&lt;/span&gt;
&lt;span class="n"&gt;    VI_CREDSTORE="/etc/vicredentials.xml" \&lt;/span&gt;
&lt;span class="n"&gt;    HOSTLIST="bob=bob.example.com" \&lt;/span&gt;
&lt;span class="n"&gt;    RESETPOWERON="0" \&lt;/span&gt;
&lt;span class="n"&gt;    pcmk_host_check="static-list" \&lt;/span&gt;
&lt;span class="n"&gt;    pcmk_host_list="bob" \&lt;/span&gt;
&lt;span class="n"&gt;  op monitor interval="60"&lt;/span&gt;
&lt;span class="n"&gt;location l_fence_alice p_fence_alice -inf: alice&lt;/span&gt;
&lt;span class="n"&gt;location l_fence_bob p_fence_bob -inf: bob&lt;/span&gt;
&lt;span class="n"&gt;property stonith-enabled="true"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;At this point you should be able to test fencing with &lt;code&gt;stonith_admin
-F&lt;/code&gt; or &lt;code&gt;crm node fence&lt;/code&gt;. Or simulate a node problem with &lt;code&gt;killall -9
corosync&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Special thanks for this goes to Nhan Ngo Dinh both for writing the
plugin in the first place, and for providing an excellent and
straightforward README file for it.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Pacemaker"></category></entry><entry><title>Mandatory and advisory ordering in Pacemaker</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/mandatory-and-advisory-ordering-pacemaker/" rel="alternate"></link><published>2012-03-22T15:02:14+01:00</published><updated>2012-03-22T15:02:14+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-03-22:/resources/hints-and-kinks/mandatory-and-advisory-ordering-pacemaker/</id><summary type="html">&lt;p&gt;Ever wonder what’s the difference between &lt;code&gt;order &amp;lt;name&amp;gt; inf:
&amp;lt;first-resource&amp;gt; &amp;lt;second-resource&amp;gt;&lt;/code&gt; and a score of something other
than &lt;code&gt;inf&lt;/code&gt;? We’ll explain.&lt;/p&gt;
&lt;p&gt;If you specify an order constraint score of &lt;code&gt;INFINITY&lt;/code&gt; (&lt;code&gt;inf&lt;/code&gt; or the
keyword &lt;code&gt;mandatory&lt;/code&gt; in crm shell syntax), then the order constraint is
considered mandatory. If you …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ever wonder what’s the difference between &lt;code&gt;order &amp;lt;name&amp;gt; inf:
&amp;lt;first-resource&amp;gt; &amp;lt;second-resource&amp;gt;&lt;/code&gt; and a score of something other
than &lt;code&gt;inf&lt;/code&gt;? We’ll explain.&lt;/p&gt;
&lt;p&gt;If you specify an order constraint score of &lt;code&gt;INFINITY&lt;/code&gt; (&lt;code&gt;inf&lt;/code&gt; or the
keyword &lt;code&gt;mandatory&lt;/code&gt; in crm shell syntax), then the order constraint is
considered mandatory. If you specify &lt;code&gt;0&lt;/code&gt;, or the keyword &lt;code&gt;advisory&lt;/code&gt;
then it’s advisory. What does that mean?&lt;/p&gt;
&lt;p&gt;Firstly, anytime two resources are started in the same cluster
transition, order constraints do apply regardless of whether they’re
mandatory or advisory. So for the two constraints shown here:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;order o_foo_before_bar inf: foo bar
order o_foo_before_bar 0: foo bar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;… if &lt;code&gt;foo&lt;/code&gt; and &lt;code&gt;bar&lt;/code&gt; are just starting, &lt;code&gt;foo&lt;/code&gt; starts first, and
&lt;code&gt;bar&lt;/code&gt; starts only when &lt;code&gt;foo&lt;/code&gt;‘s start operation is completed. So what’s
the difference, really?&lt;/p&gt;
&lt;h2&gt;Mandatory ordering&lt;/h2&gt;
&lt;p&gt;In a &lt;strong&gt;mandatory&lt;/strong&gt; order constraint, the order is enforced under all
circumstances. Consider the following example (primitive definitions
omitted to keep this short):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;order o_foo_before_bar inf: foo bar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Suppose &lt;code&gt;foo&lt;/code&gt; fails. Now &lt;code&gt;foo&lt;/code&gt; must be recovered, but before that,
&lt;code&gt;bar&lt;/code&gt; must also stop. So the sequence of events is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;foo&lt;/code&gt; fails&lt;/li&gt;
&lt;li&gt;Pacemaker attempts to stop &lt;code&gt;foo&lt;/code&gt; again (to make sure it’s cleaned
   up).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bar&lt;/code&gt; stops.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;foo&lt;/code&gt; starts&lt;/li&gt;
&lt;li&gt;&lt;code&gt;bar&lt;/code&gt; starts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If &lt;code&gt;foo&lt;/code&gt; fails to start back up, then &lt;code&gt;bar&lt;/code&gt; will remain stopped. Based
on the start-failure-is-fatal and migration-threshold settings both
resources can now potentially migrate to other nodes, but if &lt;code&gt;foo&lt;/code&gt;
can’t be started anywhere, &lt;code&gt;bar&lt;/code&gt; also remains stopped.&lt;/p&gt;
&lt;h2&gt;Advisory ordering&lt;/h2&gt;
&lt;p&gt;In an &lt;strong&gt;advisory&lt;/strong&gt; order constraint, the order is enforced only if
both resources start in the same transition. Otherwise, it’s
ignored. Consider the following example (primitive definitions again
omitted):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;order o_foo_before_bar 0: foo bar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Again, suppose &lt;code&gt;foo&lt;/code&gt; fails. &lt;code&gt;foo&lt;/code&gt; must be recovered, but now &lt;code&gt;bar&lt;/code&gt; can
keep running as it’s not being started in the same transition. Thus:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;foo&lt;/code&gt; fails&lt;/li&gt;
&lt;li&gt;Pacemaker attempts to stop &lt;code&gt;foo&lt;/code&gt; again (to make sure it’s cleaned
   up).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;foo&lt;/code&gt; starts&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If &lt;code&gt;foo&lt;/code&gt; fails to start back up, then &lt;code&gt;bar&lt;/code&gt; can continue to
run. Still, based on the &lt;code&gt;start-failure-is-fatal&lt;/code&gt; and
&lt;code&gt;migration-threshold&lt;/code&gt; settings applying to &lt;code&gt;foo&lt;/code&gt;, either it or both
resources (depending on colocation constraints) can potentially
migrate to other nodes.&lt;/p&gt;
&lt;h2&gt;So when do I use which?&lt;/h2&gt;
&lt;p&gt;Advisory ordering is good for when your dependent resource can recover
from a brief interruption in the resource it depends on. For example,
you’ll want to fire up your libvirt daemon before you start your
Pacemaker-managed virtual machines, but if libvirtd were ever to crash
you can restart it without needing to restart VMs.&lt;/p&gt;
&lt;p&gt;Mandatory ordering is for stricter dependencies. Filesystems mounted
from an iSCSI device will probably want to be remounted if the iSCSI
initator has reported an error. Likewise, you’ll probably also want to
restart the applications working with that filesystem.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Pacemaker"></category></entry><entry><title>Managing cron jobs with Pacemaker</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/managing-cron-jobs-pacemaker/" rel="alternate"></link><published>2012-03-19T16:42:40+01:00</published><updated>2012-03-19T16:42:40+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-03-19:/resources/hints-and-kinks/managing-cron-jobs-pacemaker/</id><summary type="html">&lt;p&gt;It’s not uncommon in Pacemaker clusters to run specific cron jobs only
on a node that currently runs a particular resource. The
&lt;code&gt;ocf:heartbeat:symlink&lt;/code&gt; resource agent can be exceptionally helpful in
this situation. Here’s how to use it.&lt;/p&gt;
&lt;p&gt;Suppose you’ve got a cron job for Postfix …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It’s not uncommon in Pacemaker clusters to run specific cron jobs only
on a node that currently runs a particular resource. The
&lt;code&gt;ocf:heartbeat:symlink&lt;/code&gt; resource agent can be exceptionally helpful in
this situation. Here’s how to use it.&lt;/p&gt;
&lt;p&gt;Suppose you’ve got a cron job for Postfix whose definition normally
lives in &lt;code&gt;/etc/cron.d/postfix&lt;/code&gt;. All your Postfix related data is in a
mountpoint &lt;code&gt;/srv/postfix&lt;/code&gt; (that filesystem could live on iSCSI, or DRBD,
or it could be a GlusterFS mount – that’s irrelevant for the purposes
of this discussion). And as such, you’ve moved your cron definition to
&lt;code&gt;/srv/postfix/cron&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now you want that cron job to execute only on the node that also is
currently the active Postfix host. That’s not hard at all:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;primitive p_postfix ocf:heartbeat:postfix \
  params config_dir="/etc/postfix" \
  op monitor interval="10"
primitive p_symlink ocf:heartbeat:symlink \
  params target="/srv/postfix/cron" \
    link="/etc/cron.d/postfix" \
    backup_suffix=".disabled" \
  op monitor interval="10"
primitive p_cron lsb:cron \
  op monitor interval=10
order o_symlink_before_cron inf: p_symlink p_cron
colocation c_cron_on_symlink inf: p_cron p_symlink
colocation c_symlink_on_postfix inf: p_symlink p_postfix
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What this will do for you is this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Check whether a file named &lt;code&gt;postfix&lt;/code&gt; already exists in &lt;code&gt;/etc/cron.d&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If it does, rename it to &lt;code&gt;postfix.disabled&lt;/code&gt; (remember, cron ignores
  job definitions with dots in the filename)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;(Re-)Create the postfix job definition as a symlink to
  &lt;code&gt;/srv/postfix/cron&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Restart &lt;code&gt;cron&lt;/code&gt; when it’s done.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;c_symlink_on_postfix&lt;/code&gt; colocation ensures that all of this happens
on the node where the &lt;code&gt;p_postfix&lt;/code&gt; resource is also active.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Pacemaker"></category></entry><entry><title>Storage Replication in High-Performance High-Availability Environments</title><link href="https://xahteiwi.eu/resources/presentations/storage-replication-high-performance-high-availability-environments/" rel="alternate"></link><published>2012-03-19T10:54:00+00:00</published><updated>2012-03-19T10:54:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-03-19:/resources/presentations/storage-replication-high-performance-high-availability-environments/</id><summary type="html">&lt;p&gt;At linux.conf.au 2012, Florian I this presentation on the integration
of &lt;a href="https://www.hastexo.com/drbd"&gt;DRBD&lt;/a&gt;, Flashcache and
&lt;a href="https://www.hastexo.com/knowledge/high-availability/pacemaker"&gt;Pacemaker&lt;/a&gt;
in the High Availability and Distributed Storage miniconf.&lt;/p&gt;
&lt;p&gt;In this 30-minute presentation, I explore the benefits of
&lt;a href="https://github.com/facebook/flashcache"&gt;Flashcache&lt;/a&gt; for replicated
storage. Flashcache, &lt;a href="https://www.facebook.com/note.php?note_id=388112370932"&gt;originally developed at
Facebook,&lt;/a&gt; is a
general purpose, &lt;a href="https://github.com/facebook/flashcache/blob/master/doc/flashcache-doc.txt"&gt;block level cache …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;At linux.conf.au 2012, Florian I this presentation on the integration
of &lt;a href="https://www.hastexo.com/drbd"&gt;DRBD&lt;/a&gt;, Flashcache and
&lt;a href="https://www.hastexo.com/knowledge/high-availability/pacemaker"&gt;Pacemaker&lt;/a&gt;
in the High Availability and Distributed Storage miniconf.&lt;/p&gt;
&lt;p&gt;In this 30-minute presentation, I explore the benefits of
&lt;a href="https://github.com/facebook/flashcache"&gt;Flashcache&lt;/a&gt; for replicated
storage. Flashcache, &lt;a href="https://www.facebook.com/note.php?note_id=388112370932"&gt;originally developed at
Facebook,&lt;/a&gt; is a
general purpose, &lt;a href="https://github.com/facebook/flashcache/blob/master/doc/flashcache-doc.txt"&gt;block level cache device implemented in the Linux
device-mapper
framework.&lt;/a&gt;
You can use flashcache in two distinct ways in Pacemaker
high-availability clusters, which I both explain in his talk.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Video: &lt;a href="https://youtu.be/l910kiEuHOM"&gt;YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="presentations"></category><category term="DRBD"></category><category term="Flashcache"></category><category term="Pacemaker"></category><category term="Performance"></category></entry><entry><title>Roll Your Own Cloud</title><link href="https://xahteiwi.eu/resources/presentations/roll-your-own-cloud/" rel="alternate"></link><published>2012-03-19T08:27:00+00:00</published><updated>2012-03-19T08:27:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-03-19:/resources/presentations/roll-your-own-cloud/</id><summary type="html">&lt;p&gt;Tim Serong and I explore the capabilities of KVM, iSCSI, DRBD and Pacemaker to
create a fully open-source enterprise cloud.&lt;/p&gt;
&lt;p&gt;Shot at linux.conf.au 2011 in Brisbane, this is me babbling, and Tim
live-cartooning to the delight of the audience.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Video: &lt;a href="https://youtu.be/NyHJ8Uf03qg"&gt;YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo …&lt;/code&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Tim Serong and I explore the capabilities of KVM, iSCSI, DRBD and Pacemaker to
create a fully open-source enterprise cloud.&lt;/p&gt;
&lt;p&gt;Shot at linux.conf.au 2011 in Brisbane, this is me babbling, and Tim
live-cartooning to the delight of the audience.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Video: &lt;a href="https://youtu.be/NyHJ8Uf03qg"&gt;YouTube&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="presentations"></category><category term="DRBD"></category><category term="KVM"></category><category term="libvirt"></category><category term="Pacemaker"></category></entry><entry><title>The Zen of Pacemaker</title><link href="https://xahteiwi.eu/resources/presentations/zen-pacemaker/" rel="alternate"></link><published>2012-03-13T13:37:00+00:00</published><updated>2012-03-13T13:37:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-03-13:/resources/presentations/zen-pacemaker/</id><summary type="html">&lt;p&gt;I team up with Tim Serong and Andrew Beekhof for a tutorial at
linux.conf.au 2012.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Pacemaker author Andrew Beekhof dropped in as well, to field questions
and provide additional insight into Pacemaker development.&lt;/p&gt;
&lt;p&gt;The tutorial covers the configuration of a MySQL 2-node high
availability cluster front to back …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I team up with Tim Serong and Andrew Beekhof for a tutorial at
linux.conf.au 2012.&lt;/p&gt;
&lt;!--break--&gt;
&lt;p&gt;Pacemaker author Andrew Beekhof dropped in as well, to field questions
and provide additional insight into Pacemaker development.&lt;/p&gt;
&lt;p&gt;The tutorial covers the configuration of a MySQL 2-node high
availability cluster front to back, diving into the configuration of
replicated storage, the cluster communications infrastructure, cluster
resource management, and of course the MySQL database itself.&lt;/p&gt;
&lt;p&gt;Reviews for this tutorial were rather enthusiastic, with bloggers
comparing the experience to &lt;a href="http://www.anchor.com.au/blog/2012/01/lca-day-3-high-availability/"&gt;an enlightenment session from the Jedi
grand
masters&lt;/a&gt;
of high availability.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Video: &lt;a href="https://youtu.be/3GoT36cK6os"&gt;YouTube&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Slides: &lt;a href="https://www.slideshare.net/slideshow/embed_code/key/jIo0ZxOl7znyYd"&gt;SlideShare&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="presentations"></category><category term="Conference"></category><category term="Pacemaker"></category></entry><entry><title>Fencing in Libvirt/KVM virtualized cluster nodes</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/fencing-libvirtkvm-virtualized-cluster-nodes/" rel="alternate"></link><published>2012-02-29T13:56:42+01:00</published><updated>2012-02-29T13:56:42+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-02-29:/resources/hints-and-kinks/fencing-libvirtkvm-virtualized-cluster-nodes/</id><summary type="html">&lt;p&gt;Often, people deploy the Pacemaker stack in virtual environments for
purposes of testing and evaluation. In such environments, it’s easy to
test Pacemaker’s fencing capabilities by tying in with the hypervisor.&lt;/p&gt;
&lt;p&gt;This quick howto illustrates how to configure fencing for two virtual
cluster nodes hosted on a libvirt …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Often, people deploy the Pacemaker stack in virtual environments for
purposes of testing and evaluation. In such environments, it’s easy to
test Pacemaker’s fencing capabilities by tying in with the hypervisor.&lt;/p&gt;
&lt;p&gt;This quick howto illustrates how to configure fencing for two virtual
cluster nodes hosted on a libvirt/KVM hypervisor host.&lt;/p&gt;
&lt;h2&gt;libvirt configuration (hypervisor)&lt;/h2&gt;
&lt;p&gt;In order to do libvirt fencing, your hypervisor should have its
libvirtd daemon listen on a network socket. libvirtd is capable of
doing this, both on an encrypted TLS socket, and on a regular,
unencrypted TCP port. Needless to say, for production use you should
only use TLS, but for testing and evaluation – and for that purpose
only – TCP is fine.&lt;/p&gt;
&lt;p&gt;In order for your hypervisor to listen on an unauthenticated,
insecure, unencrypted network socket (did we mention that’s unsuitable
for production?), add the following lines to your libvirtd
configuration file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="na"&gt;listen_tls&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;0&lt;/span&gt;
&lt;span class="na"&gt;listen_tcp&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;1&lt;/span&gt;
&lt;span class="na"&gt;tcp_port&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"16509"&lt;/span&gt;
&lt;span class="na"&gt;auth_tcp&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;"none"&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can also set the &lt;code&gt;listen_addr&lt;/code&gt; parameter, for example to have
libvirtd listen only on the network that your virtual machines run
in. If you don’t set listen_addr, libvirtd will simply listen on the
wildcard address.&lt;/p&gt;
&lt;p&gt;You’ll also have to add the &lt;code&gt;-l&lt;/code&gt; or &lt;code&gt;--listen&lt;/code&gt; flag to your libvirtd
invocation. On Debian/Ubuntu platforms, you can do so by editing the
&lt;code&gt;/etc/default/libvirt-bin&lt;/code&gt; configuration file.&lt;/p&gt;
&lt;p&gt;Once you’ve done that, you can use &lt;code&gt;netstat -ltp&lt;/code&gt; to check whether
libvirtd is in fact listening on its configured port, 16509/tcp. Also,
make sure that you don’t have a firewall blocking that port.&lt;/p&gt;
&lt;h2&gt;libvirt configuration (virtual machines)&lt;/h2&gt;
&lt;p&gt;Inside your virtual machines, you’ll also have to install the libvirt
client binaries – the fencing mechanism uses the virsh utility under
the covers. Some platforms provide a &lt;code&gt;libvirt-client&lt;/code&gt; package for that
purpose; for other’s, you’ll simply have to install the full &lt;code&gt;libvirt&lt;/code&gt;
package.&lt;/p&gt;
&lt;p&gt;Once that is set up, you should be able to run this command from
inside your virtual machines:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;virsh&lt;span class="w"&gt; &lt;/span&gt;--connect&lt;span class="o"&gt;=&lt;/span&gt;qemu+tcp://&amp;lt;IP&lt;span class="w"&gt; &lt;/span&gt;of&lt;span class="w"&gt; &lt;/span&gt;your&lt;span class="w"&gt; &lt;/span&gt;hypervisor&amp;gt;/system&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;list&lt;span class="w"&gt; &lt;/span&gt;--all
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;… and that command should list all the domains running on that host,
including the one you’re connecting from.&lt;/p&gt;
&lt;h2&gt;Pacemaker configuration&lt;/h2&gt;
&lt;p&gt;In one of your virtual machines, you can now set up your fencing
configuration.&lt;/p&gt;
&lt;p&gt;This example assumes that you have two nodes named alice and bob, that
their corresponding virtual machine domain names are also alice and
bob, and that they can reach their hypervisor by TCP at 192.168.0.1:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;primitive p_fence_alice stonith:external/libvirt \
  params hostlist="alice" \
   hypervisor_uri="qemu+tcp://192.168.0.1/system" \
  op monitor interval="60"
primitive p_fence_bob stonith:external/libvirt \
  params hostlist="bob" \
    hypervisor_uri="qemu+tcp://192.168.0.1/system" \
  op monitor interval="60"
location l_fence_alice p_fence_alice -inf: alice
location l_fence_bob p_fence_bob -inf: bob
property stonith-enabled=true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now you can test fencing to the best of your abilities.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Pacemaker"></category></entry><entry><title>Network connectivity check in Pacemaker</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/network-connectivity-check-pacemaker/" rel="alternate"></link><published>2012-02-27T17:45:19+01:00</published><updated>2012-02-27T17:45:19+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-02-27:/resources/hints-and-kinks/network-connectivity-check-pacemaker/</id><summary type="html">&lt;p&gt;If you want a Pacemaker cluster to move resources on changes on the
network connectivity of an individual node, there are two major steps
involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let Pacemaker monitor connectivity;&lt;/li&gt;
&lt;li&gt;Configure constraints to react on connectivity changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Be sure to run at least Pacemaker 1.0.11 or 1.1 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you want a Pacemaker cluster to move resources on changes on the
network connectivity of an individual node, there are two major steps
involved:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Let Pacemaker monitor connectivity;&lt;/li&gt;
&lt;li&gt;Configure constraints to react on connectivity changes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;Be sure to run at least Pacemaker 1.0.11 or 1.1.6 to include some
important fixes affecting the &lt;code&gt;ocf:pacemaker:ping&lt;/code&gt; resource agent.&lt;/p&gt;
&lt;p&gt;Preferably, choose more than one reliable ping targets in your network
(like a highly available gateway router, a core switch, or DNS
server).&lt;/p&gt;
&lt;h2&gt;Pacemaker configuration&lt;/h2&gt;
&lt;p&gt;The following crm shell code snippet configures a cloned ping resource
including constraints to run Dummy resources on any node that has
connectivity at all. Please note, that the first constraint forbids to
run &lt;code&gt;p_dummy1&lt;/code&gt; if all nodes lose connectivity. The second constraint
places &lt;code&gt;p_dummy2&lt;/code&gt; on the node that has the best connectivity:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;primitive p_ping ocf:pacemaker:ping \
   params host_list="dns.example.com router.example.com" \
   multiplier="1000" dampen="60s"\
   op monitor interval="10s"
clone cl_ping p_ping

primitive p_dummy1 ocf:pacemaker:Dummy
primitive p_dummy2 ocf:pacemaker:Dummy

location l_dummy1_needs_connectivity p_dummy1 \
  rule -inf: not_defined pingd or pingd lte 0
location l_dummy2_likes_best_connectivity p_dummy2 \
  rule pingd: defined pingd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Pacemaker"></category></entry><entry><title>GFS2 in Pacemaker (Debian/Ubuntu)</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/gfs2-pacemaker-debianubuntu/" rel="alternate"></link><published>2012-02-26T20:34:08+01:00</published><updated>2012-02-26T20:34:08+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-02-26:/resources/hints-and-kinks/gfs2-pacemaker-debianubuntu/</id><summary type="html">&lt;p&gt;Setting up GFS2 in Pacemaker requires configuring the Pacemaker DLM,
the Pacemaker GFS control daemon, and a GFS2 filesystem itself.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;GFS2 with Pacemaker integration is supported on Debian
(&lt;code&gt;squeeze-backports&lt;/code&gt; and up) and Ubuntu (10.04 LTS and up). You’ll need
the &lt;code&gt;dlm-pcmk&lt;/code&gt;, &lt;code&gt;gfs2-tools&lt;/code&gt;, and &lt;code&gt;gfs-pcmk&lt;/code&gt; packages.&lt;/p&gt;
&lt;p&gt;Fencing is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Setting up GFS2 in Pacemaker requires configuring the Pacemaker DLM,
the Pacemaker GFS control daemon, and a GFS2 filesystem itself.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;GFS2 with Pacemaker integration is supported on Debian
(&lt;code&gt;squeeze-backports&lt;/code&gt; and up) and Ubuntu (10.04 LTS and up). You’ll need
the &lt;code&gt;dlm-pcmk&lt;/code&gt;, &lt;code&gt;gfs2-tools&lt;/code&gt;, and &lt;code&gt;gfs-pcmk&lt;/code&gt; packages.&lt;/p&gt;
&lt;p&gt;Fencing is imperative. Get a proper fencing/STONITH configuration set
up and test it thoroughly.&lt;/p&gt;
&lt;h2&gt;Pacemaker configuration&lt;/h2&gt;
&lt;p&gt;The Pacemaker configuration, shown here in &lt;code&gt;crm&lt;/code&gt; shell syntax, normally
puts all the required resources into one cloned group. Have a look at
this configuration snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;primitive p_dlm_controld ocf:pacemaker:controld \
  params daemon="dlm_controld.pcmk" \
  op start interval="0" timeout="90" \
  op stop interval="0" timeout="100" \
  op monitor interval="10"
primitive p_gfs_controld ocf:pacemaker:controld \
  params daemon="gfs_controld.pcmk"\
  op start interval="0" timeout="90" \
  op stop interval="0" timeout="100" \
  op monitor interval="10"
primitive p_fs_gfs2 ocf:heartbeat:Filesystem \
  params device="&amp;lt;your device path&amp;gt;" \
    directory="&amp;lt;your mount point&amp;gt;" \
    fstype="gfs2" \
  op monitor interval="10"
group g_gfs2 p_dlm_controld p_gfs_controld p_fs_gfs2
clone cl_gfs2 g_gfs2 \
  meta interleave="true"
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then when that’s done, your filesystem should happily mount on all nodes.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Pacemaker"></category></entry><entry><title>Interleaving in Pacemaker clones</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/interleaving-pacemaker-clones/" rel="alternate"></link><published>2012-02-26T20:34:08+01:00</published><updated>2012-02-26T20:34:08+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-02-26:/resources/hints-and-kinks/interleaving-pacemaker-clones/</id><summary type="html">&lt;p&gt;Ever wonder what &lt;code&gt;meta interleave&lt;/code&gt; really means in a Pacemaker clone
definition? We’ll explain.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;interleave&lt;/code&gt; meta attribute is only valid on Pacemaker clone
definitions – and their extended version of sorts, master/slave
sets. It’s not available on primitives and groups. Clones are often
used in configurations involving …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ever wonder what &lt;code&gt;meta interleave&lt;/code&gt; really means in a Pacemaker clone
definition? We’ll explain.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;interleave&lt;/code&gt; meta attribute is only valid on Pacemaker clone
definitions – and their extended version of sorts, master/slave
sets. It’s not available on primitives and groups. Clones are often
used in configurations involving cluster filesystems, such as GFS2
(&lt;a href="https://xahteiwi.eu/resources/hints-and-kinks/gfs2-pacemaker-debianubuntu/"&gt;here’s an example&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Consider the following example (primitive definitions omitted to keep
this short):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;clone cl_foo p_foo meta interleave=false
clone cl_bar p_bar meta interleave=false
order o_foo_before_bar inf: cl_foo cl_bar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;What this means is for the &lt;code&gt;order&lt;/code&gt; constraint to be fulfilled, &lt;em&gt;all&lt;/em&gt;
instances of &lt;code&gt;cl_foo&lt;/code&gt; must start before &lt;em&gt;any&lt;/em&gt; instance of &lt;code&gt;cl_bar&lt;/code&gt;
can. Often, that’s not what you want.&lt;/p&gt;
&lt;p&gt;In contrast, consider this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;clone cl_foo p_foo meta interleave=true
clone cl_bar p_bar meta interleave=true
order o_foo_before_bar inf: cl_foo cl_bar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here, for each node, as soon as the &lt;em&gt;local&lt;/em&gt; instance of &lt;code&gt;cl_foo&lt;/code&gt; has
started, the corresponding local instance of &lt;code&gt;cl_bar&lt;/code&gt; can, too. &lt;strong&gt;This
is what’s usually desired – when in doubt, allow interleaving.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One thing that often throws people is that interleaving only works
when Pacemaker is configured to run the same number of instances of
two clones on the same node. Thus,&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;clone cl_foo p_foo\
  meta interleave=true \
    globally-unique=true clone-node-max=2
clone cl_bar p_bar meta interleave=false
order o_foo_before_bar inf: cl_foo cl_bar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;… won’t work, as Pacemaker is allowed to run 2 instances of &lt;code&gt;cl_foo&lt;/code&gt;
on the same node, but only one of &lt;code&gt;cl_bar&lt;/code&gt; (the default for
&lt;code&gt;clone-node-max&lt;/code&gt; is 1).&lt;/p&gt;
&lt;p&gt;Also, &lt;code&gt;globally-unique=true&lt;/code&gt; is a requirement for any
&lt;code&gt;clone-node-max&lt;/code&gt;&amp;gt;1 – which means that interleaving between a
globally-unique and a not globally-unique clone is also not supported.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Pacemaker"></category></entry><entry><title>OCFS2 in Pacemaker (Debian/Ubuntu)</title><link href="https://xahteiwi.eu/resources/hints-and-kinks/ocfs2-pacemaker-debianubuntu/" rel="alternate"></link><published>2012-02-24T17:01:20+01:00</published><updated>2012-02-24T17:01:20+01:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2012-02-24:/resources/hints-and-kinks/ocfs2-pacemaker-debianubuntu/</id><summary type="html">&lt;p&gt;Setting up OCFS2 in Pacemaker requires configuring the Pacemaker DLM,
the O2CB lock manager for OCFS2, and an OCFS2 filesystem itself.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OCFS2 with Pacemaker integration is supported on Debian
  (&lt;code&gt;squeeze-backports&lt;/code&gt; and up) and Ubuntu (10.04 LTS and up). You’ll
  need the &lt;code&gt;dlm-pcmk&lt;/code&gt;, &lt;code&gt;ocfs2-tools&lt;/code&gt;, &lt;code&gt;ocfs2-tools-pacemaker&lt;/code&gt; and
  &lt;code&gt;openais&lt;/code&gt; packages …&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Setting up OCFS2 in Pacemaker requires configuring the Pacemaker DLM,
the O2CB lock manager for OCFS2, and an OCFS2 filesystem itself.&lt;/p&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OCFS2 with Pacemaker integration is supported on Debian
  (&lt;code&gt;squeeze-backports&lt;/code&gt; and up) and Ubuntu (10.04 LTS and up). You’ll
  need the &lt;code&gt;dlm-pcmk&lt;/code&gt;, &lt;code&gt;ocfs2-tools&lt;/code&gt;, &lt;code&gt;ocfs2-tools-pacemaker&lt;/code&gt; and
  &lt;code&gt;openais&lt;/code&gt; packages.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Fencing is imperative. Get a proper fencing/STONITH configuration
  set up and test it thoroughly.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Running OCFS2/Pacemaker integration requires that you load Corosync
  with the &lt;code&gt;openais_ckpt&lt;/code&gt; service enabled. The service definition is in
  the file &lt;code&gt;/etc/corosync/service.d/ckpt-service&lt;/code&gt; which the &lt;code&gt;openais&lt;/code&gt;
  package installs by default. Make sure you did not accidentally
  delete or disable this file.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Pacemaker configuration&lt;/h2&gt;
&lt;p&gt;The Pacemaker configuration, shown here in crm shell syntax, normally
puts all the required resources into one cloned group. Have a look at
this configuration snippet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;primitive p_dlm_controld ocf:pacemaker:controld \
  op start interval="0" timeout="90" \
  op stop interval="0" timeout="100" \
  op monitor interval="10"
primitive p_o2cb ocf:pacemaker:o2cb \
  op start interval="0" timeout="90" \
  op stop interval="0" timeout="100" \
  op monitor interval="10"
primitive p_fs_ocfs2 ocf:heartbeat:Filesystem \
  params device="&amp;lt;your device path&amp;gt;" \
    directory="&amp;lt;your mount point&amp;gt;" \
    fstype="ocfs2" \
  meta target-role=Stopped \
  op monitor interval="10"
group g_ocfs2 p_dlm_controld p_o2cb p_fs_ocfs2
clone cl_ocfs2 g_ocfs2 \
  meta interleave="true"
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h2&gt;Why keep the filesystem stopped?&lt;/h2&gt;
&lt;p&gt;Because you probably either don’t have a configured OCFS2 filesystem
on your device yet, or your ran mkfs.ocfs2 when the Pacemaker stack
wasn’t running. In either of those two cases, mount.ocfs2 will refuse
to mount the filesystem.&lt;/p&gt;
&lt;p&gt;Thus, fire up your DLM and the o2cb process like the above
configuration does, and then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you haven’t got a filesystem yet, run &lt;code&gt;mkfs.ocfs2&lt;/code&gt; on your device, or&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you do already have one, run
  &lt;code&gt;tunefs.ocfs2 --update-cluster-stack &amp;lt;device&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Then when that’s done, run &lt;code&gt;crm resource start p_fs_ocfs2&lt;/code&gt; and your
filesystem should happily mount on all nodes.&lt;/p&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="hints-and-kinks"></category><category term="Pacemaker"></category></entry><entry><title>MySQL High Availability Sprint: Launch the Pacemaker!</title><link href="https://xahteiwi.eu/resources/presentations/mysql-high-availability-sprint-launch-pacemaker/" rel="alternate"></link><published>2011-11-01T13:45:00+00:00</published><updated>2011-11-01T13:45:00+00:00</updated><author><name>Florian Haas</name></author><id>tag:xahteiwi.eu,2011-11-01:/resources/presentations/mysql-high-availability-sprint-launch-pacemaker/</id><content type="html">&lt;p&gt;This is a very dense tutorial given at Percona Live UK 2011 in London,
England. In three hours, I covered the MySQL HA Stack with
Pacemaker and DRBD, front to back.&lt;/p&gt;
&lt;!--break--&gt;
&lt;ul&gt;
&lt;li&gt;Slides: &lt;a href="https://www.slideshare.net/slideshow/embed_code/key/sz9doig59uDdAC"&gt;SlideShare&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr/&gt;
&lt;p&gt;This article originally appeared on the &lt;code&gt;hastexo.com&lt;/code&gt; website (now defunct).&lt;/p&gt;</content><category term="presentations"></category><category term="Conference"></category><category term="MySQL"></category><category term="Pacemaker"></category></entry></feed>