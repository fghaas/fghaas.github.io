var tipuesearch = {"pages":[{"title":"About me (and this site)","text":"I've worked pretty much exclusively with open source software since about the time I graduated from college in 2003. I worked at a small outfit out of Vienna called ADS-System, which was then acquired by Kapsch BusinessCom . I then went to work for LINBIT for about 4½ years, and co-founded hastexo with my partners Martin Loschwitz and Andreas Kurz in 2011. We parted ways 3 years in, and 3 years after that I sold hastexo to City Network AB. Finally, in 2022, City Network rebranded as Cleura . At Cleura I currently serve as Head of Education, and I am also the MD of City Network Education Services, the legal entity formerly known as hastexo (the hastexo brand is defunct). Some of the content on this site was originally hosted on hastexo.com , was not carried over onto the City Cloud web site, and I've resurrected it here based on popular demand and for reference purposes. Such content is clearly spelled out as such at the bottom of the respective pages. Regardless, I work on this site in my spare time (which is also why articles tend to appear here on weekends) and you should not mistake anything on this site as official pronouncements of my employer. For those, please head over to cleura.com . Most of the articles on this site cover technical issues, largely around OpenStack , Open edX , and Ceph — but there's also a sprinkling of Python content and other topics. Some are write-ups of conference talks. Some articles are about how people work . Some are labeled Philosophy , though Ramblings might be a more appropriate tag. This site has an Atom and RSS feed. Its content is available under the CC-BY-SA 4.0 license, except where expressly indicated otherwise. Hey, can I ask you a favor? If you quote me by full name, I'd really appreciate if you got my name right. My surname is Haas , not \"Hass\", and it does irk me a little bit when someone misspells my name to the thing that in my language refers to the worst and most useless emotion of all . Thank you!","tags":"pages","url":"about/","loc":"about/"},{"title":"About comments on this site","text":"I've opted against including a comment facility on this site. My rationale is this: if you find something interesting on this site and want to discuss it, then share it on your favorite social network (maybe give Mastodon a try!), and your friends will see what you think. Unless you have exceptionally boring and stubborn friends, this will stimulate the discussion far better than a random comment on a random blog will. 1 However, if you find something that's factually wrong in an article, I would very much appreciate a correction. Since this blog is hosted on GitHub, and built and published automatically from its repository, there are a few things you can do to let me know something is wrong, and help me fix it. File a GitHub issue. Go to my repo's issue tracker and hit New Issue. I've created a couple of issue templates for you to choose from. Send a GitHub pull request. Just fork my repo , create your changes on a topic branch, push it to your fork, and send me a PR. You can combine this with filing an issue, but for minor fixes that don't need discussion (such as a typo or grammar fix), just a PR is fine. Use the GitHub Edit feature. If you open any file in my repository's content/articles directory, you'll find an Edit button (look for the 🖉 icon). That way, you can make an edit and create a PR directly from GitHub's web interface. It's functionally the same process as forking, editing your fork, and then sending a PR, but it makes the process a little bit smoother. You can test how that works by opening the edit mode for the page you're currently reading . Also, if you have something to add to an article, you can follow the same process in order to inject a reference or add a footnote. Hat tip to my former colleague Kriss Andsten , whose thinking I borrowed here. ↩","tags":"pages","url":"comments/","loc":"comments/"},{"title":"xahteiwi.eu","text":"My name is Florian, and this is something resembling a website. Tech Technical tips, hints, and workarounds. Talks Videos, slide decks, and additional material to go with conference and meetup talks. Thoughts My blog (of sorts). Updates, musings and ramblings.","tags":"pages","url":"index/","loc":"index/"},{"title":"Legal Notice","text":"Information according to §25 Mediengesetz (MedienG) This website, along with all information provided through it, is provided by Florian Haas Karl Theuer Strasse 21 A-2514 Traiskirchen Austria Contact information Email: webmaster at xahteiwi dot eu General Disclaimer Materials displayed on this website are intended for reference use only. A Disclaimer of Warranties and Limitation of Liability are included in the applicable license . Standard Disclaimer for External Links External links are being provided as a convenience and for informational purposes only; they do not constitute an endorsement or an approval of any of the products, services or opinions of the corporation or organization or individual. I bear no responsibility for the accuracy, legality or content of the external site or for that of subsequent links. Contact the external site for answers to questions regarding its content.","tags":"pages","url":"legal/","loc":"legal/"},{"title":"Privacy Information/GDPR Notice","text":"Personal Data This web site does not collect, store, or process Personal Data. It does not require the use of cookies. Privacy Policy This web site is hosted on GitHub Pages . The GitHub Privacy Statement applies. External resources Through this site, you will access some resources that are hosted externally, from the following sources: Source Domain(s) Applicable Policy Creative Commons creativecommons.org , licensebuttons.net Privacy Policy GitHub github.com , github.io Privacy Statement Google Fonts fonts.googleapis.com Privacy Policy Prezi prezi.com Privacy Policy YouTube youtube.com , youtu.be Privacy Policy","tags":"pages","url":"privacy/","loc":"privacy/"},{"title":"Uncertainty, industrious compliance, and the illusion of control","text":"If I had visited you from the future six months ago, and given you today's headlines to read, you would probably have dismissed me as a lunatic. And you'd probably say the same to someone who dropped in on you today and read you the headlines from six months out. And if conversely you tried to sit down tonight and write the headlines for May 29, 2023, you'll probably notice on the day that your predictions were nowhere near actual events. You have no idea what the world, or even your life, will look like just six months from now. Make it five years or ten, and you won't have the faintest shimmer of a clue. There is no certainty about your future. Now, there are three approaches to deal with this kind of uncertainty: Fatalism. The idea that whatever happens will happen, you have no control over it, and therefore any attempt to wield any influence over your own life is doomed. Control. The idea that you can achieve certainty in your life by planning everything and leaving nothing to chance. Creativity. The idea that, although some events in your life (and the world) are beyond your influence, you will have the ideas needed to make the best of the twists and turns, good and bad, that life throws at you. In comparing these, I'd maintain that fatalism is a terrible strategy for life, and that far superior to sticking to the illusion of control is an approach favouring creativity — \"the process of having original ideas that have value,\" according to Sir Ken Robinson . Now, almost exactly the same considerations apply to the medium-term future of a corporation. Nobody really knows what the future holds. There is no certainty. The market can go either way, interest rates can explode shattering your financing, you might close on an unexpected opportunity that sends your business flying. In principle, corporations could make the same considerations that apply to people, and strive to foster creativity left, right, and centre. I am routinely dismayed, though, by the fact that the approach heavily favoured in management is one of control, to absurd ends. I think it's obviously ludicrous to expect an engineering manager to \"plan\" what their team will be working on 5 quarters from now, or a sales person to \"project\" product sales next July. There are close to a million things that could influence either outcome, and the person would have to do the work equivalent to that of meteorologist predicting maritime weather patterns for container ships, weeks and months out. Almost nobody in business has the data, the time, the staff, and the budget to do that. So managers turn to metrics and KPIs and OKRs and whatever the TLA du jour may be, in a frantic quest to achieve some degree of certainty — a process which, as others have pointed out , suffocates the creativity that's really needed to address a constantly changing environment. The idea is that if you collect all the metrics and do all the statistics and measure everything (in other ways, that you follow some \"process\" perfectly), you'll succeed. That is so obviously non-sensical that the question becomes, why the hell does anyone think that this works? And I have a hypothesis on why that is so. I attribute it to an institution that practically every manager has gone through, indeed, one that practically everyone has gone through. That institution is school. I don't know about yours, but I can say one thing about my own school education: all of my schooling can be summarized in just a few sentences. Specifically, here's what school taught me: Here is a set of rules. Apply these rules, and you will succeed. Apply the rules perfectly, and you will excel. That applied no matter the subject. It was as true for maths as it was for foreign language education, and for writing essays in history class. Here's how it's done; do it this way and you'll succeed. As it happens, I was good at applying rules. I succeeded in creative writing even when my writing wasn't at all creative: I could spell, I had good grammar, my writing had the expected structure, I understood punctuation. As long as my writing was error-free, I made A's (or rather, 1's, in the school system I inhabited ). My schoolmates might have written far better stories, but if in a couple of pages they had three grammar errors and a few misplaced commas and a couple of spelling glitches, they'd make C's (3's). Now, there's nothing wrong with giving children a set of rules for solving a problem, and then rewarding them for applying those rules correctly. The problem is with the idea that applying the rules perfectly is where excellence lies. That the difference between doing something well, and doing something exceptionally well, is simply in the more perfect application of the rules. That couldn't be farther from the truth. What distinguishes good results from excellent ones is either that the excellent ones follow the rules very well and then add a personal creative touch, or that they deliberately bend or reject conventions and are great nonetheless. It gets worse. The people that school rewards the most — the ones it considers the overachievers, the cream of the crop — are the ones who can most perfectly follow the rules everywhere. We call them straight-A students. I was one of them. There is absolutely no way that any one child could have straight A's, if in school report cards we applied an understanding of excellence that included creativity. You cannot possibly be equally and exceptionally creative in maths, science, languages, history, and all the other subjects you take. Straight A's is what you get solely from industrious compliance. And now, in business, we are stuck between a rock and a hard place. There is a whole generation of people under 35 who have gone through this kind of schooling, but are increasingly disillusioned by it — having understood that all their rules-compliance in school and at university is not a guarantee for economic prosperity or even financial security, and further that they can scarcely expect loyalty from their company if the going gets rough, even if they follow all the rules like a straight-A student. But those same people are being managed by upper-level managers in their late 40s and 50s who not only have achieved some degree of financial stability, but who do believe that it was their education that prepared them for it — and that as long as they continue to meticulously apply a set of rules, everything will be fine and they get to expect a promotion. And when given a new rule book — say, some new management fad imposed by a person of authority, akin to their erstwhile teacher — all they need to do is apply those rules well, and it all will work for them, too. And the more industrious compliance they apply to the rules in the book, the more likely it is that results will not only be good, but excellent. And if that rulebook includes collecting metrics everywhere and doing misguided statistics on them, it will result in an inordinate amount of busywork that kills the creativity that organizations really need to succeed. And likely drive away the people that could contribute exactly that kind of creativity. Eventually, this problem will rectify itself by those managers retiring. But in the interim, maybe a few of us could ditch our school approach to work and collaboration, and engage our brains to produce the original ideas that we really need?","tags":"blog","url":"blog/2022/09/29/industrious-compliance/","loc":"blog/2022/09/29/industrious-compliance/"},{"title":"DevOpsDays Berlin 2022","text":"At DevOpsDays Berlin 2022 , I was slated to give a 30-minute talk titled Writing Professionally. Unfortunately a positive Covid test just three days prior to my departure threw a wrench in those plans, and I was unable to travel or attend a conference (much less speak at one, unmasked). So, I am converting my talk into a write-up, before any symptoms set in. Writing Professionally (in English) This is a talk about developing superpowers. Superpowers that will help you be better in any role, any industry, any profession — particularly the ones where you work in distributed teams, and use an asynchronous and distributed workflow. This is about expressing yourself clearly, succinctly, and professionally in writing. I have put this together for the English language, though parts of this talk may be applicable to other languages as well. This is about writing well in a professional context. That means we're not talking about poetry, or creative writing, or writing a message to a friend. We're talking about writing in the context of professional communications. Being understandable When you write in a professional context, your goal is the reader (or readers) understanding you. That is really the overarching goal, and it's important to understand that we must do everything we possibly can to make it easy for our reader to understand what we mean. When you're writing a novel, you can add plot twists and surprises. You can even use a literary device called an unreliable narrator, where at the very end of the novel you reveal that everything the narrator said (or the protagonist did) was a lie, a delusion, or a dream. In professional writing, we don't have that luxury. We must write in such a way that whoever reads our writing, understands us. And that is the overarching goal no matter the mode we use to communicate. Writing to be understood extends to email messages, chat messages, issue descriptions and comments (in whatever issue tracker you might be using; this includes Jira tickets, Trello cards, GitHub issues and the like) collaboratively edited documents (like wiki pages or shared Google docs) meeting notes (more on those in a bit), and of course technical documentation. And in writing, your No. 1 priority is clarity. Not beauty, elegance, or cleverness. Those all have their place, and there is some room for them in professional writing as well, but you should never sacrifice clarity for any of them. I would also say that being clear is more important than being friendly. If there is something that you need to express, but you can't do it in a friendly manner if you're being clear, I'd say you should be clear, rather than friendly. However, not being particularly friendly doesn't mean being disrespectful — even if you're expressing strong disagreement, you can do so in a respectful manner. Clarity takes two forms Writing clearly can mean one of two things: Clarity of expression, that is, caring about how we write each sentence (and word, really) with a view toward maximum clarity. Clarity of structure, that is, caring about how we design \"documents\" (in the broadest sense), so that they are clear and easy to understand, and are useful for communication. Both of them are equally important, and they depend on each other: a beautifully structured document is useless if it's full of gibberish, and splendidly clear words are useless if they end up in an unstructured wall of text. So we'll spend some time on both, starting with clarity of expression. Orwell's Six Rules One of the simplest, most concise, and most useful rulesets for writing clearly in the English language comes from George Orwell ‘s 1946 essay, Politics and the English Language . In it, Orwell suggested six rules for writers to follow: Rule 1 Never use a metaphor, simile, or other figure of speech which you are used to seeing in print. What Orwell means by this is essentially, \"don't try to be clever\". If you're using a metaphor, chances are that it's not original, that somebody else has used it before, and that people may see it as a tedious trope. More importantly though, your reader might simply not be familiar with the metaphor, depending on their background. As an example, if your readers are from Germany, or are medieval scholars from somewhere in Western Europe, they will probably understand what you mean by calling a salesperson a Pied Piper . If they are not, your readers will be very confused by your reference to a 13th century rat exterminator (or musician, depending on viewpoint) in your writing. Rule 2 Never use a long word where a short one will do. It's remarkable how much clearer your writing gets if you ruthlessly edit it. English almost always gives you a choice between using a long word, and a shorter one with the same meaning. Always opt for the short one. Rather than utilise, write use. Rather than saying methodology, say method. Instead of I don't mean to insinuate, you can write I don't mean to suggest. Rule 3 If it is possible to cut a word out, always cut it out. When you're using shorter words instead of longer ones, you cut syllables and letters from your writing. Naturally, you can also extend that concept to cutting whole words. There's a famous quote that fits well with this particular rule that comes from the French writer, poet, and aviator Antoine de Saint-Exupéry (best known to English-speaking readers for his novella The Little Prince ). « Il semble que la perfection soit atteinte non quand il n'y a plus rien à ajouter, mais quand il n'y a plus rien à retrancher. » ( Terre des hommes , 1939) In English, that roughly translates to: It appears that perfection is achieved not when there is nothing left to add, but when there is nothing left to take away. (I say roughly because cut away would be a better translation than take away, but in English the former sounds unnecessarily harsh.) It needs to be said that this quote disagrees with itself a bit, because it does contain a phrase that is perfectly okay to edit out. If he followed his own advice (and was a little more assertive), Saint-Exupéry could have written: Perfection is achieved not when there is nothing left to add, but when there is nothing left to take away. But it is a good quote to go by, still. If you make it part of your mindset to send an email, or hit the Save button on an issue comment not when you can't think of anything to add, but when you've tried to trim all the fat and find nothing else to trim, that will make you a better professional communicator. Rule 4 Never use the passive where you can use the active. Orwell's rule No. 4 states that you should be using the active voice whenever you can. There are some exceptions to this, but in general the active voice tends to be shorter, more concise, and thus clearer than the passive voice. Passive voice: The system was rebooted. Active voice: We rebooted the system. Sometimes, it's difficult for non-native speakers to even recognize that they are using the passive voice. There's a neat little trick to apply here: if you can take the sentence and append \"by dragons\" to it, then it's passive voice. Applying this rule, we could edit the Saint-Exupéry quote even more radically: We achieve perfection not when we can't add anything, but when we can't take anything away. Now of course, the quote becomes less and less poetic as we edit it. I'm not trying to say that Saint-Exupéry ought to be writing any differently than he did. What I'm trying to say is that the last version is probably the clearest to most people (including those for whom English is their second or third language), and that in professional communications we shouldn't strive to be poetic, we should strive to be clear. Rule 5 Never use a foreign phrase, a scientific word, or a jargon word if you can think of an everyday English equivalent. Orwell's fifth rule emphasises simplicity and accessibility. If there is an uncommon word that means the same thing as a common one, always use the common one. Note that this means that you can — and should — use technical terms if they have no everyday equivalent. That's simply because using an imprecise term would mean losing clarity, when compared to using a precise term. But when we have one that we can easily replace with a common word, we should do so. For example: This is a favourable conceptualization. This is a good idea. Here, we can simply replace the uncommon terms with common ones. Another example in the corporate world is that we can easily replace the jargon term \"human resources\" with \"people\", and also replace \"acquire\" with \"hire\" or \"buy\", depending on context. For this project we'll need to acquire more human resources. For this project we'll need to hire more people. Rule 6 Break any of these rules sooner than say anything outright barbarous. And finally, Orwell reminds us that while this ruleset is a good guideline to follow, it is not absolute. Yes, sometimes the passive voice is just fine, such as when you want to make a point of not assigning blame for something that went wrong, to a particular individual or group. Sometimes you can include a witty quote unedited, even though it used jargon or a foreign word. But having the rulebook in the back of your mind at all times is guaranteed to improve your writing. Little things Let's look at a few other, small things that you can think about to make your writing better. Parallelism Parallelism (parallel structure) is the simple rule that if you are using a particular structure in a sentence, then you keep using that structure until the end of the sentence. We wanted to know the time, the place, and where we were going. We wanted to know the time, the place, and the destination. In this example, both sentences are correct English, even though the first one may sound a little colloquial. The second sentence is clearer, because the use of \"we wanted to know the time, the place, and\" primes the reader to expect another instance of the definite article \"the\", followed by a noun. Finishing the sentence by \"the destination\" makes for a more straightforward reading experience. Emphasis goes last Most English speakers subconsciously read the end of a sentence as being more important than its beginning. Thus, whatever you consider the more important aspect should go at the end of the sentence for emphasis. The drug is highly effective, but has significant side effects. The drug has significant side effects, but is highly effective. The first example emphasises the side effects. Thus, most readers would read it as a warning to prescribe the drug with caution. The second example emphasises the drug's effectiveness, and could thus be seen as an encouragement to prescribe the drug for treatments. Effectively using bullets in a sentence In professional writing, you often have the opportunity to make a long sentence much more readable, by simply injecting bullets. Take this example: It is important that we listen to the customer's needs, build a good solution that keeps us within the applicable regulatory framework, and clearly delineate responsibilities in the proposal and SLA. This sentence is perfectly correct, and it isn't even awkwardly structured. It is just a long, run-on sentence, which most readers will find difficult to grasp at a glance. You probably reread it once or twice. Compare this version: It is important that we listen to the customers needs, build a good solution that keeps us within the applicable regulatory framework, and clearly delineate responsibilities in the proposal and SLA. In this version, not a single word has changed. The statement could still use further editing. But even at this stage, most people will find reading this version much easier than reading the original. That is because most readers will be able to grasp the initial part of the sentence, \"it is important that we\", at a single glance, and will also be able to notice at a glance that there are three bullets. That is to say that without even reading it in full, your reader will understand that you are about to mention what's important, and that there are three things you suggest to consider. Writing positively Writing positively means to write using affirmations, rather than negations. In other words, you state what is, rather than stating what is not. The project manager may not hire outside contractors except those that have been carefully vetted for reliability. The project manager must vet all outside contractors for reliability. Here, we have transformed the negative structure \"may not… except\" into an affirmative one, and we've further cleaned up the writing by eliminating an example of passive voice (\"carefully vetted [by dragons] for reliability\") and replacing it with active voice. Punctuation … does matter. Really. People often overlook how important punctuation is to clarity. What follows is an example of changing the meaning of a sentence drastically, by inserting commas. First, we are dealing with a perfectly normal panda: The panda eats bamboo shoots and leaves. With one comma, the panda becomes a picky eater who takes off after eating the bamboo shoots, rather than also eating the bamboo leaves: The panda eats bamboo shoots, and leaves. And with two commas, we're suddenly dealing with a panda that's a gun nut with an anger management problem: The panda eats bamboo, shoots, and leaves. Whitespace Whitespace normally goes after punctuation, not before. In English (in contrast to some other languages), when whitespace precedes punctuation, it's almost always wrong. The correct sequence at the end of a sentence is full-stop (or exclamation mark, or question mark) followed by whitespace. Likewise, its first comma, then whitespace. First semicolon, then whitespace. There are select exceptions to this rule: whitespace does precede the en dash (–), opening quotation marks, parentheses, brackets, and braces, and optionally the em dash (—). But whitespace preceding punctuation at the end of a sentence is always an error. Question marks (?) Questions should end in a question mark, shouldn't they? Don't end questions with a full-stop or an exclamation point in professional writing. That will likely come across as either passive-aggressive or confrontational. End questions, even rhetorical ones, with a question mark. Exclamation marks (!) Use no more than one explanation mark per paragraph. Zero is fine, too. Exclamation marks are something you want to use sparingly. Never use more than one in immediate succession. Limiting yourself to one per paragraph is a good rule of thumb. Semicolons Semicolons are great; splitting the sentence in two is usually better. Semicolons are great. Splitting the sentence in two is usually better. English mandates that on either side of a semicolon, you have what constitutes essentially a full sentence with a subject and verb (both obligatory) and an object (optional). Thus, they give you the option of just splitting the sentence into two — that is to say, replacing the semicolon with a full-stop. Possessive apostrophe Oscars: more than one Oscar Oscar's: of or related to Oscar A common mistake that native speakers of German or Swedish often make is the omission of the possessive apostrophe. English doesn't have much in the way of cases , but does have something like a genitive case . If something is of Oscar, or related to Oscar, it is Oscar's — such as Oscar's car, Oscar's job, Oscar's house. An \"Oscars job\" is a job for multiple people named Oscar, or a job at the Academy Awards . It's vs. its Infuriatingly, the possessive apostrophe does not apply when the thing that something is related to is simply it . It would be perfectly reasonable to assume that when we refer to the car in \"the car's roof\" as it , it becomes \"it's roof\", but alas, things don't work that way. it's stands for \"it is\" (or \"it has\") its means \"of it\" Yes, this is illogical and immensely confusing. English is weird. Parentheses and brackets Parentheses (like these) are always removable, without taking anything (of significant value) away from the statement. You can use square brackets to shorten quotes. To be, or not to be. That is the question. […] To die, to sleep. No more! Angle brackets often serve as placeholders, for when the reader must replace <important thing> with <other thing>. Hyphens and dashes Often, people think of the three principal forms of dashes as being interchangeable. They really are not. A hyphen, sometimes just called a dash, joins word-pairs together. En dashes, that is dashes roughly the width of a lowercase n , identify things like ranges or dates (March – June, page 47–49). Em dashes — like these, which have the width of a lowercase m — can replace parentheses. Avoiding Germglish (and Poglish, and Ukrainglish, etc.) As with accents, I personally think that we should celebrate the little language quirks that come from influences of our native language on English as a second (or third) language. Unfortunately, not everyone is so tolerant, and sometimes English monolinguals are particularly pesky about these things. So, what follows are a few little things to think about when it comes to typical crossovers from our respective native languages, to English. …, or? Adding the suffix \"…, or?\" to a statement that you want to follow up with a question that asks for confirmation is very common in German and Swedish. But it is a structure that does not exist in English. You must instead use an interrogative phrase, such as: This is a good idea, right? That is an excellent proposal, isn't it? I thought we agree on that, don't we? \"Disturbance\" Swedish and German native speakers often mentally translate the nouns \"störning\" or \"Störung\" into English as \"disturbance\". This is a natural error because these derive from the verbs \"störa\" and \"stören\", respectively, for which the English translation is indeed \"disturb\". However, in English \"disturbance\" is usually read as shorthand for \"civil disturbance\", or \"disturbance of the peace\", which are euphemisms for a violent riot. Thus, to an English speaking recipient the statement \"we have a disturbance in our data center\" would sound much more dramatic than the native German sender probably intended it to be. Articles Many Eastern European languages (such as Polish and Ukrainian) don't use articles, and their native speakers frequently omit them when speaking English. In standard English however, the and a are not optional. Applicable exceptions are newspaper headlines (\"Congress passes law on guns\") and controlled vocabularies (such as troops in combat using voice procedure, \"gunner, sabot, tank, on my command, fire \"). But in regular spoken and written communications, English speakers use both definite and indefinite articles. Subject English sentences always mention the subject. English, unlike Spanish and many Slavic languages, is not a null subject language. With very few exceptions, you can never leave out the subject in a sentence — even when it would otherwise be clear from context. For example, in the English sentence \"I am hungry\" the \"I\" is redundant, because the form \"am\" uniquely identifies the verb as being first-person singular. Still, the sentence cannot be shortened to \"am hungry,\" because that would simply make it incorrect. Lowercase \"you\" Unlike in German and Polish, English speakers never capitalize you in the middle of a sentence, even when using a polite form of address. \"Thank You for Your help\" would look odd and antiquated to them. Now, let's move on to clarity of structure. Five paragraphs that matter Sometimes there are pretty complex things that you want to convey to a group of people. This is a where lot of people would be inclined to call a meeting but in reality, there's a much better way to do that, in writing. And doing it in writing will help you do it in a much clearer, more concise, and more efficient fashion. Whenever you need to thoroughly brief a group of people on an important matter, in writing, consider using a 5-paragraph format. What follows is a format that is being used by many armed forces; in NATO parlance it's called the 5-paragraph field order. Situation Mission Execution Logistics Command and Signal Now I'm generally not a fan of applying military thinking to civilian life, but in this case it's actually something that can very much be applied to professional communications, with some rather minor modifications: Situation Objective Plan Logistics Communications Let's break these down in a little detail: Situation is about what position we're in, and why we set out to do what we want to do. You can break this down into three sub-points, like the customer's situation, the situation of your own company, any extra help that is available, and the current market. Objective is what we want to achieve. Plan is how we want to achieve it. Logistics is about what budget and resources are available, and how they are used. Communications is about how you'll be coordinating among yourselves and with others in order to achieve your goal. What if that's too formal? Sometimes these headings may look overly formal. People who are not used to that format might actually find it off-putting. You can totally use more colloquial headings, to make your communication less formal. For example: Why am I contacting you? What do we want to achieve? How are we going to do that? What will we need? How will we communicate? You'll quickly notice that these map precisely to the concepts of situation, objective, plan, logistics, and communications. But they sound much more casual and informal and approachable. Updates Sometimes you need to convey updates to your plan. Then, it's often not necessary to redo the whole 5 paragraphs. Instead, you just leave out the bits that are unchanged, compared to your previous plan. However, it's always a good idea to include the following 3 paragraphs: Current Situation Current Objective Updated Plan And the reason for this is easy to explain. There are really only two reasons why you would update your plan: either because the situation is different (the circumstances have changed), or the objective has been modified. And people should know which of the two it is. Meeting notes It's a very common misconception for people to think that meeting notes are for the people that were in the meeting, so they can remember what was said. News flash: if you have people in the meeting that can't remember what was said, maybe they didn't actually participate and shouldn't have been there in the first place? So, who do we write meeting notes for? For the people that weren't in the meeting. Every meeting needs notes and a summary, and you need to circulate these notes not only with everyone who attended the meeting, but with everyone who has a need-to-know. And the way you write them is so a person wo wasn't there, or may not have known of the meeting, or perhaps even wasn't part of the organization at the time of the meeting, can understand what was said, what was discussed, and what was decided. There is one person that I can guarantee is never in a meeting you are attending today, and that is you, but six months from now. Looking back at the notes of a meeting that you participated in, six months after the fact, is a terrible experience if the meeting notes are not good. Writing good meeting notes today means making your life, six months from now, easier. It follows logically, from the requirement that people who were never in the meeting must be able to read the meeting notes and understand everything that they need to know about the meeting, that only the written word counts. What's not on the written record in a meeting did not happen. Who's responsible for meeting notes? The meeting chair (the person that called the meeting). Who pulls the meeting notes together? The scribe (the appointed note-taker). Who writes the meeting notes? Everyone. Meeting notes structure In terms of what you want to include in your meeting notes, here's a reasonably useful structure. Meeting title Date, time, attendees Summary Discussion points (tabular) Action items Section 4 would be the longest, and should indeed list everything that was discussed in the meeting. It is usually practical to organise the individual points of discussion in a table. If you keep your action items in issue tracker references, you may also include those in the table together with the corresponding items you discussed. At that point, there is no need for a separate section 5. And now… the No.1 Awesome Writing Superpower Here it comes, the No.1 superpower that will improve all your writing, every time. Are you ready? Read things out loud. For real. I am dead serious about this one. It's pretty safe to say that nothing will improve your writing as much as getting into the habit of reading things out loud as you edit. There are a number of reasons for this: You can read silently at about 250 words per minute, but you can speak at only about 150. Reading things out loud deliberately slows down your reading, and gives you a better chance to spot your own errors. Forcing yourself to recite what you wrote will make it easy to detect run-on sentences, confusing phrases, and errors in flow. You will naturally want to correct these, and it will make your writing better. Making time to read what you write simply means your mind will stay on topic longer than when you just bash something out and immediately send or publish it. This additional time gives you the opportunity to spot more mistakes, detect errors in your thinking, and have better ideas . Do not squander all those opportunities. Read things out loud. Guess what I did with this article.","tags":"presentations","url":"resources/presentations/devopsdays-berlin-2022/","loc":"resources/presentations/devopsdays-berlin-2022/"},{"title":"Jammy, don't snap at me!","text":"The current Ubuntu LTS release, 22.04 \"Jammy Jellyfish\", does not install a Debian package for Mozilla Firefox anymore. Instead, Ubuntu now delivers Firefox as a snap . I'm not particularly enthralled by that idea. Every once in a while I look at the current state of snaps. And every time I look them, I find that they don't solve any problems I am having at the time, but do add some. The same, incidentally, happens to be true for Wayland, which is why I still use X.org. (I want to emphasize that the foregoing is true for me — your own experience may well differ, and that's perfectly okay.) So I have kept my systems free of snapd , and I intend to keep them that way for the foreseeable future. However, if you upgrade an existing Ubuntu Focal or Impish system to Jammy in-place, with the customary apt dist-upgrade command, Ubuntu replaces the pre-existing Debian ( .deb ) package with a snap. That is to say, firefox in Ubuntu Jammy is a transitional package that would install snapd as a dependency, and then run snap install firefox . Mid-upgrade, it does pause and prompt you about this fact — but there's no yes or no that would give you the option to bail, only an \"OK\" button. What you thus want to do if you're wired like me, prior to commencing your upgrade, is tell Ubuntu that you want to keep installing Firefox from a package. And while you're at it, you might also politely inform your package manager that you have no desire to use snaps, at all. To do so, first become root , and make the necessary changes to change the focal or impish references in your /etc/apt/sources.list and /etc/apt/sources.list.d files to jammy as you normally would. Then, make sure that you don't have the snapd package installed: # dpkg -l snapd Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-==============-============-============-================================= un snapd <none> <none> (no description available) Next, mark the snapd package with hold , so that the current state of the package ( un ) is made permanent: apt-mark hold snapd Now, add the mozillateam PPA: add-apt-repository ppa:mozillateam/ppa Next, create a file named /etc/apt/preferences.d/mozilla-firefox , containing the following three lines: Package : * Pin : release o=LP-PPA-mozillateam Pin-Priority : 1001 At this stage, your system should be set up to (a) not install the snap daemon, and (b) conduct the upgrade of the firefox package using the regular Debian package as it appears in the PPA, not the distro package that is a wrapper around snap install . Now, proceed with: apt dist-upgrade Happy jamming!","tags":"hints-and-kinks","url":"resources/hints-and-kinks/jammy-dont-snap/","loc":"resources/hints-and-kinks/jammy-dont-snap/"},{"title":"Something interesting happened at work the other week...","text":"I am sure that many of you have been summoned to your fair share of quarterly, end-of-half-year, or end-of-year updates which were held by way of a company meeting. You probably didn't like them too much, maybe even dreaded the thought of 45 minutes (or longer! 😱) of Death By PowerPoint. But perhaps you didn't mind turning up because hey, at least it was free coffee and donuts. When the pandemic rolled around, many organizations turned that quarterly all-hands summons into a quarterly all-hands virtual summons, in the shape of a video call. Now you had to supply your own self-funded donuts, and probably didn't like the idea of staring into a camera for 45 minutes straight (or longer! 😖🔨). Worse still, I hear that some of you might be have been subjected to virtual pre-recorded events, where you could watch an executive pontificate from a script, with no interactivity whatsoever. Something, I imagine, that is approximately as engaging as watching a wannabe investment advisor on a YouTube channel with 6 subscribers, 4 of them relatives. Last week, our CEO Jim (who, to his credit, previously ran these events in a reasonably engaging fashion) did something different. At the end of our second quarter, and a few days before going on annual leave, he sat down and wrote an email. As in, he actually sat down and wrote an email. It had structure, it had a format, it had clear messaging, it was to the point. What was in the message? I'm obviously not going to go into the detailed content of the email, but here's what stood out: The message started with an intro that clearly settled the reader's expectations: this is what I'm writing about; this is what you'll be reading. It clearly delineated that some of the items discussed were going to be somewhat negative (as in, worthy of improvement); many others were positive. It also clearly established that the negatives were coming first, and the positives thereafter. 1 It used paragraphs, which were clearly separated by topic. As you read, you knew exactly where one thought concluded and the next one started. It conveyed an obviously personal viewpoint: Jim was giving his own perspective on things, rather than writing like a detached omniscient narrator. It was very evident that Jim hadn't just bashed out the message and sent it off in a hurry, but that some careful re-reading and editing went into it. I have no idea if he did this by himself or asked someone else to go over it with him, but that does not matter: what matters is that he edited, not whether he was his own editor. 2 What else is good about this? Best of all, the whole thing was a remarkable exercise in efficiency. Jim put his thoughts into precisely 1,500 words. 3 As I've pointed out elsewhere , information you can express in 1,500 concisely written words is at the top end of what you can convey in a 60-minute verbal meeting — but when put in writing, it takes a fluent English speaker only about 6 minutes to read. How does this compare to the conventional meeting-based approach? Had Jim prepared slides and a speech for an all-hands meeting, it would have taken him at least two hours, plus the hour of conducting the meeting. He probably spent the same total of three hours writing out, rereading, and editing his message. So the total effort he had to spend on his email was about the same he would have needed to spend on the preparation and the conduct of a meeting. For everyone else, that is the other 98% of the company, reading the email took one-tenth of the time that participating in a meeting conveying the same information would have taken. Jim gave back 90% of the productivity that would have been spent in a meeting, to 98% of the company. Makes sense, doesn't it? Yes it eminently does. So, next time you consider summoning your whole company to an all-hands meeting or video call, try to be a little more like Jim. This is an excellent move on two counts: first, readers naturally perceive what comes last as being emphasised. By getting the negatives out of the way first, the positives stick in people's minds more strongly. Second, it establishes that once you get to the positives, there is no further downer coming to sucker-punch you. ↩ A trusted editor is a gift from god (if you believe in such things). I can highly recommend asking someone you enjoy working with to lend you an extra pair of eyes to go over what you write. I was mutual editors with Elena Lindqvist for two years; it was glorious. ↩ I am told this exact round-number precision was coincidence. ↩","tags":"blog","url":"blog/2022/07/16/something-interesting/","loc":"blog/2022/07/16/something-interesting/"},{"title":"Python package dependency checking in a CI pipeline with pipdeptree","text":"Recently at work we ran into rather strange-looking errors that broke some functionality we depend on. In an application run from a CI-built container image, we were seeing pkg_resources.ContextualVersionConflict errors indicating that one of our packages could not find a matching installed version of protobuf . Specifically, that package wanted protobuf<4 installed, but the installed version of the protobuf package was 4.21.1. This was somewhat puzzling: all Python packages in the image were installed with pip , and the packages' requirements ought to have been in good shape. We found another dependency that did specify protobuf<5 , but taken together pip should surely resolve that into a 3.x version of protobuf , in order to satisfy both the protobuf<4 requirement from one package, and the protobuf<5 one from another? To visualize and test such dependencies, the pipdeptree utility comes in quite handy. So, I hacked up a couple of minimal tox testenvs: [testenv:pipdeptree] deps = pipdeptree commands = pipdeptree -w fail [testenv:pipdeptree-requirements] deps = -rrequirements.txt pipdeptree commands = pipdeptree -w fail The first one, pipdeptree , merely installs the package being built, obeying the install_requires list in its setup.py file. This is the \"minimal\" installation. The second one, pipdeptree-requirements , runs a full installation, pulling in everything needed from the requirements.txt file. pipdeptree generates warnings on potential version conflicts between dependent packages. So, in both testenvs, we run pipdeptree in -w fail mode, which turns all warnings into errors that fail the testenv. So now, having added tox to both our CI and our local Git hooks , we can run these checks locally and from GitHub Actions, and they should both fail and thereby expose our package dependency bug, right? Well, here is where it got weird. Because if I ran that locally, on my Ubuntu Focal development laptop, I got: - protobuf [required: >=3.15.0,<4.0.0dev, installed: 4.21.1] - protobuf [required: >=3.15.0,<5.0.0dev, installed: 4.21.1] This is \"bad\" in the sense that it's the wrong protobuf version, but good in that it exposes the bug that we're trying to fix. Progress! However, running the same thing from our GitHub Actions workflow, there's this: - protobuf [required: >=3.15.0,<4.0.0dev, installed: 3.20.1] - protobuf [required: >=3.15.0,<5.0.0dev, installed: 3.20.1] So here, in GitHub Actions, we see a protobuf version being installed that doesn't break anything, but it also means that our test doesn't expose our bug, which is a problem! I'll spare you the details of finding this out, but it turned out that this is actually a pip problem. pip 20.0.2 (which is the version you get when you run apt install python3-pip on Ubuntu Focal) has the dependency resolution error, which results in a protobuf package that is \"too new\". If you install with pip version 21 or later, you get a protobuf that is \"old enough\" to make all installed packages happy. So, how do we test that? There is a package called tox-pip-version that comes in very handy here, in that it allows you to set an environment variable, TOX_PIP_VERSION , instructing tox what pip version it should use in order to install packages into testenvs. This you can use from a GitHub Actions jobs definition, making use of a matrix strategy: jobs : build : runs-on : ubuntu-latest strategy : matrix : python-version : - 3.8 - 3.9 pip-version : - 20.0.2 - 22.0.4 steps : - name : Check out code uses : actions/checkout@v1 - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v2 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | pip install tox tox-gh-actions tox-pip-version - env : TOX_PIP_VERSION : ${{ matrix.pip-version }} name : Test with tox (pip ${{ matrix.pip-version }}) run : tox What this does is it sets up a 2×2 matrix: run with Python 3.8 and Python 3.9, and for both those Python versions run with pip 20.0.2 and 22.0.4 (these happen to be the two versions that we're interested in). That way, we were able to expose the package dependency bug, and then fix it. The test now serves as a regression test, to make sure we don't run into a similar issue again. If you're curious, the full PR discussion with additional context is on GitHub .","tags":"hints-and-kinks","url":"resources/hints-and-kinks/pipdeptree-ci/","loc":"resources/hints-and-kinks/pipdeptree-ci/"},{"title":"Writing Professionally (C!Conf 2022)","text":"At work we have an annual 1 conference, C!Conf, where we pull the whole company together for 3 days of talks and workshops. All conference sessions are conducted by employees — we don't bring in any external consultants or (deity forbid) \"motivational speakers\", so it tends to be a very productive affair. This year I ran a 45-minute session on writing in a professional context. There is no recording, but the whole talk is available with my full speaker notes at https://fghaas.github.io/writing-professionally . It is available under a CC BY-SA license, as usual. For obvious reasons, we didn't have this event in 2020 and 2021, so this was actually the first we did in 3 years. But the intention is for it to be an annual occurrence. ↩","tags":"presentations","url":"resources/presentations/c-conf2022/","loc":"resources/presentations/c-conf2022/"},{"title":"Batch-processing stereograms with StereoscoPy","text":"I have two methods of taking stereoscopic images, both of which I use regularly: The \"left foot, right foot\" method, which I can do with any camera, including that on my smart phone (I covered this method at length in my 2021 FrOSCon talk ). This is the method I prefer when doing stereograms of landscapes, buildings, statues and such like, and also what works very well for posed stereo portraits. My Loreo stereoscopic lens , shown in the picture above attached to my camera. Either way, I need to post-process my images to get cross-view stereograms like the one you're seeing here. In the former case the need is obvious: I start with two images and need to make them into one stereogram. In the latter case, it's perhaps less so: my stereo lens obviously already produces a stereogram, but it's a wall-eyed one (which I'm not particularly good at viewing), and it has an area in the centre of the frame where the two images slightly overlap. I have found this area to be about 6% of the total width of the image. So that means that what I need to do, starting with the original stereo image, is this: Split the original image into two halves. Cut off 3% on the left and right of each image — on one side, that crop removes the overlap; on the other, it restores symmetry. Swap the sides of the image: the originally left side goes right, the right side goes left. For easier viewing, add a divider, and a border. What comes in very handy here is a neat little tool: StereoscoPy is a small Python library and CLI that is helpful in batch-processing stereo images. In combination with convert from ImageMagick , this enables me to batch-process a whole folder of stereo images into something that is much more suitable for general consumption than the original images that the Loreo lens produces. #!/bin/bash # Set the border/divider width, in pixels BORDER = 40 for f in *.JPG ; do # Grab the file name, sans extension name = ${ f /.JPG/ } # If the cross-view stereogram already exists, # skip to the next original image if [ -e stereo/ ${ name } -cross.jpg ] ; then continue fi # Convert the wall-eyed stereogram foo.JPG # into foo-0.jpg (left) and foo-1.jpg (right) convert $f -crop 50 %x100% stereo/ \" ${ f %.JPG } \" .jpg # Create a cross-view image, with auto-alignment, # that crops 3% off each side of the image, auto-aligns, # and creates a border and divider StereoscoPy -x -A \\ --div $BORDER --border $BORDER \\ -C 3 % 0 3 % 0 \\ stereo/ ${ name } -0.jpg stereo/ ${ name } -1.jpg \\ stereo/ ${ name } -cross.jpg # Remove the intermediate images rm stereo/ ${ name } -0.jpg stereo/ ${ name } -1.jpg done Maybe I'll eventually get round to submitting a patch to StereoscoPy itself, so that the pre-processing step with convert is no longer necessary and the little script above becomes an actual one-liner. But for now this works okay for me.","tags":"hints-and-kinks","url":"resources/hints-and-kinks/batch-process-stereo/","loc":"resources/hints-and-kinks/batch-process-stereo/"},{"title":"My experiment: an interim update","text":"My social media experiment is still ongoing. A week without posting articles on social media, while asking people to subscribe my RSS feed, has yielded practically zero engagement on Twitter (except for the article on Drizzle article, which was reviewed by other people and then shared by my reviewers), very much non-zero engagement on Mastodon (where some of my followers have apparently been subscribers to my RSS feed for quite a while). This doesn't surprise me much: evidently, the habit of subscribing to feed sources via RSS comes more naturally to citizens of the Fediverse. One nice upside of the experiment is that I have been enjoying content written by others more. My aggregation of about 20-or-so feeds, followed via Aggregator , gives me about 1 to two notifications new post per day, and most of those I find truly enjoyable and insightful. I will now progress to phase II in my experiment which is to throw new posts out via RSS and Mastodon, but not on Twitter.","tags":"blog","url":"blog/2022/05/14/experiment-update/","loc":"blog/2022/05/14/experiment-update/"},{"title":"Running a solar-powered laptop","text":"This piece of kit has been a conversation starter everywhere I take it out, so I figured it could use a short writeup. In 2020 I purchased a Pinebook Pro laptop. I had wanted a low-power ARM laptop for a while, the PBP came in a tolerable size (this is a 14\" screen; about the top end of acceptable screen sizes for me), and it was an absolute steal. Including shipping and import duty — my device shipped from Hong Kong — I got mine for €277 all told. 1 Now if you haven't heard of the Pinebook Pro, or for that matter of the PINE64 community, you should check out their web site . They make a bunch of really neat devices, though I can only speak to the Pinebook Pro as that's the only one of their devices I've ever owned. Obviously, the device's claim to fame is its low power profile. Thus it should come at no surprise that its charging input voltage is a USB-typical 5V, like you know from your phone. Now the PBP comes with a separate barrel-plug charging port, but most of the time I just charge it via it USB-C. This I do primarily for convenience; it's simply one fewer piece of kit to carry around. I can thus charge the PBP with a standard wall-socket USB charger, a USB power bank, or any other USB power source. 2 Which is where the solar panel comes into play. Mine is a 28W charger from BigBlue . Now, please don't mistake me for an authority on solar panels; there may be better or more efficient ones on the market — I just found this one useful and compact enough for my liking. Nominally, this panel's maximum amperage is 4.8A, but I've never seen it actually generate that. Under optimal conditions where I live (at 48°N latitude), that is direct sunlight around solar noon on a cloudless day, I can get just under 3A out of the panel in total. Out of this, the maximum output of a single port is 2.4A, so that's my maximum solar charge current for the PBP. Overall, for the PBP's power consumption this is generally perfectly fine. I can work under a sunny or partly cloudy sky for the whole day if I want to. I've also found the display contrast to be sufficient even in full sunlight. I do use a light GNOME theme for my desktop settings, but I don't need to enable the high-contrast accessibility features. It's not advisable to work with the whole laptop exposed to full sunlight, though, as the black device body does absorb a fair bit of heat. If you're sitting outside with a light breeze going, that mitigates this problem. Of course, sitting in the shade with just the panel exposed to the sun is the most preferable setup overall. In terms of software running on the device, I never particularly warmed to the idea of running Manjaro (which the PBP ships with by default), so I run armbian with Ubuntu. I'm not a big fan of Cinnamon or XFCE either, but that's no big issue: I just started with the Ubuntu Focal XFCE image, and then installed the vanilla-gnome-desktop metapackage and subsequently removed xfce4* . Overall the Ubuntu aarch64 port works very well on this device with the armbian Linux kernel (currently, that's 5.9.14), with a couple of small caveats: Suspend support is essentially limited to suspend-to-idle. I'd really love to have suspend-to-disk support on this device (ideally in combination with encrypted swap, which by itself works fine), but neither that nor suspend-to-ram are currently reliable. Even suspend-to-idle is sometimes unreliable and requires that I restart gdm after resuming. Some packages just behave oddly, or don't function at all. For example, ykcs11 just won't want to accept my PIN when I try to hook my Yubikey up with ssh-agent . Most PPAs don't build with aarch64 support. Thus, if you like to run Ubuntu with a bunch of packages that are not in Ubuntu proper, you might have a hard time with the PBP. The PBP's SoC maxes out at 4GiB RAM, which means you shouldn't be using the PBP for video editing or gaming or any other RAM-intensive activities. Even the GIMP runs out of steam pretty quickly at about 3 or 4 concurrently opened images. 3 So can I use this as my daily driver? Yes, with some minor drawbacks. But those I can work around fairly well. If the PBP becomes available for order in Europe via pine64.eu , then — if you are an EU resident — shipping should be faster and you wouldn't need to pay import duty. At the time of writing, however, the PBP can only be purchased from the main pine64.com store. ↩ The device cannot charge over the barrel port and USB-C simultaneously. ↩ Note that I can get cloud computing capacity for cheap at work, so if I need more RAM for something I can get it in a pinch — I am aware that that option is not available to everyone. ↩","tags":"hints-and-kinks","url":"resources/hints-and-kinks/solar-powered-laptop/","loc":"resources/hints-and-kinks/solar-powered-laptop/"},{"title":"Drizzle: the most influential software project you've (probably) never heard of","text":"Drizzle was an open-source project 1 that, for all intents and purposes, died in 2016. Its project web site is now defunct, and the most recent snapshot from the Wayback Machine is that of September 2, 2016 . In July of that year, Stewart Smith (one of the project's core team) announced on the project mailing list that neither he nor any other core team members had time to dedicate to Drizzle anymore. Prior to that, the project had been mostly dormant since 2012 2 , having been founded in 2008. So it was properly \"active\" for just 4 years, and then in limbo for 4 more before finally wrapping up. Chances are, you've probably never run a Drizzle database server in production, and quite possibly never spun one up for any purpose either. And yet, if you're an open source software developer, you're probably using something, every single day, that came out of Drizzle. And that something isn't even software. Drizzle's history, a very brief summary Drizzle started as an attempt to refactor MySQL, and was originally driven by Brian Aker, together with a small team of engineers at Sun (which had then-recently acquired MySQL), in the first half of 2008. A skunk works project that flew under the radar — to put it charitably — at Sun, Drizzle was publicly announced at O'Reilly OSCON of that year. There are a couple of videos floating around from that event ( from the keynotes , and from a booth presentation ) that are both… well, go and see for yourself. The aforementioned Stewart Smith did a very entertaining talk at linux.conf.au some five years later that covers those events, which you can watch from the official Linux Australia mirror , or from a YouTube upload . There's also an interesting old blog post from MySQL co-founder Monty Widenius, written in late July of 2008 , which outlines the state of affairs at the time. Of course, in 2010 Oracle acquired Sun (and with it, the MySQL database) — and Oracle was presumably less than keen on having an in-house fork of the database technology it had just acquired. Thus, the Drizzle engineers found a new home at Rackspace, with the goal of getting Drizzle to a production-ready release. That sort of happened, and the Drizzle package even got into Debian, but after the Drizzle 7.1 release in 2012 , adoption did not exactly skyrocket. Development on Drizzle stagnated and eventually petered out. The 7.2 release branch never made it out of the alpha stage . Today, to the best of my knowledge, you can't install a Drizzle package on any contemporary operating system. There is no official Drizzle container image on Docker Hub , no DBaaS offering based on Drizzle, nothing. But Drizzle left a very important legacy. What did Drizzle do differently? In 2008, it was already common for open source software to live in public version-controlled repositories. But far from all of them used Git , like the vast majority do today: some used CVS or Subversion , some used Mercurial , and the Launchpad platform (which Drizzle lived on) used Bazaar . But most of them did have one thing in common, which is how changes landed in the tree. You had a small group of \"core committers\", who had write access to the \"official\" code repository. They could (and would) push changes to the codebase on their own volition and authority. In smaller projects, the core committers \"group\" might be just one person. If someone outside the core committers group wanted to make a contribution, they had to convince a core committer to merge it. Sometimes (though quite rarely at the time), projects had some form of scripted unit testing — typically implemented with the then-popular Hudson server, which was subsequently forked to become Jenkins . But such unit tests would be seen as merely advisory: breaking unit tests didn't necessarily mean that a patch couldn't land, specifically if the patch originated with a core committer. Unit tests would also not necessarily run automatically when a patch was submitted, they might instead run only if specifically kicked off by a core committer. The Drizzle team, as Brian put it in a talk I recall attending (though not exactly when and where), \"took commit rights away from everybody.\" That meant that nobody could push changes directly to a central repository, and everything had to flow through CI tests. The process generally went like this: You submitted a patch to Drizzle, implementing a new feature. Immediately after your submission, an automated process (in Hudson, later Jenkins) would automatically run its complete suite of unit tests against the current code base, with your patch applied. Your patch would perhaps break an existing regression test. You would immediately be notified of the failure, giving you a chance to fix the problem that your change introduced. You submitted a new version of the patch, which would now pass the test suite. Humans would now review your patch. They would no longer have to worry that your patch broke anything pre-existing (a common question in patch reviews in many contemporary projects), and could instead focus on the merit of your feature addition. If your reviewers determined that your new feature should come with additional tests (and they usually should), they would recommend you implement a test for your new feature. You would then resubmit your patch with the added testing functionality, and — assuming everyone was happy with the implementation — your reviewers would give the go-ahead to merge your patch. At this stage of course, the rest of the codebase might have changed: some other patches might have landed before yours. So, the entire pipeline — including tests that predated your patch, the new tests your patch introduced, and the new tests that other patches might have added in the interim — would re-run with the current state of the codebase with your patch applied. If your patch broke things now, you would be asked to fix them once more. However, if your change didn't break anything even now, then there would be no human blocking the merge anymore: as soon as the tests passed, the thing that ran the tests (I don't recall if in 2008 we already had the term \"CI pipeline\" for that thing) would merge the patch on your behalf. Much of this automation was brand new innovation at the time, largely due to the work of Drizzle developer Monty Taylor — who later went on to becoming a highly influential engineer in other projects, which (among many other things) landed him a profile in WIRED in 2013 . The Drizzle team also was pretty diligent about what they considered \"breaking things:\" for example, the Drizzle test suite contained several performance benchmarks. If a patch made the server perform worse, i.e. introduced a performance regression, that would be treated the same as a functional regression. So you not only would be unable to land a patch that actually broke functionality or made the database server eat data; you would also be unable to land a patch that made the server slower. The Drizzle team is also where, to the best of my knowledge, a coinage for this kind of approach originated: \"gated commits\", or \"gating\" in general. How is this relevant? A substantial fraction on the Drizzle core team — which had moved to Rackspace in 2010 — was instrumental in launching another project that came out of that company (and NASA) that same year: OpenStack . And OpenStack took the gating approach from its humble beginnings with Drizzle to an absolutely massive scale in its hype years (2011 – 2015 or thereabouts) — so much so that it established a new default in collaborative software projects. Many other projects that launched in that timeframe (including Kubernetes and Terraform ) adopted this approach as well. Today, having automated CI testing on every submitted patch is considered par for the course in a collaborative software project. GitLab CI and GitHub Actions workflows have made these much more accessible than they used to be with Hudson and Jenkins. It's also exceedingly common to do detailed collaborative reviews in a public forum before merging — GitHub's PR review workflow is ever more closely approaching the Gerrit review workflow that OpenStack uses. GitHub's auto-merge functionality (which lands patches automatically once they have passed both automated unit tests and human review) is more or less a direct copy of the automated merge found in OpenStack's development workflow , which itself can be directly traced back to Drizzle's review process. And all these things are found in open source software projects across all sorts of communities. Kubernetes, Terraform, Django , CPython , Open edX — you name it, it probably uses an approach first pioneered in Drizzle. And that's the real lasting legacy of a project that few people even remember by name. Who do we owe this to? I know some of the Drizzle developers personally, though certainly not all. What follows is an incomplete list of people you can buy a meal or a drink if you run into them, and you like the way you collaboratively develop software today: Brian Aker Mark Atwood Aeva Black Patrick Crews Eric Day Patrick Galbraith Andrew Hutchings Jay Pipes David Shrewsbury Stewart Smith Pádraig O'Sullivan Monty Taylor Acknowledgements Stewart Smith and Mark Atwood kindly reviewed this article and provided valuable feedback on it. Thanks to both of you! All errors and omissions are of course mine, and mine alone. Also, though I've been meaning to write something like this post for a while, it was ultimately a Mastodon thread by Julia Ferraioli that became my writing prompt. Thanks for that, too! Disclaimer: I was never a part of the Drizzle project in any role, which for the purposes of this article is probably a good thing as I am not talking about personal accomplishments or failures, in other words I have no skin in the game. This article also does not contain any information about the Drizzle project except that which was available via public channels at the time, or has become public since. ↩ The project did participate in Google Summer of Code in 2013, which is what the last tweets on the project's Twitter account are about. But the project's development branch had its last alpha release in September 2012 . ↩","tags":"blog","url":"blog/2022/05/10/drizzle/","loc":"blog/2022/05/10/drizzle/"},{"title":"Sweet & savoury stir fry","text":"This is inspired by a dish that goes by \"Mongolian beef\" in parts of the U.S., but I opted for the generic title since it's neither Mongolian, nor does it require beef. It works with any red meat, but you can also leave the meat out altogether, at which point this becomes a vegan dish. I normally serve this with jasmine or basmati rice. I have yet to try it with udon , which I am guessing should work well too. Ingredients Amounts are for 4 servings. Meat (optional): About 400g of red meat (beef flank steak, or leg of lamb) 3 tbsp soy sauce 1 tbsp rice vinegar (or Shaoxing wine , for a finer flavor) 1 tbsp sesame oil 3-4 tbsp cornstarch or potato starch 4-5 tbsp peanut oil (for frying) Vegetables: 2 tbsp peanut oil (for frying) 4 cloves garlic, finely chopped 1-2 red bell peppers, cut into 2cm squares 1 hot chili pepper, seeds removed (alternatively a jalapeño pepper, if you like it milder), chopped 180g (drained) bamboo shoots 5-6 scallions (spring onions), cut diagonally into 2-3cm pieces Sauce: 2 tbsp granulated sugar 3 tbsp soy sauce 2 tbsp hoisin sauce 2 tbsp sesame seeds 1 tbsp cornstarch or potato starch 1 tbsp rice vinegar (optional) Equipment 1 small bowl (for sauce) 1 medium-size bowl (for meat) Cooking knife and board Whisk Large skillet or wok Method Prepare the sauce: whisk sugar, soy sauce, hoisin sauce, sesame seeds, and optionally rice vinegar together in a small bowl and set aside. If you're including the meat: prepare the marinade by whisking all liquid ingredients together in a medium bowl. Cut the meat into very thin strips and put them in the bowl. Marinate for at least 20 minutes. Toss the meat in the cornstarch or potato starch. Heat peanut oil in the skillet or wok on high heat. Fry the meat in batches until the starch turns golden brown, about 3 minutes or so per batch, keeping heat on high. Put fried meat aside in a bowl. Vegetable fry: heat peanut oil in wok or skillet on high heat. Sauté garlic until fragrant but not brown. Add bell peppers, chili pepper, and bamboo shoots. Continue frying on high heat for about a minute. Add the cut scallions, mix thoroughly. Cover skillet or wok with a lid, and cook vegetables for about 5 minutes in their own steam. Taste the scallions. If they've lost their onion punch and developed a slightly sweet taste but still retain some crunch, they're perfect. If going for the meat option, return the fried meat to the wok or skillet. Pour the sauce prepared in step 1 over everthing, mix thoroughly, and keep cooking for another 30-60 seconds until sauce thickens. Serve. Nutrition facts No warranty of any kind on these. Values are per serving. With meat: Calories (kcal) 561 Total fat (g) 34.9 Saturated fat (g) 7.8 Total carbohydrates (g) 29.4 Sugars (g) 13.6 Protein (g) 32.8 Without meat: Calories (kcal) 223 Total fat (g) 13.0 Saturated fat (g) 2.1 Total carbohydrates (g) 24.8 Sugars (g) 13.4 Protein (g) 4.2","tags":"blog","url":"blog/2022/05/08/sweet-savoury-stir-fry/","loc":"blog/2022/05/08/sweet-savoury-stir-fry/"},{"title":"Entropy, management, and xkcd 927","text":"xkcd 927 is a modern internet classic that is frequently brought up in conversations to remind people that a proposal that they're making will, while being intended to simplify things, actually make them more complicated. Most people quote that strip to satirize or even ridicule the idea of introducing a \"15th standard\", as if the natural order of things was simplification. Such people are frequently baffled by the amount of cruft and clutter that accumulates over time in an organization they work in, and some of them embark on a constant — perhaps career-long — quest of \"streamlining,\" \"process optimization,\" or \"reducing technical debt.\" If you are one such person, please get ready for some bad news. As far as we know, there are three fundamental theories that, combined, explain the universe as we know it: general relativity , quantum mechanics , and thermodynamics . Thermodynamics has a famous Second Law that can be stated in various ways — in one modern and simplified form, we say: The total entropy of a system never decreases. \"Entropy,\" in this context, is essentially the degree to which the system is disorderly . In effect, the Second Law states that any system can stay just as orderly as it is now, or it can become more disorderly, but in can never again become as orderly as it once was. The normal state of the world is that things keep getting more and more disorderly. There are multiple classic examples of this: you can mix two paints in a bucket but cannot unmix them, you can open a container of gas in a vacuum chamber and the gas will disperse but never go back into the container, you can scramble and cook and egg but never return it to its original protein structure. Sometimes the growth in entropy isn't noticeable: you can of course pick up your cluttered desk and put everything neatly away in boxes or drawers (or the trash), and your office will look nice and clean and uncluttered thereafter. But, in the process you will have turned so much of your body's energy into heat that the overall disorder in the \"system\" (consisting of the things in your office, the room, you, all the gas molecules in the air, and so forth) will have gone up quite considerably. Now, I realize that not all laws of physics can be directly applied on a macro scale, that is, to organizations, families, or societies. For example, you'll have to go through various mind-bends to imagine your life as a path through gazillions of Everettian many-worlds bifurcations . But I'd posit that the constant growth of entropy is indeed rather fundamental — after all, growth in entropy is one of our best definitions of the passage of time . Escaping the growth of entropy is literally just as impossible as stopping time. What does that mean for each of us, individually? It means, bluntly speaking, that our lives get objectively and perpetually messier over time. I don't know if you're in a better or worse place than you were 10 or 20 years ago in your life, but I'm pretty sure that you're in a more complicated place now. And many of us might probably want to go back to our less-complicated life from back then, but alas, backwards time travel (and hence entropy reduction, read: \"a more orderly life\") is not an option. Now as long as you're just trying (and failing) to rewind disorder in your own life, then — as long as you live and work alone — that will probably not have a harmful effect on anyone. But it gets tricky when you're applying the same thinking to living with a spouse, or in a family. Good luck trying to rewind your life with teenage offspring, for example, to the presumably simpler time when they were three month old babies that slept most of the day. But let's also talk about how this affects your work in a management position. If you are a manager, it is your job to slow the growth of disorder in your part of the organization. You won't be able to reduce disorder, and any attempt to do so pits you against a most fundamental law of physics. (Laws of physics are like terrorists: you shouldn't attempt to negotiate with them.) However, many managers are exactly the opposite: they are entropy accelerators; they speed up the growth of disorder in the organization. You can do better than that. 1 Here are a few suggestions you can apply when dealing with your management peers, so you can act as your organization's entropy decelerator. Somebody wants to replace multiple existing things with one new thing (the original xkcd 927 scenario): the only circumstance under which you should agree to this is when you already know for certain that the existing things must go away, within a manageable timeframe. For example, the software solutions that your company has been buying from one vendor have had such a massive price hike that they now break the budget, or the legal ramifications of continuing to use them have become untenable. That's when you have an option of possibly replacing two (or three) things with one. Under all other circumstances, you can hope to replace one thing with one other, at best. Somebody wants to solve a communications issue by adding more channels to your company chat, more categories to your issue tracker, more whatever? That's your cue to stop that dead in its tracks. Opening more lanes of communication never simplifies anything; it always makes things more complicated. Those new chat channels? Tit for tat. They want three new ones, so they must retire three. No, not two. Three. Somebody wants to \"open up team communications\", or \"flatten the organization\", so that everyone's complete graph has way more edges? That's when you educate them about ${n(n-1)}\\over 2$, and what quadratic growth means. Do you notice how a lot of these involve saying \"no\" to someone, and that that may place you at odds with the the well-meaning proponent? Congratulations on your realization that leadership is not a popularity contest among your management colleagues. One word of caution though: even if you fight this good fight — and trust me, it is a good fight — you will still occasionally look back at when you started working in your organization, and realize that despite all your efforts it's a messier place than when you started. Not just the whole organization, but maybe even your own team or whatever your little corner of the corporate world is. The part where you are responsible for your part of the mess. This is especially true if you are just in the middle of leaving an organization (or a role therein), and are reflecting on the impact of your tenure: you might fall for the thought of \"I tried really hard, but things still are messier than when I got here.\" They always will be. The point is not to compare today's degree of disorder to that when you started. The point is to compare how disorderly it is now, to how disorderly it would have been if you hadn't been there. Making a positive contribution to a group in a leadership role is frequently — and somewhat counter-intuitively — achieved by simply focusing on not making things worse for everyone. Canadian astronaut and former ISS commander Chris Hadfield calls this approach \"aiming to be a zero\" and dedicates a whole chapter in his excellent Astronaut's Guide to Life on Earth to this idea. ↩","tags":"blog","url":"blog/2022/05/07/entropy/","loc":"blog/2022/05/07/entropy/"},{"title":"An experiment","text":"A little while back, I posted on my Twitter and Mastodon feeds 1 asking people who kept a personal blog to post their RSS or Atom feed URLs. A surprising number of people responded — many more on Mastodon than on Twitter, incidentally, though I have about 15 times as many Twitter as Mastodon followers. You can find the Twitter thread here and the Mastodon thread here . I now have a very decent aggregated feed, thanks to all who responded! Now, normally, when I put a new post out, I chuck out a link to the post on those two social networks. For some time, I'd like to do something different, and if you're inclined, you can help! So what I'd ask you to do is subscribe to the RSS feed or the Atom feed for my site. For the next few things I post, I will not link them on Twitter or Mastodon — but if you notice them in your feed and find them share-worthy, I would very much appreciate if you posted them there. 2 Feel free to tag me in them, I'm @xahteiwi on Twitter and @xahteiwi@mastodon.social on Mastodon. I'd be really curious to see if it's feasible in 2022 to reach blog readers with essentially just an RSS feed, and word-of-mouth. So, if you would like to participate, then just add my feed to your reader, and keep your eyes peeled for the next few articles. Thank you! If you're curious about these things: I use Renato Cerqueira 's Mastodon Twitter Crossposter to mirror my Twitter feed to Mastodon, and vice versa. You can take a look at its GitHub project . ↩ I don't want to run Google Analytics on this site out of concern for your privacy, which is why I don't know where my traffic comes from. If you post them and tag me, I have something functionally approaching a pingback beacon. ↩","tags":"blog","url":"blog/2022/05/06/experiment/","loc":"blog/2022/05/06/experiment/"},{"title":"This site now lives on GitHub","text":"I have moved this site to GitHub. It's still available under the same URL, of course, but it uses GitHub Pages for hosting. Why did I do this? A few reasons: I don't have a comment facility on this site, and I don't intend to add one, but I do want to give people the ability to submit corrections or sugggestions. You can do that now, by creating a GitHub issue , sending me a pull request, or doing a GitHub edit (which is really just a streamlined way of sending a PR from your browser). It gives me the option to use a GitHub Actions workflow to deploy the site fully automatically. As you may know I build this site with Pelican , and wiring up a workflow that first sets off a Pelican build and then invokes ghp-import (via tox ) was a breeze. You're welcome to take a look at the implementation if you like. (You can also look at the relevant section in the Pelican docs, of course.) Overall, this gives me the ability to do quick edits from almost anywhere, and also gives someone else (like you!) the ability to suggest fixes, which I can then apply almost instantaneously. But please don't expect any such things; I do maintain this site on a \"when I get around to it\" basis. In short: if you've used this site as a regular or irregular visitor/reader, not much will change. If however you wanted to chuck in the occasional fix or correction, you can do that more easily now. If at any point I find that GitHub Pages hosting isn't the right thing to after all, I'll happily rehome the site elsewhere. Please be advised that this is still my site, though, and I maintain editorial control of all content. If you're sending me a PR, please do so with the understanding that I might decline to merge or publish it, for any reason at all. If that's not for you, please use your own platform.","tags":"blog","url":"blog/2022/05/05/moving-to-github/","loc":"blog/2022/05/05/moving-to-github/"},{"title":"The Review Review","text":"I wanted to share a few thoughts on something I consider a rather important topic in our industry: code review and CI/CD tools, and how they relate. This means that I'm talking about source code management: where we store our code, and how we manage access to it; code review: how we coordinate changes to our code; testing and gating: how we make sure that those changes don't break anything; deployment: how we push changes and updates out to the consumers of our code. In case it's not obvious, that means I'm talking about a large fraction of the software engineering cycle. Not all of it; the part involving \"fooling around\" ( creative play ) is perhaps excluded — but substantially everything where people can be said to be \"developing\" in a software engineering organization is encompassed in these things. And there's a few things that follow from that: First, whatever tools we use in order to accomplish these four things, they simultaneously influence and are influenced by our collaboration culture. It's ludicrous to presume that tools and culture are independent of each other, or to categorically declare that tools must be made to fit processes, not the other way around. That's not how people work. Culture and tools always have an influence on each other. Second, the scope of these things is continually expanding as the field evolves. To illustrate, a few years ago a CI/CD platform could get away with supporting automated unit tests and kicking off an Ansible playbook to deploy things to VMs. Today, what we expect out of a continuous deployment pipeline includes support for a package registry (for Python packages or Node.js modules, to give just two examples), a container image registry (for Docker/Podman/OCI containers), a secret store, the ability to deploy to a Kubernetes cluster. And that's just a few examples. I might be forgetting others. Third, this is a classic example of where we must apply systems thinking : since substantially everything the organization does is connected to the toolchain, we cannot make changes to one part of the system without considering the consequences for the system as a whole. That is not to say that we cannot make incremental changes, just that we can't pretend that anything in the system stands alone. To illustrate what I mean, consider the example of an automotive engineer implementing a design change for an engine. If the design change makes the engine so much more efficient that it means a range extension by 10% then that's excellent. But if in the process the designer has made it impossible to connect the engine to its battery (or the fuel line, if we're talking about obsolescent technology), then installing the new engine doesn't just not improve anything — it renders the vehicle immobile. Responsibility Now, what does that mean about responsibility? Who is ultimately in charge of the system consisting of source code management and review tools, and your CI/CD pipeline? The answer is hopefully a no-brainer: since everything I talk about including your organizational culture encompasses substantially all of your engineering organization, the responsibility rests with whoever is in charge of your engineering organization (in most companies, that's often the CTO). And if you're a software technology company so your entire enterprise is substantially a software engineering organization, it's your CEO's or MD's responsibility. Of course, that person may delegate some of the tasks and details of running your source code management and code review and CI/CD platform, but responsibility stays with them. And that responsibility requires both an understanding of the technology itself , and an understanding of how it interacts with your engineering culture. A profound understanding. And I'd go so far as to say if you head up a software engineering organization and you don't have a profound understanding of this toolchain and its mutual influence on your culture, you should find another job. And if you work in a software engineering organization and the person in charge lacks precisely that profound understanding, you should also find another job, because you deserve better. So having said all that, we can start talking about tools. And I'm going to talk about three of them, all of which I use in some professional capacity on an at-least-weekly basis. GitHub The first one is the toolchain that — I think — a majority of open source developers will be most familiar with: GitHub, whose collaboration model is based on the Pull Request (PR). Now the GitHub PR model was strongly influenced by the distributed development model of the Linux kernel. The kernel project is what Git was originally written for, so naturally it is also where the original convention for pull requests emerged. In kernel development, during a kernel merge window, subsystem maintainers fix up a publicly accessible Git tree for Linus to pull from. They then send a message that follows a conventional format to the linux-kernel mailing list (the LKML) outlining the purpose of the changes they want merged. This email contains a summary of the changes, and then an enumeration of each commit to be merged. (There's a git subcommand, git request-pull , to format such a message.) The review then proceeds in an email exchange on LKML. Once Linus is happy with the change, he pulls from the subsystem maintainer's branch and informs them that their changes have merged. Individual subsystem maintainers replicate this model, perhaps with small modifications, for contributions to the subsystems they are responsible for. GitHub Pull Requests (PRs) GitHub replicates some features of the kernel's model: The collaboration model is generally, \"fork and pull\". Individuals maintain their own forks of an upstream codebase, and then send pull requests when they are ready to review. (However, the review process then uses a web interface, rather than a mailing list — in principle, a GitHub reviewer can do a complete review within the GitHub web interface and source code browser and would never even need to check out the repository locally.) Each PR generally consists of multiple commits, which however are expected to closely relate and serve a common purpose. That common purpose is enumerated in a summary at the top of the pull request. GitHub calls this the PR description. Submitters can mark a PR as a draft, with which they indicate that the PR is not ready to be merged yet. When drafts became available in 2019, they replaced an emerging convention in which PR descriptions would be prefixed by WIP (work in progress) or DNM (do not merge) . GitHub PRs can be approved, rejected or commented on by maintainers or other contributors, and an approval can be made a mandatory requirement for merging, but by default GitHub will let anyone merge the PR who has write permissions to the repository that the PR targets. This includes the possibility for a maintainer to merge the contributor's remote branch to their own local checkout, and then pushing the merged branch to he target repo of the PR. Such an event will automatically close the PR and mark it as merged. GitHub Actions GitHub has, for a long time, allowed maintainers to require that PRs pass automated testing. However, until rather recently, it relied on them to run (or interface with) a separate testing infrastructure outside of GitHub to do that. Typical examples for this included CircleCI, or Travis, or Jenkins. It was only in 2019 that GitHub announced automated testing via GitHub Actions. At the time of writing however, GitHub Actions workflows are in widespread use for CI/CD, but it is still quite common for GitHub-hosted projects to allow maintainers to circumvent CI/CD tests and merge directly. When this happens, it often creates a rather unpleasant situation in which CI/CD testing is only run for contributions by \"outsiders\" or \"newbies\", whereas maintainers get to break things with impunity. This means that issues are often not detected until a casual contributor sends a PR, at which point the test breaks and leave the contributor confused (and sometimes lead to the change not even being considered because, well, \"it makes the tests break.\") Another thing that comes bundled with GitHub (and GitHub workflow actions) is the ability to maintain your own package registry and push artifacts to it from your workflow . Interestingly, at the time of writing, GitHub's definition of \"packages\" includes container images, Ruby gems, and npm modules among others , but presently does not include Python modules — although you do, of course, have the option to push your packages to PyPI from your workflow . GitLab The equivalent to the GitHub pull request (PR) is the GitLab merge request (MR) . In principle, a GitLab MR is quite similar to a GitHub PR, albeit with a few noticeable differences: The \"fork and pull\" model is less prevalent on GitLab. Instead, it is far more common for collaborators to work on one project, and then create topic branches within that project for each set of changes. Since the project repo is shared, this facilitates collaboration on a single changeset by multiple people: if two or more people wish to collaborate on a change, they simply push additional squash or fixup commits on the topic branch. They can also agree to force-push amended commits to the topic branch, in which the GitLab web interface will helpfully point out differences between individual versions of a commit (something that GitHub presently cannot do in a PR). As in a GitHub PR, a GitLab MR is generally expected to include one or more commits. Also as in a GitHub PR, an MR is expected to contain a summary that outlines its purpose. GitLab MRs have a Draft status just like GitHub PRs do, and they were introduced about the same time in both products, but GitLab had a preceding feature called work-in-progress MRs (WIP MRs). GitLab has the handy feature that MRs are automatically marked as drafts once any commit with squash: or fixup: in the commit message ends up in the topic branch — GitLab rightfully infers that the branch still needs a squash rebase prior to merge. GitLab MRs can be reviewed in full using the web interface alone: the review interface and the source code browser are closely integrated, just like in GitHub. GitLab CI CI/CD has been an intrinsic part of the GitLab review experience for years, since GitLab includes full CI integration via the .gitlab-ci.yml configuration file. Since GitLab CI has been around for quite a while, and it has a multitude of ways to be used, it \"feels\" more intrinsic to the review process than GitHub Actions do, which to me still leave an impression of being bolted on. In addition, GitLab CI comes with multiple options of using the CI runner: You can use shared runners, which GitLab operates for you. These are Docker containers that GitLab spins up on your behalf in the cloud, and which you share with other GitLab subscription customers. You can also host your own runners. You can do that in Docker containers, in Kubernetes clusters, in virtual machines, and even on bare metal. The runners need no incoming network connectivity; they simply connect to a service on your GitLab host and then poll whether jobs wait for them. You can also specify runners that are exclusive to a project, or to a group or subgroup of projects. GitLab also comes with a package registry , to which you can push packages from CI pipelines. This differs from GitHub in such a way that it includes more package different formats , including a private PyPI workalike for Python packages. In addition, there's also a separate container registry for container images. Gerrit/Zuul Now, it feels a bit awkward to call this one \"Gerrit/Zuul\" when I've called the others just \"GitHub\" and \"GitLab\" respectively, and tacitly included the corresponding CI integrations (GitHub Actions and GitLab CI, respectively) in them. There are a couple of reasons for that: Zuul is a CI/CD framework that is, in principle, not tied to Gerrit, whereas GitHub Actions only apply to GitHub, and GitLab CI only to GitLab. Gerrit/Zuul is a particular combination that was largely popularized by the OpenStack community, which is why a lot of people who are or were part of that community intuitively associate Gerrit with Zuul and vice versa. Likewise, Gerrit is not tied to a specific CI/CD framework. It's perfectly feasible to run code reviews in Gerrit and use a different CI/CD pipeline (or even none at all). And Gerrit/Zuul does differ quite notably from GitHub and GitLab, whose features often map quite closely to each other, and I'd like to highlight some of those differences. Gerrit reviews The Gerrit review process differs in a few crucial points from the one we know from GitHub and GitLab: You don't ask someone to pull from a branch or a fork or yours. Instead, you run git review and Gerrit will make a branch for you. Everything else flows from there. Unlike a GitHub PR and GitLab MR, which both typically contain a series of commits to be taken as a whole, a Gerrit change is really just that: one change. Which, of course, also means that we don't need a separate summary for the change: the summary is the commit message. It's still possible to submit a series of commits in the course of a Gerrit review. However, Gerrit simply sees those as a series of changes that all depend on one another. Dependencies between changes can also be expressed explicitly, by including appropriate keywords in commit messages. Crucially, these dependencies can cross project boundaries. That is to say, a change in one Git repository can depend on a change in another Git repository, so long as they both use the same Gerrit instance for review. And we also have the equivalent of a Draft PR/MR; in Gerrit that's called a work-in-progress change. Because of this, when used in combination with CI such as Zuul, a Gerrit-reviewed project generally expects CI tests to pass on every commit, without exceptions. This is in contrast to many GitHub or GitLab managed projects, which typically only expect the head commit of the topic branch associated with a PR/MR to pass CI. In Gerrit/Zuul managed projects, it's also Zuul that merges the commit. This is also in contrast to projects that live in GitHub or GitLab: in those, the pipeline run results are generally advisory in nature, and a successful pipeline run must still be confirmed by a human clicking a Merge button (or running a git merge command locally, and then pushing to the repository). In addition, even a failing CI run can generally be overridden by a \"core committer\" who has the ability to merge the PR/MR anyway. A Gerrit/Zuul project typically has no such shortcuts, meaning the only way to get changes into the repo is to pass both peer review, and the CI pipeline. In my experience, this tends to create a climate of leadership by example, which has a beneficial effect on both experienced developers (\"seniors\" in a corporate setting) and newcomers (\"juniors\"). Speculative merging There is one other property that Gerrit/Zuul has that sets it apart from other review/CI toolchains: speculative merging. This involves the parallel execution of CI jobs for interdependent changes . With speculative merging, even complex, long-running CI/CD pipelines don't hold up the development process — and this massively enhances project scalability. No direct repo browser integration Notably, in Gerrit/Zuul there is no close integration with repository browsing. Gerrit does include the Gitiles plugin for the purpose, but its user experience is rudimentary at best. A popular alternative is to deploy Gerrit with Gitea , but again, that's not built-in and your trusted Gerrit/Zuul admin has to set it up for you. In addition, while source code browsing in GitHub and GitLab is tightly integrated with project permissions, and that is also true for Gitiles, there is a certain amount of administrative duplication to make your Gerrit repository and project permissions apply to Gitea. No built-in package registries There's another difference in the Gerrit/Zuul stack when compared to GitHub and GitLab, and that is its absence of built-in package registries. Zuul has ready-to-use jobs for pushing to a container registry , or to PyPI , but you do have to either push to upstream public registries, or build your own. Zuul does not come bundled with multitenant private registries the way GitHub and GitLab do. Administrative complexity In view of the above, there's another thing that you might want to consider, which in my humble opinion is an important reason why the Gerrit/Zuul combination has less uptake than it deserves on its technical merits. And this may sound overly dramatic, but: people like to be in charge of their own actions, and software developers are people. And here's an issue with Zuul: there are quite a few things a developer can do on their own in GitHub Actions or GitLab CI that they'd need to ask an admin's help for in Zuul. Creating a relatively standard workflow of building a private container image, pushing it to your own registry, and then rolling out that image to a Kubernetes deployment, is something you can do in GitHub or GitLab as a project owner. With Zuul, you'll need an admin at least to set up and manage your container registry. Rerunning a pipeline, a simple click of a button or API call in GitHub or GitLab, is something you trigger via a Gerrit keyword (typically recheck ) for Zuul — but only on the pipelines where your admin has defined that trigger . So, which one's best? So you want to know which one of these you should choose (or advocate for)? That's surprisingly difficult to answer, and greatly depends on your priorities. And I'll give you this from four angles. When it comes to scalability — the ability to adapt to massive organizational sizes, and/or rapid project growth, or an obscenely large number or projects within an organization — the Gerrit/Zuul combination wins hands down if you have a competent, responsive, and dedicated crew to manage it. When it's about getting started quickly — helping a project get off the ground with a good, usable, easily manageable review and fully integrated CI/CD structure — you can't beat GitLab. In terms of beneficial effect on your development culture, Gerrit/Zuul again probably scores best. If you have a team that's great at reviews and commit and CI and doesn't cut corners, or you want to build a team like that, Gerrit/Zuul can really help. And when it's about giving developers the lowest barrier to entry — meaning using tools that they're most likely already familiar with — GitHub is your platform of choice.","tags":"blog","url":"./blog/2022/01/29/review-review/","loc":"./blog/2022/01/29/review-review/"},{"title":"Scaling the flat organization","text":"There's a common trope in management that goes something like \"in order to better scale the organization as we grow, we need to keep it flat.\" The thrust of the argument is that as the organization grows to meet customer and demand growth (and with it, growth in head count), additional levels of corporate hierarchy stifle that growth, and should thus be avoided. For any knowledge-driven organization this is wrong, and just how wrong it is can be proven, numerically, with simple high school level maths. And in this context, \"knowledge-driven organization\" encompasses any technology company, any software engineering outfit, any technology services provider — in short, any organization that makes its money off the brains of its people. Let's establish a few self-evident facts about knowledge-driven organizations: The people who actually \"make things happen\" are the ones with no direct reports. The frontend designers, the infrastructure engineers, the backend specialists, the data analysts. Their managers (and their managers, and everyone else all the way up to the CEO) are charged with aggregating information, making decisions, removing obstacles to productivity, and perhaps providing some form of vision and guidance. But it's individual, non-managerial contributors of all specializations that actually do things. In doing so, engineers work best in small teams with a great degree of autonomy. They will usually benefit from close working relationships with a small group of people. A manager's role is thus twofold: remove any obstacles that stand in the way of the team accomplishing its goals, and act as an interface to other parts of the organization. An example With that in mind, let's consider a hypothetical small company that is currently structured in teams of 5. There's always 4 people reporting to one manager. Currently, that company is made up as follows: the CEO/founder, Alex (1 person), 4 team leads (4 people), 4 employees on each team, all of whom report to the respective team lead (in total, 16 people). So, 21 people in all. Management theory calls the number of reports per manager in an organization the span of control . I don't like that term a great deal. For one thing, at four syllables it's a bit of a mouthful, particularly if it needs to mentioned frequently. But more importantly, it's not an accurate reflection of reality: in a knowledge-driven organization (like any technology or engineering company), it's ludicrous to think that a manager \"controls\" their reports like puppets or robots. So, I'll use a different term for the remainder of this article: I'm going to call the number of reports per manager the width of the company. Also, I'll use the term depth for the number of hierarchy levels that the company has. A sole proprietorship has a depth of zero. A company with a founder-CEO and a few employees, but no other managers, has a depth of 1. Alex' company, with one level of management reporting to Alex, and everyone else reporting to one of those managers, currently has a depth of 2. So we can say that Alex' company is currently narrow and shallow — it has small teams, and few management levels. Now, the company has just closed a major funding round and several big customer deals, putting them on a solid growth trajectory. So, Alex expects the company to double in headcount on an annual basis for the foreseeable future. So the question is: is it better for the organization to stick to the current width, and add depth as it grows, or should Alex increase its width, so that it can accomodate more people while retaining a shallower depth? In other words, as the company scales, should it become deeper while staying narrow , or should it grow wider while staying shallow? Fast-forward five years To look at that, Alex mentally fast-forwards five years under the currently assumed growth model. After five years of doubling in headcount, the company now has $21 \\cdot 2&#94;5 = 672$ employees. In this scenario, everyone in the company works still works in a 5-person team, out of which one person is the leader. So every leader has 4 people that report to them. Let's look at one employee, Sam. Sam works in a team with Joe, Jane, Harry and Ruth, and Ruth is the team lead. Let's say her title is simply, \"Manager\". Ruth now has at most 3 peers of her own, and reports to someone who goes by \"Senior Manager,\" putting her in another team of no more than 5 at her management level. That Senior Manager has at most 3 peers again, all of whom report to a Director. A Director also, together with a maximum of 3 other Directors, reports to a VP, and the 4 VPs work together under Alex, who is still the CEO. Now, I'll tell you that for 672 people, you'll not nearly have filled all those 5-person teams. But try to intuitively guess, without doing the math, what organizational size this structure would accommodate. That is to say, with every person in the company being at most 5 hops away from the CEO, and everyone working in a group of 5, what's the maximum company size this model can handle? The answer is 1,365. Let's quickly break that down and see how we can plug other numbers in. A gentle bit of maths, part I: team size and hierarchy levels Say we take company's width , that is the number of people working together in any group, excluding the leader, as $x$. In our example, that's $4$. Then, any team's size (which we'll call $n$, for reasons we'll get to in a jiffy) is of course $x+1=5$. The number of people any Senior Manager is reponsible for is $(x+1)x + 1 = x&#94;2+x+1 = 21$ (that is, their Manager's teams, and themselves). The number of people any Director is responsible for is $((x+1)x+1)x+1 = x&#94;3+x&#94;2+x+1 = 85$. You see where this is going. For any additional level of depth , we simply need to add another power of $x$. And of course $1 = x&#94;0$ — at the zeroth depth level there's one single person: the CEO. So we can express the number of people in an organization with a width of $x$ and a depth of $y$ as $$x&#94;0 + x&#94;1 + x&#94;2 + … + x&#94;y$$ or, more briefly: 1 $$\\sum_{i=0}&#94;{y} x&#94;{i}$$ And that, in turn, happens to work out to 2 $${x&#94;{y+1}-1} \\over {x-1}$$ Plug in the numbers for $x=4$ and $y=5$, and we get 1,365. A gentle bit of maths, part II: communications in complete graphs Now, what's our scaling constraint in a knowledge organization? The number of people you need to constantly be in touch with in order to accomplish your goals. For Sam, those people are principally your Sam's teammates team colleagues, including their manager, Ruth. That's 4 people. However, it's not enough for Sam to understand what he is exchanging with Jane, Joe, Harry, and Ruth; it's also imperative for him to understand what they communicate about. So, Sam needs to keep himself appraisedof what Ruth told Harry, or what information Jane gave to Joe, and how Joe and Harry are coordinating their latest change (etc.). That means that within a team, communications are a complete graph . And for a complete graph, the number of edges is given by $${n(n-1)}\\over 2$$ In our case, $n$ is our team size (including the leader), thus $x+1$ (the reports plus the leader). So we can rewrite the complete-graph formula as: $${{(x+1)(x+1-1)} \\over 2} = {{x(x+1)} \\over 2}$$ So in order for the team to be well informed of everyone's actions at all times, a 5-person team must keep track of 10 communications links between people. That's absolutely doable, though we must keep in mind that the number of links does not grow linearly with the number of people in direct communications which each other, but it grows proportionally to the square of that number. Sam's manager Ruth, of course, works on two 5-person teams: Sam's, and Ruth's team of fellow Managers reporting to a Senior Manager. That means Ruth needs to constantly keep in touch with the people on her team (including Sam), and also understand what everyone on her team of Managers is doing. Thus, she keeps track of 20 communications links. This is also true for her Senior Manager, that Senior Manager's Director, and that Director's VP. It's only at the very top that the CEO has the luxury of directly managing only 4 VPs. 3 This should be flatter! Or should it? Now, suppose someone tells Alex that in this growth plan the organization is much too hierarchical, and the organization must thus lose some of its projected hierarchy levels — that is, reduce its depth. Of course, the only way to do that while still being able to manage the same headcount growth is to make the company wider — in other words, have more people report to one manager than previously planned. So Alex, being a good CEO, opens some spread sheet software and creates this handy table that simply plugs in values for $x$ and $y$, with $x$ (width) in columns and $y$ (depth) in rows. 4 2 3 4 5 6 7 8 9 10 11 1 3 4 5 6 7 8 9 10 11 12 2 7 13 21 31 43 57 73 91 111 133 3 15 40 85 156 259 400 585 820 1,111 1,464 4 31 121 341 781 1,555 2,801 4,681 7,381 11,111 16,105 5 63 364 1,365 3,906 9,331 19,608 37,449 66,430 111,111 177,156 6 127 1,093 5,461 19,531 55,987 137,257 299,593 597,871 1,111,111 1,948,717 7 255 3,280 21,845 97,656 335,923 960,800 2,396,745 5,380,840 11,111,111 21,435,888 8 511 9,841 87,381 488,281 2,015,539 6,725,601 19,173,961 48,427,561 111,111,111 235,794,769 9 1,023 29,524 349,525 2,441,406 12,093,235 47,079,208 153,391,689 435,848,050 1,111,111,111 2,593,742,460 10 2,047 88,573 1,398,101 12,207,031 72,559,411 329,554,457 1,227,133,513 3,922,632,451 11,111,111,111 28,531,167,061 For our previous five-year plan, Alex can just look up the cell matching $x=4$, $y=5$ and finds our known outcome, a maximum head count of 1,365. Now, Alex looks at what it takes to flatten the organization by eliminating one hierarchy level, or by two. If we want to reduce depth by 1, we simply go up one row (thus, $y=4$) and find the value for $x$ that just accommodates 1,365 people or more. Alex sees that that's $x=6$, which can accommodate 1,555 people. That is, increase the width by 2: reorganize from teams of 5 to teams of 7. Alex could also pick $x=5$, that is increase the width by only 1, which would land the company at a maximum head count of 781. That is well below what $x=4$ can handle, but it still lands Alex north of the original growth target of 672. If we want to reduce depth by 2, we go up two rows ($y=3$) and do the same. We end up at $x=11$, which means to increase width by 7: reorganize from teams of 5 to teams of 12. Thus, we land at a maximum of 1,464 people, slightly exceeding the headcount we're able to accommodate if we keep growing with the current structure. We could also do $x=10$ or $x=9$, landing us at maxima well below that (1,111 or 820), but still north of 672. Now what does that mean in terms of communication channels each person has to maintain? Again, what we want to keep in mind is the number of edges in a complete graph connecting $n$ (that is, $x+1$) points. For regular employees, we know that that's $${x(x+1)}\\over 2$$ And for any manager, who is effectively on two teams of size $x+1$ simultaneously, that's $$2 \\cdot {{x(x+1)}\\over 2} = x(x+1)$$ Which means: If we want to reduce depth by 1 and go from $x=4$ to $x=5$, every non-manager employee now needs to be aware of 15 communications links (instead of 10), every manager, of 30 (instead of 20). If instead we go from $x=4$ to $x=6$, every non-manager employee now needs to be aware of 21 communications links, every manager, of 42. So that's a least a 50% increase, or even a doubling, of communications complexity. For the elimination of two hierarchy levels (a depth reduction by 2), we'll need to move from $x=4$ to at least $x=8$. At that point, every regular employee has at least 36 communications links on their teams to deal with; every manager deals with 72. If instead we go to $x=9$, every non-manager employee now needs to be aware of 45 communications links, every manager, of 90. And for $x=10$, every non-manager employee now needs to be aware of 55 communications links, every manager, of 110. At this point Alex realizes that making the company wide and shallow, instead of narrow and deep, is painfully expensive in communication cost. But what about all those managers we won't have to pay? A well-meaning advisor interrupts Alex in the middle of planning. He interjects that Alex is missing a point, namely all the managers that the company will now no longer need, and the cost savings thus generated. So Alex looks at the table again (width in columns, depth in rows): 2 3 4 5 6 7 8 9 10 11 1 3 4 5 6 7 8 9 10 11 12 2 7 13 21 31 43 57 73 91 111 133 3 15 40 85 156 259 400 585 820 1,111 1,464 4 31 121 341 781 1,555 2,801 4,681 7,381 11,111 16,105 5 63 364 1,365 3,906 9,331 19,608 37,449 66,430 111,111 177,156 6 127 1,093 5,461 19,531 55,987 137,257 299,593 597,871 1,111,111 1,948,717 7 255 3,280 21,845 97,656 335,923 960,800 2,396,745 5,380,840 11,111,111 21,435,888 8 511 9,841 87,381 488,281 2,015,539 6,725,601 19,173,961 48,427,561 111,111,111 235,794,769 9 1,023 29,524 349,525 2,441,406 12,093,235 47,079,208 153,391,689 435,848,050 1,111,111,111 2,593,742,460 10 2,047 88,573 1,398,101 12,207,031 72,559,411 329,554,457 1,227,133,513 3,922,632,451 11,111,111,111 28,531,167,061 What's handy here is that Alex can look at any one table cell, and the cell directly above it will contain the total number of managers (that is, people who have direct reports) for the same width. So, for $x=4$, $y=5$ (our original scenario allowing the company to grow to 1,365 people), Alex would have to hire and pay a total of 341 managers. for $x=6$, $y=4$ (the scenario that eliminates one level, and can accommodate 1,555 people), Alex' company will need 259 managers. That's 82 fewer managers, or a reduction by about 24%. for $x=5$, $y=4$ (the scenario that eliminates one level, but accommodates only 781 people), Alex' company will need 156 managers. That's 185 fewer managers, or a reduction by about 54%. for $x=11$, $y=3$ (the scenario that eliminates two levels, and can accommodate 1,464 people), the company will need 133 managers. That's 208 fewer managers, or a reduction by about 61%. for $x=10$, $y=3$ (the scenario that eliminates two levels, but accommodates only 1,111 people), the company will need 111 managers. That's 230 fewer managers, or a reduction by about 67%. for $x=9$, $y=3$ (the scenario that eliminates two levels, but accommodates only 820 people), the company will need 91 managers. That's 250 fewer managers, or a reduction by about 73%. A gentle bit of maths, part III: how much of our company will be managers? It so happens that we can generalize this. If Alex looks at our table again, but considers the number of managers proportional to the number of people in the company, a pattern quickly emerges (again, width is in columns, depth is in rows): 2 3 4 5 6 7 8 9 10 11 1 33.33% 25.00% 20.00% 16.67% 14.29% 12.50% 11.11% 10.00% 9.09% 8.33% 2 42.86% 30.77% 23.81% 19.35% 16.28% 14.04% 12.33% 10.99% 9.91% 9.02% 3 46.67% 32.50% 24.71% 19.87% 16.60% 14.25% 12.48% 11.10% 9.99% 9.08% 4 48.39% 33.06% 24.93% 19.97% 16.66% 14.28% 12.50% 11.11% 10.00% 9.09% 5 49.21% 33.24% 24.98% 19.99% 16.66% 14.28% 12.50% 11.11% 10.00% 9.09% 6 49.61% 33.30% 25.00% 20.00% 16.67% 14.29% 12.50% 11.11% 10.00% 9.09% 7 49.80% 33.32% 25.00% 20.00% 16.67% 14.29% 12.50% 11.11% 10.00% 9.09% 8 49.90% 33.33% 25.00% 20.00% 16.67% 14.29% 12.50% 11.11% 10.00% 9.09% 9 49.95% 33.33% 25.00% 20.00% 16.67% 14.29% 12.50% 11.11% 10.00% 9.09% 10 49.98% 33.33% 25.00% 20.00% 16.67% 14.29% 12.50% 11.11% 10.00% 9.09% You'll see that at a depth of 1, the share of managers is obviously $1 \\over {x+1}$, but then as we increase in depth it quickly trends toward: 5 $$1 \\over x$$ The number of managers in Alex' company is roughly the reciprocal of the company's width. In other words, the number of managers is inversely proportional to width. In contrast, the cost of communications is directly proportional to the square of the width. At this point Alex realizes that while there are indeed savings to be made by the elimination of management in a wide-and-shallow company, they cannot possibly balance the added communication cost. In other words: the cost in communications inefficiency grows much faster with width, so much so that it will eat up Alex' company's manager payroll savings several times over. In summary The \"flat\" (wide) organization scales poorly. Its growth in communication cost far outpaces its savings in payroll cost. And it scales progressively worse, the \"flatter\" (wider) it gets. In this capital-sigma summation formula , $i$ doesn't mean anything other than it being a counter. The formula is pronounced, in English, as \"sum of $x$ to the $i$, from $i$ equals zero to $y$\" (in other words, add up all whole-number powers of $x$, from $x&#94;0$ to $x&#94;y$). ↩ You might notice that this expression is indeterminate for $x = 1$. Now I'd say the idea of a hierarchical company made up of one-on-one teams (every manager has one report, who in turn is the manager of one report, and so on) is extremely unrealistic. But just for completeness' sake, we can apply a limit to show that $$\\lim_{x \\to 1} {{x&#94;{y+1}-1} \\over {x-1}} = {y + 1}$$ In other words, such an organization could accommodate a number of people that is equal to its depth plus 1. ↩ This why they might also be able to appoint a CFO, CSO, CTO or whatever other C-suite functions are appropriate for the organization. So in the scenario we might end up with a handful more people than 1,365 for the C-suite and perhaps some number of staff in their offices. But for the purposes of this discussion those don't make a big difference, so we'll disregard them for now. ↩ I encourage you to compare the bottom rows and rightmost columns of this table to Wikipedia's list of largest employers . ↩ If you're curious, that is because the share of managers in relation to the total number of people in the company is $${\\sum_{i=0}&#94;{y-1} x&#94;{i}} \\over {\\sum_{i=0}&#94;{y} x&#94;{i}}$$ That works out to be $${x&#94;y-1} \\over {x&#94;{y+1}-1}$$ Which, for $y=1$, is $${{x-1} \\over {x&#94;2-1}} = {{x-1} \\over {(x+1)\\cdot(x-1)}} = {1 \\over {x+1}}$$ And for larger values of $y$, both $x&#94;y$ and $x&#94;{y+1}$ become so large that the $-1$ part barely matters, so it's effectively: $${{x&#94;y-1} \\over {x&#94;{y+1}-1}} \\approx {x&#94;y \\over x&#94;{y+1}} = {1 \\over x}$$ In slightly more formal terms, we can consider $1 \\over x$ the limit of the expression as $y$ goes to infinity: $$\\lim_{y \\to \\infty} {{x&#94;y-1} \\over {x&#94;{y+1}-1}} = {1 \\over x}$$ ↩","tags":"blog","url":"blog/2022/01/16/flat-org-scaling/","loc":"blog/2022/01/16/flat-org-scaling/"},{"title":"Voice messages","text":"As of late, I've noticed that when people share one of my articles on asynchronous communications on Twitter (particularly any from the Getting out of Meeting Hell series, or the one on meetings that should have been an email ), there's a reply from a brand account that likes to plug/advertise their service. That service recommends that synchronous meetings be replaced by \"asynchronous meetings\" based on voice messages. I'd like to point out that I consider that an utterly terrible idea. Let me explain why. Voice is slow First, voice messages suffer from the exact same drawback that meetings do: they are incredibly slow. Most of us speak at a rate of approximately 4 syllables per second . 1 In English, that translates to about 120-140 words per minute. That means that as a listener, you absorb the content of a voice message at the same rate. You might make that a little more efficient by increasing playback speed, but that's only feasible to about a 25% speed increase, which lands you around 150 words per minute. In contrast, unless you are dyslexic (I'll get to that in a bit) you can read at 240 words per minute. In other words, conveying a certain amount of information by voice takes nearly twice as long as doing the same in writing. And that's if your verbal expression is perfect, which it never is — any voice message will come with its fair share of filler words (\"uh\", \"um\", \"y'know\") and incomplete sentences. Add to that the occasional slurred word or phrase, or idioms unfamiliar to the recipient of the message. If you come across something that's unclear while reading, it takes you fractions of a second to re-read a sentence, and maybe a few seconds to re-read from the beginning of a paragraph. But in the listening case, it may take you upward of a minute to go back and re-listen to a passage you missed or didn't understand. (Anyone who both reads books and listens to audiobooks will relate to this.) Voice is more difficult to follow and retrieve Secondly, voice messages are usually much more difficult to understand for a recipient listening in their second or third language, particularly if the other person is a native speaker using an accent that is unfamiliar to them — say, a French person listening to heavily Scots-accented English or a pronounced Australian twang. Written messages might still have their ambiguities — as an example, the word \"doubt\" meaning \"question\", a common substitution in Indian English, frequently confuses non-Indian English speakers — but those are far fewer and easier to resolve for a reader. Furthermore, until speech recognition is perfect and automatic transcription is thus exquisitely faithful, your voice messages aren't searchable. You could say that they're practically half-off-the-record. Good luck trying to come back to an important bit of information that someone conveyed via a voice message that you have only a vague recollection of. Or, worse, trying to establish the context in which a decision was made, and having to piece it together from multiple voice messages. Voice doesn't convey as much nuance as you think Thirdly, the notion that you ought to be using voice messages to add \"nuance\" when you can't convey such nuance in your writing strikes me as patently ludicrous. When you need to convey emotion or feeling or nuance to a greater extent than you would be able to in writing, that is absolutely a situation in which you should meet with a person face-to-face, one-on-one, and where that doesn't permit itself, get on a video call. At that point, when a written message won't cut it, a voice message absolutely won't. One good use? Now, there may be one useful use of voice messages that I can think of: they may work for people with dyslexia, for whom consuming a lot of writing may cause cognitive overload . In that case, voice messages might be a workable alternative. If so, then that would make the option to communicate via voice message a very valid accessibility consideration. That said, I've talked to people who are dyslexic and who said that voice messages are not an adequate substitute for interactive verbal communication to them — but that's of course highly anecdotal, and shouldn't dismiss the idea outright. However: even if voice messages are a good thing for people with dyslexia, though, I think that as screen readers continuously improve, generating speech from text may a be preferable option. That's because it retains the searchability advantage for everyone, and the efficiency advantage for non-dyslexics, while also accomodating people with dyslexia. But, I want to re-emphasize that that's just a hunch, and I may well be completely wrong. If you're dyslexic and have thoughts on this, I'd love to hear from you! Please find me on Twitter or Mastodon . Fun useless fact: the rate of 4 syllables per second is practically universal across spoken languages. How many words a native speaker of a particular language speaks in a minute depends largely on the average number of syllables per word in that language. ↩","tags":"blog","url":"blog/2021/12/05/voice-messages/","loc":"blog/2021/12/05/voice-messages/"},{"title":"Creativity: How we lost it, why that's bad, and how we get it back","text":"Right now, it's easy to open a news site and come to the conclusion that the world as we know it is well and truly fucked. It doesn't matter if you're looking at Covid response, or inaction on climate change, or corruption, or communications surveillance: it increasingly looks like we are being governed and managed overwhelmingly by dunderheads just bumbling along, equipped with less than the most basic empathy, and lacking the mental faculties required to understand exponential growth or conditional probability or even percentages. And people — intelligent people — are seriously pondering the situation with utter befuddlement: how the hell did we get here? The German writer Max Buddenbohm recently asked his followers a question to that effect on Twitter , which I am taking the liberty to translate: Do you have a reflected opinion on why everything is so poorly organized, as in at its core? Historically or sociologically, what's the real principal reason? How did this happen? Now it's perhaps a bit amusing and stereotypically German to complain of ſchlechte Organiſation! in the middle of a pandemic and global climate cataclysm, but the sentiment behind the question is sound: it looks as though at every twist and turn, those empowered to make any kind of decision make the wrong one, or none at all, or — the worst — aren't even able to come up with sensible options between which to choose. And I have a hypothesis: I think the issue at the core of why everything appears to be going down the tubes is that we have systematically drummed creativity out of people, for at least fifty years. And as a result, we collectively have no idea how we can get ourselves out of a rut. Let me swiftly explain what I mean by \"creativity.\" The wonderful Sir Ken Robinson , who left us much too soon in 2020, described creativity as \"the process of having original ideas that have value.\" And in arguing for the value of creativity, we don't need to get all hippy-touchy-feely: creativity — and teaching and learning creativity — is a simple economic necessity that is also essential to our survival as a species. If we have no idea what our world will look like ten years — or even ten months — from now, then how the hell is any \"hard skill\" we acquire today guaranteed to be useful in future challenges? The paramount faculty we need to acquire, train, and nurture is the ability to come up with flexible, intelligent, creative solutions to the problems we'll encounter tomorrow. And this we haven't been doing. From an early age onwards, generation after generation has been schooled in \"the right way\" to do things. And since \"the right way\" exists only for things we already know, we are now ensnared in a trap called conformity that is no good at all in the present situation where so many of us are confronted with things we haven't a clue about. To illustrate what I mean, allow me to offer an anecdote from my own personal family experience. When my son, who is now nearly an adult, was a nine-year-old pupil in his fourth year of primary schooling, he started to be tasked with writing little stories — it would be an exaggeration to call them \"essays\" at that point — in school. And, given the fact that he was quite an imaginative kid, the first couple of stories he wrote were truly charming and delightful. But after a few weeks, something strange happened: his stories were getting rather bland and boring, and were hardly a reflection of his vivid imagination anymore. So as his parents, we gently queried about this mysterious phenomenon — and he was quite happy and forthcoming to explain the reason. He had observed, he informed us, that the fellow pupils of his that had got good marks and the teacher's appreciation on the first stories they had written were the ones that had turned in the writing with the fewest spelling and grammar mistakes. And apparently he resolved then and there to henceforth only turn in stories that were composed of sentences strung together from words he already knew how to spell, using constructions that he was already familiar with — and that he was thus unlikely to stuff up. Of course that made reading the stories about as exciting as watching paint dry, but it kept the teacher happy and thus, off his back. And I can attest that this very much goes for my own schooling as well. It doesn't matter if we're talking about my German classes or English classes or French classes, \"correctness\" always prevailed over originality or wit or creativity. I don't mean to insinuate that clever writing should get you a free pass to absolutely butcher your spelling and grammar, but then on the other hand perfect orthography and punctuation did always make up for abject boredom, and that's not quite right either. And this wasn't restricted to just language classes: in maths exams there was no extra credit to be had for arriving at the correct solution of a problem in a novel or unconventional way. Nay, such brazen nonconformity would net you either a reprimand from the teacher for not showing the correct path to the solution, or at least a snide remark of the \"oh you think you're very clever don't you, now sit down and behave\" type. My English and French and German teachers appear to have been unaware of another fact related to their institutional correctness obsession. Consider this: people who actually make a living from writing — no matter if it's fiction or non-fiction — tend to work with editors, people whose calling it is to not only correct issues with orthography or punctuation, but also to helpfully point out plot holes and suggest the occasional rewrite of dialogue or rearrangement of chapters. Editors are highly respected by writers and instrumental to the success of a book but yet, strangely, these people's names are normally not printed in bold letters across the book cover, and they also don't appear in best seller lists. If my language teachers had been correct, editors should be celebrity superstars! And they should hold far greater prestige than the silly authors who only come up with the storylines but regularly struggle with the placement of a semicolon. And the problem with the idea that errors are awful, and the fact that that idea is being hammered into our heads from an early age, is that this has a devastating effect on our creative imagination: I don't mean to say that being wrong is the same thing as being creative. What we do know is: if you're not prepared to be wrong, you'll never come up with anything original. — Ken Robinson, \" Do schools kill creativity? \" (2006) So, if you want people to be boring and dull and utterly devoid of originality, then foster a culture where being error-free is paramount. In the business world, some tend to look down on others who have a tenuous relationship with spelling and grammar and punctuation, chiding such deficiencies as \"unprofessional.\" But at the same time, it's commonly accepted to be dragged into a meeting and then forced to listen to someone drone on for an hour in a narcotic monologue consisting of the recitation of thirty-four wall-of-text slides with precisely seven bullet points each, in a ferocious assault on everyone's attention and consciousness that gives an overdose of Valium a run for its money. How, pray tell, is it ever \"professional\" to steal people's precious lifetime by boring them out of their fucking minds? And yet, this is somehow acceptable behavior in the world of business. Let me add another bit of anecdotal first-hand experience: a few years ago when I was making most of my living as a travelling technical consultant, I was often brought in to help a team of engineers chart a path for solving a particular problem using one of the technologies I knew a little bit about. And in doing so, some decisions frequently boiled down to two choices, which I was always happy to lay out in detail: here is option A, it comes with these advantages and disadvantages, and here's option B, it comes with those advantages and disadvantages. And I would explain to my client that it is now their business decision to determine whether the pros of A outweigh the pros of B for their business, and whether or not they would be able to live with the cons of whatever option they chose. And inevitably, the reactions to this nearly always fell into one of two categories. The first category was gratitude for me having laid out the options clearly and distilled the pros and cons of each, informed by my technical expertise on the matter and my understanding of their situation. And the business decision was either completely obvious to them, or they appreciated having a good basis on which to make a decision, which they resolved to make in the following days or weeks, presumably after some more empirical, experimental evaluation. And the second category was complete confusion on a person's face, followed by the exasperated question, \"so what do you recommend?\" Or, worse, \"what's the right way?\" You can probably guess which category of answers was more likely to come from managers — or \"leaders\", as such people like to be called in a gross exaggeration of their capabilities . And now imagine someone having gone through a conventional primary and secondary education, then continued on to university and from there into business or the law, or maybe the academic Ph.D. track, while in parallel having risen through the ranks of that cult of rigidity and conformism called a political party — and ultimately entering public office. On that note, another illustrative anecdote. Not from myself or my family but still close to home: you may remember how early in the Covid-19 pandemic in February 2020, the Tyrolean ski resort of Ischgl became Central Europe's first major infection hotspot , directly linked to at least 600 cases in Austria and more than twice as many across Europe. (At the time, 1,800 confirmed Covid cases seemed like a lot. As I write this, we have about 10 times as many. Per day. ) The universal understanding of the majority of observers at the time — and today — was that the situation on the ground had been horribly botched, and that egregious mistakes were made that greatly facilitated the spread of Covid-19 across Europe. The official in charge of public health in the provincial cabinet then went on national news a couple of weeks later to discuss the events. And, despite the repeated questions from the exasperated interviewer, voiced with increasing levels of disbelief, the official kept insisting that the authorities had \"done everything right\" and were not to be faulted at all for their actions — and inactions — in this public health emergency. And I don't even think that this was just hubris or an attempted cover-up. Rather, it's a symptom of the exact problem I'm trying to describe. If you're applying only what you already know to a situation that's never been here before, you're failing horribly at dealing with that situation. But if you've been conditioned that applying \"the correct solution\" is all you'll ever have to do in life to succeed, you eventually end up genuinely believing that that's enough. And that's how, in the greatest crisis that humanity has faced in peacetime in over a century, what we're stuck with are leaders who, for the most part, have been so thoroughly molded by the perpetual vicious cycle of conformism that they now operate with the decisiveness and agility of a herd of mammoths deep-frozen in permafrost. They are just shockingly ill-equipped to deal with a global health crisis affecting a closely interconnected civilization. And the ones that actually used creative ways to get to power turn out to be sociopathic one-trick cronies that could not apply their skills to something useful if their life depended on it. It looks as though systematically, those who have the power to make decisions, at all levels — in business, politics, anywhere — frequently lack the intellectual, emotional, and empathetic creative capabilities required to make those decisions. This is not to say that exceptions to this rule don't exist — I'm lucky enough to live in a town where officials from the mayor on down have been exceptionally creative, empathetic, and successful in their Covid response, for example — but I would argue that the rule does stand. Now, it would be entirely fair to counter my arguments with the observation that surely, in the say 1950s or 1930s or 1910s schooling and education were still more rigid than they are today, and certainly did not allow for more creative freedom than they do now. And that is certainly true, but there is something that children (at least those lucky enough to go to school, I am aware that for those who spent their childhood toiling in the fields or coal mines it was an entirely different story) had during their schooling in those days that is a precious rarity today: time and space to let their mind play. John Cleese writes in his excellent book on creativity that mental play — the ability to let your mind wander and thereby become open to developing new ideas — is a key element of the creative process. And this is by no means limited to music or literature or the arts; some of the examples he lists of mental playfulness leading to groundbreaking new ideas are from the world of science. Now, it is essential to be able to keep our mind in that state of playfulness for some time, because it takes a little while for new ideas to pop into our head. And so, Cleese observes: The greatest killer of creativity is interruption. It pulls your mind away from what you want to be thinking about. […] It might be an interruption from outside, like someone coming over and talking to you, or an email popping up in your inbox. Or it may come from inside, as when you suddenly remember something you've forgotten to do, or worry that time is running out, or that you don't think you're clever enough to solve whatever problem it is you're trying to deal with. — John Cleese, \"Creativity: A Short and Cheerful Guide\" (2020) And this is the bit that's incredibly difficult to achieve for most people born after about 1970, and many born earlier too. That includes any child alive today, but also their parents and many of their grandparents. We are constantly dealing with outside interruptions, many of them coming from a device we carry in our pockets all day. We have to fight for our uninterrupted mental play time. A child in the 1950s might just run off to play with friends for the afternoon, and return home for supper. Without a text from a parent or a snapchat message from the school bully rudely interrupting halfway through. It's perhaps no coincidence that some of those 1950s children ended up being 26-year-olds who could figure out powering up a disabled spacecraft while it's on a free-return trajectory around the Moon, saving the life of a three-man crew in the process. There's more evidence that even in the world of engineering, allowing your mind to let go for a bit can lead to creative breakthroughs: Jim Crocker was the Ball Aerospace engineer who solved the problem of how exactly the corrective optics on the Hubble Space Telescope should be installed — obviously not a scenario that anyone accounted for in Hubble's design. The ingenious idea that ended up saving Hubble from being a multibillion dollar boondoggle came to him in the shower . Also, in offices in which we worked with maybe one other colleague in the room, we had stretches of time where the other person was off running an errand in town or taking a meeting in a conference room. And you could close the door and put up a \"do not disturb\" sign and do some uninterrupted thinking. Around the year 2000, most of that started to change, dramatically. Open-plan offices, which of course were allegedly introduced to \"facilitate cooperation,\" eliminated any room for uninterrupted mental play at work. Emails replaced typed and printed memos, office workers transitioned from workstations to laptops. Pagers and cell phones started to buzz people at home. Around 2010 we progressed to smart phones, tablets and other portable devices that had the ability to ping us out of playful thought with an audible notification at any time of day. Knowledge work became interrupt-driven — a sentence that in itself should make anyone shudder that knows anything about knowledge work at all. And it's no surprise that people who rose to corporate leadership after being imprinted by an interrupt-driven lifestyle — that is, people now in their 50s — think that such a thing is normal, and try to impress the same thing on their subordinates. That's how you get to Slack-driven companies. That's how you end up in a culture where people are proud of getting back to any email within 30 minutes (or less), and expecting the same from everyone. That's how you end up with managers who take being signed into a chat (and thereby constantly listening for interruptions) as a measure of being \"active\", so much so that they end up tracking metrics for it , and of course also making it a target they call \"engagement\" or some other abomination. Organizations that do that are at risk of constantly shutting down creativity, problem-solving, and innovation. What they're good for is developing efficient cookie-cutter techniques for optimizing solutions for yesterday's issues. What we need today — in a global pandemic, and in the roiling climate crisis that'll make this pandemic look like a walk in the park — is people and organizations equipped with the mindset for the issues of tomorrow. And a first imperative for making that happen is to let people think. So is everything all doom and gloom? I'd say it isn't, and there are indeed some things that make me hopeful. Some of those are related to a changed approach to creativity in education, some, to a changing approach to work. For example, in my country as of a few years ago it is indeed such that creativity and originality are accorded at least some merit in marking and grading standards. And this is at the secondary school level, traditionally one of the most rigid and unchanging branches of education where I live. Much more still is happening at the primary and preschool level, where I see much more emphasis on thinking, creative play, and innovation in my younger kids' education than I did in my older ones'. And then, there's the big push towards asynchronous distributed work — where obviously the asynchronous part is the bit that matters. Sure, companies suffering from offissification still exist, but for a while, so did dodos . But an ever-increasing share of humanity is beginning to understand what it's like when you're no longer shackled to seventeen Slack channels you constantly need to watch, when you can take a walk in the middle of the day because you know it's OK to push something off for an hour to clear your head and come up with an idea, and when instead of spending an hour in a meeting you can spend 5 minutes reading a memo and use the other 55 minutes for thinking . And that's where things get interesting.","tags":"blog","url":"blog/2021/11/21/creativity/","loc":"blog/2021/11/21/creativity/"},{"title":"Meaningless Metrics, Treacherous Targets","text":"A common feature of organizations in the software technology industry (but certainly not only in that industry) is their fixation on metrics, measurements, and quantifiers. I understand that this is frequently done and advocated for in the spirit of making management more objective, less arbitrary, more scientific, and perhaps fairer. But since they say that the road to hell is often paved with good intentions, here's a quick summary of what we know about about the undesirable side effects of such an approach. Goodhart's Law British economist Charles Goodhart wrote in 1975, in an article about British monetary policy: Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes. — Charles Goodhart, \"Problems of Monetary Management: the U.K. Experience\" (1975) That's a mouthful of somewhat niche technical jargon, but let me try to paraphrase it like this: You collect some data. You crunch the numbers using statistics. You observe a pattern. You distill a value (a \"statistical regularity\") from it. Someone decides that that value should change: it is too high or too low. Someone — an individual or a group — is tasked with bringing that value up or down, and then keeping it high or low, or rising or falling, or above or below a particular threshold. That value now is no longer a useful statistical indicator. What you probably knew as Goodhart's Law if you'd heard about it prior to reading this article is a generalization by anthropologist Marilyn Strathern , also from the UK: 1 When a measure becomes a target, it ceases to be a good measure. — Marilyn Strathern, \" ‘Improving ratings': audit in the British University system \" (1997) Why is that so? It's because once you make the measure a target that has an influence on people (for example, meeting it gets them a bonus, failing at it gets them a demotion), you have wired them to improve the measure, and not necessarily to improve the underlying conditions that the measure originally arose from. Therefore, they might opt for gaming the measure, because that gets them to their goal (a promotion, for example) more quickly and at less effort to them. Furthermore, even keeping the option of fudging the numbers aside: when faced with a choice between doing something that might have a negative effect on the measure and something else that might have a negative effect on something other than the measure, people will tend to choose the latter. This may lead to situations where people avoid an activity with significant inherent value, just to avoid depressing a measurement — a concept known as creaming. 2 For example, a hospital may be interested in measuring individual surgeons' intraoperative death rates: the percentage of a surgeon's patients that die in the middle of surgery. On its face, this metric could help weed out bad surgeons. If a particular surgeon is an outlier and has way more patients dying on their operating table than their peers, it's possible that that surgeon might be doing something wrong: they could be incompetent, or frequently intoxicated, or even be a Dr. Death type serial killer. It gets tricky, though, when in the interest of transparency the hospital doesn't just fire or retrain incompetent surgeons which it identifies based on such statistics, but when it \"publishes\" the patient mortality data. (I use quotes here because this does not necessarily mean sharing it with the general public, but perhaps sharing it with all of the surgical staff.) At that stage, an individual surgeon's rank in the statistics will become at least a matter of pride, status, and prestige, even if it's not otherwise rewarded in any way, nor seen as a precondition for continued employment. This, then, will incentivize surgeons to avoid taking on risky surgeries where there is a significant chance of the patient dying mid-surgery — surgeries typically attempted in the first place to save the patient's life, in the course of an immediate major emergency. Thus, Dr. Alpher who only ever treats torn knee ligaments might look better in the ranking than Dr. Bethe the polytrauma specialist, or Dr. Gamow the neurosurgeon who specializes in particularly challenging malignant brain tumor removal. If there is a non-negligeable risk of intraoperative death for a particular brain cancer patient and such an event would be bad for Dr. Gamow's ranking, then Dr. Gamow might have an incentive to declare that patient inoperable — and as a result the patient would certainly die, just not in surgery. 3 Campbell's Law Although less well known than Goodhart's law, Campbell's law is closely related and, in my humble opinion, just as important. Donald T. Campbell , a U.S.-based social scientist, wrote in 1976, on the subject of standardized testing in education: The more any quantitative social indicator is used for social decision-making, the more subject it will be to corruption pressures and the more apt it will be to distort and corrupt the social processes it is intended to monitor. […] Achievement tests may well be valuable indicators of general school achievement under conditions of normal teaching aimed at general competence. But when test scores become the goal of the teaching process, they both lose their value as indicators of educational status and distort the educational process in undesirable ways. — Donald T. Campbell, \"Assessing the impact of planned social change\" (1976) In other words, if you conduct a one-time evaluation of student achievement across many students in multiple schools, then the fact that the test is standardized might help in achieving comparable results. However, as soon as you make the tests a repeat occurrence, and tie students' test results to school funding allocations, teacher salaries, or even just school prestige, you're undermining their original purpose: teachers will now spend a significant portion of their time and effort to ensure that students score well on the test , rather than build the competence that the test was originally designed to measure. This is an example of allocating resources (teacher and student time and effort) to an activity with no inherent value (taking a standardized test) just to improve a measurement (the test score). And since the resources are finite, spending them on the activity with no inherent value (test-taking) makes less of them available to the inherently valuable activity the indicator is intended to assess (teaching and learning). This is the \"corruption and distortion\" Campbell talks about. The McNamara Fallacy, and the Yankelovich Ladder Closely related to Goodhart's, Strathern's and Campbell's observations is something called the McNamara Fallacy. Robert McNamara , U.S. Secretary of Defense during much of the Vietnam war, infamously believed that he could scientifically measure the progress of the war by quantitative indicators alone. One of his favourites was body count, the number of enemy personnel killed, in comparison to friendly casualties. The rationale appears to have been, whatever other factors (qualitative or quantitative) are in play, whichever side kills more of the other wins the war. Indeed he seems to have been inclined towards ignoring all non-quantitative indicators of how the war was going. An anecdote told by U.S. Air Force general Edward Lansdale alleges that he (Lansdale) pointed out to McNamara in a briefing that McNamara, when assessing the progress of the war, failed to take into account the feelings of the common rural Vietnamese people. McNamara then allegedly wrote an item saying \"feelings of the Vietnamese people\" on his list of things to keep track of in pencil, pondered it for a moment, and then erased it — reasoning to Lansdale that feelings cannot be measured, thus must not be important. 4 This is step 3 on a progressive scale social scientist Daniel Yankelovich described a few years later: The first step is to measure whatever can be easily measured. This is OK as far as it goes. The second step is to disregard that which can't be easily measured or to give it an arbitrary quantitative value. This is artificial and misleading. The third step is to presume that what can't be measured easily really isn't important. This is blindness. The fourth step is to say that what can't be easily measured really doesn't exist. This is suicide. — Daniel Yankelovich, \"Corporate Priorities: A continuing study of the new demands on business\" (1972). And it's somewhat remarkable just how often businesses and organizations fall into this trap, fifty years later. They might not end up at step 4, but falling for step 2 or 3 is bad enough. An applied example Let's now turn to an example from our industry. Something that's so important, evidently, that it has given rise to a whole discipline in our field: site reliability. Now it's perhaps a bit amusing that although you can find myriads of articles describing what site reliability engineering (SRE) is, a definition of \"site reliability\" lives only in a small footnote of the Google SRE Book : For our purposes, reliability is \"The probability that [a system] will perform a required function without failure under stated conditions for a stated period of time\". — Betsy Beyer, Chris Jones, Jennifer Petoff, Niall Murphy, \"Site Reliability Engineering: How Google Runs Production Systems\" (2017) 5 But, at least there is a definition, which is good. Now I think it's reasonable to say that the following two statements about site reliability are probably true: In keeping with SRE reflecting a holistic approach to engineering, trying to unify a multitude of considerations, site reliability is not something we can judge by a single, numerical, universal, and useful metric. You can't measure a single \"site reliability score\", and then compare hundreds of platforms based on that. 6 Whatever site reliability is as a whole, it certainly includes a site's ability to process your data and not mangle it. So, if you upload your data into a platform, you want to be able to do something useful with it. SRE tends to rely on service level indicators (SLIs) to measure compliance with service level agreements (SLAs), manage error budgets, and generally keep track of what shape the site/platform is in. So, let's compare two indicators that differ greatly in their measurability. Availability is exceptionally easy to measure for, say, a REST API. You send a request with a defined payload, you measure the time it takes to serve your request, you check the status code, you check whether the response contains what you expect, and you record a data point. Durability is much more difficult to measure at any given point in time. Effectively, to properly take a data point for durability at the same time as getting one for availability, you'd have to read back some data you wrote, say, a year ago, and check its content against something like a known hash. 7 And also write some data now, travel a year into the future, read it back at that point, travel back into the present, 8 and record your data point. Now before I continue I'd like to inject another thought to the issue of data durability: not every platform is a storage solution. In other words, you don't always have the option of reading your data back verbatim. Say for example you're feeding an inordinate number of data points into a platform that ingests and aggregates them. You may not even be interested in the original data some months or years down the road, so it might be acceptable (and even necessary, as dictated by cost concerns) to discard the original data immediately after it has been processed. And that rules out the possibility (or necessity) to ever read it back exactly as it went in. But you will be interested in the statistics that you generate based on the aggregated data. And now suppose there is a subtle bug in the implementation of the aggregation algorithm. As in, the algorithm itself is perfectly fine, but there's a flaw in the implementation. That, too, may render part of your data unusable or outright invalid, violating data integrity and durability. But the tricky part here is that availability is easy to measure. Data durability isn't . Therefore, availability lends itself to becoming a target (hello, Professor Strathern), and durability tends to be seen as difficult to measure and hence less important (hello, Secretary McNamara). So now, if you find yourself in charge of a system that you suspect has started to corrupt a significant fraction of customer data, data which customers are pouring into it at an alarming rate, what do you do? You're not sure whether there's actual corruption yet. The proper thing to do, if it's impossible to rule out or fix the data corruption problem immediately, 9 is probably to stop intake, and also ensure that no requests are served that may touch potentially corrupted data — that is, shut the service down even before you've ascertained corruption. But if you suspect that your next bonus payout or promotion may rely on you meeting your availability goals, and you know you're already shaving it close with your availability error budget, would you really be inclined to do that? \"You can't manage what you don't measure\" There's a popular saying in management circles that takes one of the following forms: \"If you can't measure it, you can't manage it.\" \"You can't manage what you don't measure.\" \"You can't manage what you can't measure.\" Whichever variant you discuss, it is commonly attributed to either Austrian-American management thinker Peter Drucker , or to American engineer and statistician William Edwards Deming . Drucker is seen by many as highly influential in management theory, Deming developed groundbreaking sampling techniques used on the massive scale of the United States census. So either of them would be an authority on management and measurement, lending high credibility to the statement. There's just a small problem: neither of them appears to ever have said or written anything to that effect. The closest that one of them, Deming, ever wrote was: It is wrong to suppose that if you can't measure it, you can't manage it — a costly myth. — W. Edwards Deming, \"The New Economics for Industry, Government, Education\" (1993) In case you didn't notice, the point this makes is the exact opposite of the popular version of the quote. It's so wrong that it comes close to the corruption of the Seneca lament, \"non vitæ sed scholæ discimus\", \"we learn not for life but for school,\" of which you surely learned the inverse… in school. Metrics-obsessed managers often take the misquote for gospel. So much so that they frequently see issues where a qualitative approach is obviously necessary, and they still try to apply quantification. My standard example for this are employee satisfaction surveys. Ultimately, what leadership should be interested in learning from those surveys is how good people feel about working in the company. There are a number of factors that contribute to this: are they overloaded, well utilized, or bored? Are people treating each other with respect and kindness, or malice and contempt? Does everyone feel that they are doing something meaningful, or do they all hate their work and are solely in for the money? All these things are inherently qualitative. And the company could do a great job by hiring a person trained in sociology or psychology, who sits down with people for confidential qualitative interviews, and then prepares a research report with findings and recommendations that management can act on. But no, we have to measure. Make everyone take an online survey where they rate everything on a scale of 1 to 5. Do you know what that is? Exactly, step 2 on the Yankelovich ladder. Give that what can't easily be measured an arbitrary quantitative value — because that's what it is, arbitrary. People from different cultures won't agree even on what a simple 5-step scale really means . And depending on what version of the faux quote they adhere to, a manager may even be farther up the ladder: If they say \"you can't manage what you don't measure\" (with the translation being \"I won't concern myself with anything for which I don't have quantitative data\"): that's step 3, blindness, that which isn't measured isn't important. If they insist that \"you can't manage what you can't measure\" (with the translation being \"I won't concern myself with anything that isn't quantifiable\"): that's step 4 (suicide), that which isn't measured doesn't exist. So, what now? Every article and book on bad metrics ends on a positive note, giving you suggestions for \"good\" metrics: for example, make them hard to game, make sure they are defined by competent experts, ensure that they are in line with inherent ideas of respectability and professionalism. Honestly, I've yet to come across a metric that ticks all these boxes. 10 So, I am aware that if you are running a platform under an existing SLA, you will be running under some metrics of questionable utility that you cannot get rid of — just because they happen to be industry standards. However, instead of expanding metrics obsession to your entire organization by introducing ever more counterproductive metrics, I want to propose a different approach: Whatever you measure, make the marginal cost of a measurement negligeable. 12 The cost of adding a new metric should be practically zero. The moment someone has to repeatedly spend time on collecting and compiling the data, they can't spend that time on doing productive work (and Campbell says hi), so you want to avoid that. This effectively means that all the systems you care about (machines, services, applications) should generate collectable data points, everywhere, all the time. 11 And you probably won't be collecting metrics from anything else. In other words, you are just measuring that which is easily measurable, and you keep aware that there a lot of things you don't measure that are just as important. You stay on step one of the Yankelovich ladder. Now, I'd propose you make the data thus ingested available throughout your organization, in machine-readable form and using standardized APIs. You want people to actually discover things from your data. Encourage people to use real, scientific, statistical methods to figure out statistical regularities (\"indicators\"). Offer statistics training to people who are interested. Once someone identifies a statistical regularity, encourage them to form an opinion of whether it would be beneficial for it to go up or down, formulate a hypothesis on what change to your system would have the desired effect, and conduct an experiment. If the experiment has no effect, roll back the change and proceed with the next hypothesis. If it has an adverse effect, roll back and try the opposite. If it has the desired effect, keep the change. Move on to discovering the next regularity. Resist the urge to make the discovery a target. (Otherwise, Strathern will drop by.) Constantly observe and identify things that are important, but not measurable. Apply qualitative analysis, emotion, and empathy. (Otherwise, McNamara will introduce himself.) So, is there anything inherently wrong with measuring or measurements? Nope. But making them targets, introducing arbitrary quantifiers, and ignoring everything else is. The reason the condensed version is called \"Goodhart's Law\" and not \"Strathern's Law\" is apparently due to a coinage by British researcher Keith Hoskin, who wrote a year prior to Strathern, in a paper she cited: \"Goodhart's Law\" — that every measure which becomes a target becomes a bad measure — is inexorably, if ruefully, becoming recognized as one of the overriding laws of our time. — Keith Hoskin, \"The ‘awful idea of accountability': inscribing people into the measurement of objects\" (1996) ↩ If you think that term sounds a bit odd, I'd agree. I guess it comes from the idea of milking a cow and then skimming only the cream, discarding the rest. ↩ The surgery statistics example of creaming is paraphrased from Jerry Z. Muller, \" The Tyranny of Metrics \" (2018). ↩ The Lansdale/McNamara anecdote is paraphrased from the Wikipedia article on the McNamara Fallacy , which in turn cites Rufus Phillips and Richard Holbrooke, \"Why Vietnam Matters: An Eyewitness Account of Lessons Not Learned\" (2008) as its source. ↩ It should be noted that the SRE book is itself quoting a definition of reliability found in Patrick P. O'Connor & Andre Kleyner, \"Practical Reliability Engineering\" (2012). ↩ The irony is not lost on me that by the definition quoted in the SRE book, such a score absolutely should exist if its definition of reliability were adequate: it claims to be a probability. Probabilities go from 0 to 1. That would make site reliability a dimensionless quantity between 0 and 1, end of story. But it goes without saying that such a score would be \"an arbitrary quantitative value\", which would put it on step 2 of the Yankelovich ladder. ↩ That hash would have to be separately stored outside the system. If the hash is stored alongside the data whose integrity it's meant to protect, then it only guards against unintentional data corruption, but not against deliberate manipulation. ↩ I wish to point out that the only bit that's impossible here is the backwards time travel. The forwards time travel is fine, we all travel forwards in time all the time, just at a constant rate of one second per second. ↩ I've run into a few issues of suspected silent data corruption in my career and I've never been in the situation where a reliable fix was available immediately. ↩ In particular, pretty much any real-world metric fails the \"hard to game\" test. Said Lukas Grossar on Twitter : \"It always amazes me that people don't believe that slapping a KPI onto something won't lead to people gaming that KPI. We're engineers for God sake, making broken stuff work in our favor is basically our job description.\" ↩ I'd argue that this requires strong privacy guarantees for your users/customers. Effectively, just don't collect data that's none of your business. ↩ Emphasis on marginal. It's obvious that the fixed cost of building and maintaining an instrumentation platform and metric system is nonzero. But once you've got it set up, the cost of adding a new metric should be substantially zero. ↩","tags":"blog","url":"blog/2021/11/14/meaningless-metrics-treacherous-targets/","loc":"blog/2021/11/14/meaningless-metrics-treacherous-targets/"},{"title":"Warnock's Dilemma, Objections, and Acknowledgements","text":"People skeptical of distributed, asynchronous, written communications sometimes make they understandable objection that it is often difficult to interpret the reactions, specifically the absence of reactions, to written online communications. The reasoning goes like this: if you inform someone of something in a face-to-face conversation, there is practically no way for them not to provide some sort of feedback. Even if the recipient of a verbal message doesn't say a word, they usually exhibit some unconscious, nonverbal reaction, which can carry a whole load of information: The person might smile, light up, and become actively engaged, they might express surprise (pleasant or unpleasant), they may show signs of dismay or annoyance, or even anger, they might just stare or wander off, indicating disconnection or indifference, or anything in between. In online, textual communications, people obviously also exhibit all those reactions, sitting at their desk, lounging on their couch, walking with their phone — it's just that the sender of the message usually never gets to see them. In addition, a face-to-face conversation is a one-to-one communication mode that we direct our undivided attention to. In contrast, textual online communications are often many-to-many, and we usually get many more parallel inputs than we do when we're speaking to a colleague or acquaintance or friend. That means that while in a face-to-face conversation we're always answering, or at least showing our reaction to what was said, in online textual communications we must pick and choose what to react to, and what to just absorb without providing any kind of feedback to the message sender. Warnock's Dilemma In online communities this has been known since at least 2000, when Bryan Warnock formulated it as \"the ostrich theory\", although it eventually was named \"Warnock's Dilemma\" 1 by Dave Mitchell. Writing about mailing list posts without replies, Bryan wrote: The problem with no response is that there are five possible interpretations: 1) The post is correct, well-written information that needs no follow-up commentary. There's nothing more to say except \"Yeah, what he said.\" 2) The post is complete and utter nonsense, and no one wants to waste the energy or bandwidth to even point this out. 3) No one read the post, for whatever reason. 4) No one understood the post, but won't ask for clarification, for whatever reason. 5) No one cares about the post, for whatever reason. — Bryan Warnock, \" Re: RFCs: two proposals for change \", perl.bootstrap mailing list, 2000-08-07 In asynchronous and distributed work communications, we have much the same issue. The beautifully crafted five-paragraph briefing that you sent out this morning may have been considered manna from heaven by your recipients (if you're a manager, your recipients are usually your direct reports), and they immediately sprung into action energized by your electrifying leadership. Or maybe nobody understood a word of the unintelligible drivel you concocted, but out of respect or courtesy they are very hesitant to point this out. So in my humble opinion, there are two very simple things you can do as a manager to address Warnock's Dilemma in your distributed team: making it a habit to specifically encourage objections, and establishing a culture of acknowledgements. Encouraging objections The habit I have developed to encourage objections is to not merely ask the recipients of a message to raise questions if they have them, but to ask them to poke holes in whatever I've been writing. To that end, I have standing phrases that I use, such as: \"Do you think that sounds reasonable?\" \"Did I overlook something important?\" \"Can you think of a better way to do this?\" (Better than my suggestion, that is.) \"I'm pretty sure I'm missing something here, can you pitch in?\" \"Am I way out in left field with this?\" \"How nuts of an idea is this?\" I might use a variation on one or several of these phrases at the end of an email, but also in the comments section of a wiki page (or in individual inline comments), or even in the reply thread of an issue tracker. This serves multiple purposes: There are many individual areas of knowledge where someone on my team is more of an expert than I am. Obviously, I want those people's ideas on the table. It establishes the notion that nobody's opinions or suggestions on technical matters are sacrosanct, and we want to do the right thing in any situation, not follow the hippo. 2 It encourages others to ask for feedback in the same manner, whenever they float ideas or suggestions of their own. It establishes that there's nothing wrong with being wrong from time to time. Acknowledgements So now, on to acknowledgements, that is, what to do when you have no objections on something. Here's a general rule that I use: all communications should be acknowledged. Yes, really. Anything my team sends me, I try to reply to with at least \"ack\" or \"OK\", but frequently it's something like \"great, thanks!\" — it costs nothing to be kind. Likewise, for everything I send to my team I can count on getting the same kind of reply back. There's something I need to pass on from higher up? Or just something I want everyone to know? Out goes an email, in come a few \"ack\" replies over the next few hours. I don't even have to specifically ask anyone for acknowledgement anymore, it just happens. 3 In this context, we deliberately use written acknowledgements — as in, somebody actually types something, even if it's just the two letters \"OK\". I find Like buttons or thumbs-up or email \"read receipts\" (anyone remember those?) oddly perfunctory. This has a nice side effect, in combination with encouraging objections: someone who — while knowing that objections are always encouraged — acknowledges an idea, opinion, or plan, actually makes it clear that they are on board with it. To his credit, Dave points out in the same message that \"dilemma\" is technically inappropriate as the problem described includes five choices, not two. It's properly a pentalemma . ↩ HiPPO: the Highest-Paid Person's Opinion. Following the hippo is when you value ideas and opinions by seniority of their originator, not by correctness or factual merit. ↩ We've also codified this in my team's communications guidelines. You don't have something like that? Write them. ↩","tags":"blog","url":"blog/2021/10/30/warnock-dilemma/","loc":"blog/2021/10/30/warnock-dilemma/"},{"title":"This Meeting Should Have Been an Email","text":"Fellow managers, there is an ongoing trope in just about any software technology or knowledge based organization (and probably others, too) that goes like this: This meeting should have been an email. It's such a well-established meme at this point that you can buy mugs saying so . Or cross-stitched \"award certificates\" , or ribbons . And yet, many of you appear to dismiss it as a nerdy joke, and refuse to take the sentiment behind it seriously. And this even though you may agree that your organization has too many meetings. Even that you are in too many meetings. But you're convinced that sadly, sadly you can't cancel that meeting. Or that one. Or the quarterly financials update. Or the update about the shakeup in the CTO office. Or the meeting explaining at your level what the CEO just communicated to everyone via a video message or an email of their own. You can do it. I'm here to help. \"Email\" means any structured, written communication that allows for feedback Let's set one thing straight to begin with. The standing phrase is \"this meeting should have been an email\" because that's catchy. But that's not to say that you actually need to write an email message. What it really means is that to communicate whatever it is that you're trying to get across, you use a medium that uses written expression, allows you to formulate complex thoughts and reasoning in writing, allows people to comment and share feedback, in writing, ideally allows for that feedback to subsequently be worked back into the original writing. You'll see that particularly considering item #4, email isn't even the best option available at your disposal. Instead, you can look at the following, additional options, all of which will probably be available to you in some form: a shared flow-text document, like a Google Doc , a collaboratively edited Office 365 Word document , or a Nextcloud Text document, a page in your organization's wiki, like MediaWiki or Confluence , or even a barebones shared text editor, like Etherpad . So all of these are good. 1 All of them are better than a meeting. With near certainty at least one of them is available at your disposal. Meetings burn people's time Meetings are gigantic time consumers. And the productivity gains from switching to well-structured written communications are enormous . To illustrate, allow me to offer some first-hand experience. When I'm being called to attend a meeting, my colleagues will attest to the fact that I am a meticulous note-taker. I write meetings up in our corporate wiki, and I record notes, rather than producing a verbatim transcript. But I can guarantee you that I will write down every point that the attendees make that's worth remembering or referring back to. This includes some key points that I do record word-for-word. I've been in meetings with 20 attendees of 1 hour in length. My meeting notes never go over 2,000 words for such a meeting, and usually they're more like 1,000 words. So that means that for a meeting that burns 20 person-hours just to attend (that is, not including meeting prep), what actually gets said can be summarized in 2,000 words, tops. Now, consider that the average silent reading rate for English speakers is approximately 240 words per minute. So people can read a 2,000-word summary in under 9 minutes, a 1,000 word one in about 4. In other words, by conveying the information in writing rather than orally, you can eliminate five-sixths to fourteen-fifteenths of that useless overhead. Or put differently, replacing an hourlong meeting with a well-written briefing gives each and every person 6 to 15 times more productivity. And that's not even counting the benefits of eliminating the meeting as a forced synchronization point. But writing things up means more work for me! You may argue that although you understand that putting together a well-written briefing (instead of calling a meeting) saves everyone else time, it takes up more of your time. Let me observe this: If you've been convening and chairing meetings of an hour, and you haven't been spending about as much time preparing for that meeting yourself, then I'm sorry to break it you but you may not have been a very conscientious meeting chair all along. In fact, you may have be been rather disrespectful of other people's time, and now is a very good time for you to change. If however you have been a conscientious meeting chair and every one-hour meeting did, in aggregate, consume about one hour of meeting prep (including scheduling, collecting information, and preparing it so you have it all ready to go), then rejoice: the onerous scheduling-and-roping-everyone-in bit is gone, so that saves up a sizable chunk of your time, and you can punch out 1,000 to 2,000 words in 30-45 minutes. So, less work for you. Admittedly, not as dramatically so for you (the writer) as for your erstwhile attendees (now readers), but still pretty substantial. (Not to mention the fact that team productivity gains in the order-of-magnitude range, see above, should make your heart jump with joy.) OK but how? I don't know where to start! I've written about this before , but I'd like to come back to this again: if you're looking for guidelines on structuring your writing for what you would otherwise communicate in your meetings, look at the 5-paragraph briefing format, adapted from the NATO 5-paragraph field order . If you make it your habit to at least think about this format, chances are that your briefing will be pretty damn comprehensive: Situation Objective Plan Logistics Communications Let's break these down in a little detail: Situation is about what position we're in, and why we set out to do what we want to do. You can break this down into three sub-points, like the customer's situation, the situation of your own company, any extra help that is available, and the current market. Objective is what we want to achieve. Plan is how we want to achieve it. Logistics is about what budget and resources are available, and how we can use them. Communications is about how we'll be coordinating among ourselves and with others in order to achieve our goal. Sometimes you want to give not a full briefing, but a simple update, such as because circumstances have changed. In that case, you may only include the first three items, and the changes that apply to it. It's good practice to always include these three (that is, situation, objective, and plan): to you it may be clear and obvious that since the situation has changed, a slight modification of the plan (or the objective!) is necessary. To others, it might not. So just always include the current situation, the current objectives, and the current plan. You can also apply this to a problem statement, where it's just as useful: This is what we're currently dealing with, and how I see it (that's the situation ) Here's why it's a problem, and why it needs to be fixed (that's an objective ) This is my suggestion for how it could be fixed (that's a plan ) And finally, some poetry And as my final writing tip for improving communications and eliminating needless meetings, I want to leave you with some poetry. These lines that just so happen to serve as a perfect mnemonic for professional briefings. I keep six honest serving-men: (They taught me all I knew) Their names are What and Where and When And How and Why and Who. — Rudyard Kipling, The Elephant's Child, 1902 Strive for all your professional writing to answer most or all of what, where, when, how, why, and who, and watch your need for meetings evaporate like morning dew in glistening sunlight. Please note that interactive chat (like Slack) is not in this list. It fails the \"formulate complex thoughts and reasoning\" test. ↩","tags":"blog","url":"blog/2021/10/27/this-meeting-should-have-been-an-email/","loc":"blog/2021/10/27/this-meeting-should-have-been-an-email/"},{"title":"Please, Make My Company Distributed!","text":"After No, We Won't Have a Video Call for That , which covered how productive distributed teams operate, and the Getting out of Meeting Hell series, which focused on how you as an individual can get to being a member of a functional distributed team, let's zoom out a bit. Let's discuss a slightly larger, organizational picture, just in case you ask yourself this question: \" Why isn't my organization distributed and asynchronous? And, as companies bleed talent right and left because competent people run for the competition that does get distributed work right, why don't they wake up and become like that? Can someone please make my company more distributed and asynchronous?\" For additional context, let me interject the following observations from Kris Köhntopp , who posted them in German on Twitter (I am taking the liberty to translate here; emphasis mine): What's funny is that [what matters to being successful as a distributed company] are all learnable skills — written communications, sensible meeting prep and follow-up, correct definition of objectives and tasks, etc. That's a craft. But it appears that organizations prefer to bleed their teams dry by attrition, rather than to learn, or to hire people that can help teams to write things down, professionally maintain a wiki, and teach and drive communications. And further downthread , Kris says (again, my translation and emphasis): This is all definitely feasible, after all there's remote-first companies of substantial size and they work. They must have built themselves somehow, they didn't get to where they are by pure chance. I agree with all of that (obviously), but things are complicated. So let's drill into this a bit. Building is easy. Changing is hard. I've founded and bootstrapped a distributed company, and I've also been involved in making a localized company more distributed. Take my word for it: founding was daunting and scary and stressful, but in terms of shaping structure and communications — even if you have to figure it out as you go, as I did with my cofounders 1 — it's easy. Changing the communication structure of an established company is hard. And slow. Even if you have full support from the top. So, if you're in an organization that wasn't distributed from the get-go, do not compare it to one that was. Also, do not compare it to one that is two years ahead of yours in reshaping itself in a distributed and asynchronous manner. Also, if your company went distributed kicking and screaming at the start of the pandemic, do not compare it to one where the top leadership made a conscious decision to be more \"remote friendly\" or \"remote first\" or whatever their preferred term is, long before the pandemic hit. These organizations are ahead of yours. Their change may be happening just as slowly as yours, they just started the race earlier and are a couple of laps ahead of you. But your company hasn't even started the change to distributed and async work? You spent the last nearly two years in unfettered meeting hell? Your bosses think you'll now go \"back to normal\", because an office is the only \"normal\" they know? You know what to do. Hiring people to help: aye, there's the rub. Now on to the idea of hiring people to help. There's two ways to look at that: hiring people as employed managers to work and lead in the organization in a distributed and async fashion, or hiring people as management consultants to guide and advise the company in the distributed and async transition. (Please note: the idea of hiring only regular employees that drive a distributed and async change \"from within\" — against the resistance or inertia of established management — is ludicrous, unethical, and unworkable.) Hiring distie managers If you make it a priority to hire managers geared toward distributed and asynchronous work, then if they're worth their salt I can guarantee you that one of the first things that they'll ask in their first interview is this: \"What's my line manager's (director's, VP's, etc) distributed and async work experience? How about my lateral peers'?\" And if the truthful answer is \"they're new to this,\" it's quite likely that your prospective new management colleague will politely end the conversation. Their skills are in high demand; they have plenty of options to go elsewhere. Why should sign they up for a job that's going to come with a ton of needless friction? And of course, that consideration applies in all management positions, all the way to the top. So for this to work, in an organization currently hellbent on localized-synchronized work (I use offissification for that state of affairs), it would need at least its entire C-suite to come around first. And it would then probably need to replace at least one-third of its current managers at all levels, to have any chance of attracting fresh blood in management. You'll notice that this isn't exactly easy to do, plus it'll probably take longer than you have the patience to put up with it. Hiring management consultants So that leaves the option of hiring people that are not part of the organization but are brought in to advise, ideally on all management levels. These people are called management consultants. And you should understand that they are commonly loathed by mid-level line managers. But let's leave that aside for now. Let's assume that you want to consider the possibility of a management consultant whose communication skills and empathy and talent are so outstanding that they are absolutely not hated by anybody. Then, please put yourself in that consultant's shoes. Before they get anywhere near you, they would have spent countless hours in — you guessed it — meetings with the CEO, with top-level management, possibly with department heads, to get buy-in . And then, they embark on a multi-month project where they must use the style of work currently prevalent in your company, because that's the only way to even approach people. So they spend more time in — are you with me — meetings to convince and educate people. Oh, and they probably need to bill by the day or by the hour, in some arbitrary increment that your bean counters dictate, because otherwise they can't get paid. Async work, on your own time and schedule, much? So that means that under most circumstances, such a project will either involve a person who's actually perfectly fine with localized and synchronous work, and for this kind of project that's probably not a person you'd want to hire. Or else they're miserably unproductive throughout the project, and that doesn't exactly bode well for their health nor your project. (Which means a person understanding this probably will never start such a consultancy in the first place.) Now, I want to mention that I suppose from the perspective of a consultant, there is one way to square this circle: charge exorbitant rates. If you can make a killing working two days a month, and you actually do work just two days of meeting hell per month and take the rest of your time off to recuperate, then that might actually work. But then it's less likely — not impossible, but less likely — that your company will retain their services. Because unlike a consultant that you bring in to chop heads, monetary gains (that is, return on investment for the consultant's fees) are much more difficult to put in numbers when you're \"only\" making everyone's life better, and attracting better employees, increasing productivity, building better products, attracting more customers, and hence making a bigger profit are mere knock-on effects from that. 2 So, how do things change, then? It's my rather firm belief that these things change by evolution, not on an organizational scale but on one of market and society. Many of the companies still stuck in the office mindset will not change, at least not dramatically. They will continue to bleed talent, not attract much fresh blood, and face fierce competition from companies that do better and attract good people. Some will undergo a slow and sometimes painful transition and some will succeed at it. Some will fail and go under, or become irrelevant. But expecting dramatic, sudden change at a large organization is just not realistic. And if you're stuck in one that's still pretending that this'll all blow over, your best option is probably to make a change for yourself, than to try to wait for one in your organization. This is not to say that we invented anything, just that we had to figure out how to make things work for ourselves that already worked for others. Distributed companies existed well before my cofounders and I started a company in 2011. MySQL AB (incidentally, Kris Köhntopp's former employer) established those practices in the 2000-2005 time frame. There is an excellent November 2011 talk from ex-MySQL CEO Mårten Mickos, in which he runs through the entire history of MySQL as an independent company, until its acquisition by Sun in 2008. Around the 27-minute mark , he starts talking about work in a distributed company. ↩ And even if there is a demonstrable expected positive monetary effect, meaning in terms of numbers it's an absolute no-brainer, you should consider that top management are people, and people don't always act rationally. ↩","tags":"blog","url":"blog/2021/10/23/make-my-company-distributed/","loc":"blog/2021/10/23/make-my-company-distributed/"},{"title":"No, We Won't Have a Video Call for That: The Companion Pieces","text":"The original article continues to prompt a lot of thoughts and discussions, so I've written a a couple of follow-up pieces: Getting out of Meeting Hell is a short series about getting from a distributed workplace that attempts to duplicate the synchronous nature of an office by sticking everyone into video meetings all the time — with usually disastrous results — to one that successfully adopts an asynchronous way of working. It has suggestions for employees, mid-level managers, and executives. Please, make my company distributed! takes a broader view on changing organizations so that they become better suited for a distributed and asynchronous style of work, and why that's a lot harder than most people think.","tags":"presentations","url":"resources/presentations/no-we-wont-have-a-video-call-for-that-the-companion-pieces/","loc":"resources/presentations/no-we-wont-have-a-video-call-for-that-the-companion-pieces/"},{"title":"Universal tox tests (from just about any CI)","text":"I like tox . A lot. I use it all the time. This is a quick summary on how to use it in such a way that it becomes a central anchor point that you can use from all your CI systems. What's tox for? Normally tox is used to run tests for Python projects, and it's very well suited for that. You can use it with Python libraries, Django projects, scripts you use for system automation, whatever. But you can use it just the same for code that isn't a Python application or library itself, but a Python application just happens to come in handy for testing that code. In this example, I'll describe a super simple use case: using a barebones tox configuration that lints YAML configurations. Suppose you've got a Git repo that's full of YAML files. And you want to make sure, for example, that all your truthy values are true or false and never yes , no , on or off . Or that your indentation is always consistent. tox.ini There first thing you'll do is create tox.ini , the central tox configuration file, in the top level directory of your repository. Here's a tiny example: [tox] envlist = py{3,36,39} skipsdist = True [testenv] deps = yamllint commands = yamllint {toxinidir} That's it. What this'll do, when invoked as simply tox , is create a Python 3 venv, pip -install the latest version of yamllint , invoke the yamllint command, which will recursively check for all .yml , .yaml , and .yamllint files in the directory where the tox.ini file itself lives. What's helpful here is that tox does a little bit of magic with the testenv names. tox knows that if you call a testenv py36 , you want to test with Python 3.6 (more precisely, CPython 3.6). py39 , that's Python 3.9. Just py3 means whatever Python version maps to the python3 binary on your system. 1 Running tox on every commit Now the first thing you might want to do is run tox on every commit, and encourage your collaborators to do the same. You can easily do that by dropping this tiny shell script 2 into your repo as a file named pre-commit in the .githooks directory: #!/bin/sh exec tox -e py3 Add that file to your repository as .githooks/pre-commit , and make it executable. Also, add a little note to your README explaining that, to enable the pre-commit hook, all your collaborators can simply run git config core.hooksPath .githooks Easy, right? And once you've run that command, every git commit will kick off a tox run and you'll never commit borked YAML again. 3 Now of course, using those hooks is entirely optional, and can be overridden with --no-verify . So, for those slackers that can't be bothered to use them, you also want to check centrally. Here's where your CI comes in. Running tox on every GitHub PR If you collaborate via GitHub, you can run tox on every PR, with a simple GitHub Actions workflow. To use it, you'll need a small addition to your tox.ini file: [tox] envlist: py{3,36,39} skipsdist = True [gh-actions] python = 3.6: py36 3.9: py39 [testenv] deps = yamllint commands = yamllint {toxinidir} And then, you add a workflow to .github/workflows , say .github/workflows/tox.yml : --- name : Test with tox 'on' : - push - pull_request jobs : build : runs-on : ubuntu-latest strategy : matrix : python-version : - 3.6 - 3.9 steps : - name : Checkout uses : actions/checkout@v2 with : submodules : true - name : Set up Python ${{ matrix.python-version }} uses : actions/setup-python@v2 with : python-version : ${{ matrix.python-version }} - name : Install dependencies run : | pip install tox tox-gh-actions - name : Test with tox run : tox So that sets up your workflow so that it tests with two different Python versions that you care about, and then runs a test with each of them. It does this via a combination of the information contained in the [gh-actions] section of tox.ini , and the matrix strategy defined in the workflow. The tox-gh-action plugin then pulls that information together and sets up testenvs as needed. And it runs these checks every time you push to a branch (topic branch or default branch), and also on every pull request. Running tox from GitLab CI So you're either using only GitLab and not GitHub, or you're mirroring a GitHub repo to a self-hosted GitLab and want to run your pipelines there as well? Easy. Here's the exact same functionality for your .gitlab-ci.yml file: 4 --- py36 : image : python:3.6 stage : build script : - pip install tox - tox -e py36 py39 : image : python:3.9 stage : build script : - pip install tox - tox -e py39 In GitLab CI I know of no elegant matrix syntax to map the image version to the testenv. But on the other hand there's a bunch of things that \"just happen\" in a GitLab CI pipeline, which you specifically need to define in a GitHub Actions workflow definition. So overall your .gitlab-ci.yml ends up shorter than your GitHub Actions tox.yml . Running tox from Zuul If you're running a tox testenv from Zuul , you would use the built-in tox jobs in your pipeline, as referenced in .zuul.yaml : --- - project : check : jobs : - tox-py36 - tox-py39 gate : jobs : - tox-py36 - tox-py39 Here, the tox-py36 and tox-py39 environments are both derivatives of the base tox job, which will run with cPython versions 3.6 and 3.9, and by default invoke testenvs called py36 and py39 , respectively. And now? Now that all of your Python testing standardizes on tox, you can go to town. Add more tests, add more testenvs, more Python versions, whatever. You might need to make minimal changes, like add one line for each new Python version you want to support, to all your CI definitions. But if your project moves from GitHub to GitLab or from GitLab to Gerrit/Zuul, or your entire company goes on a great big CI migration, then you'll have one less thing to worry about, because your tests already run anywhere. By the way: when you set up your tox.ini and your CI configuration files as shown in this article, then yamllint will of course also lint your YAML CI configuration files themselves. Which comes in handy; I found 4 yamllint warnings and one error while testing the examples I've given here. Testing with multiple Python versions may seem less than useful when you're dealing with just one upstream package, yamllint . I use that here as an oversimplified example. As soon as you add your own Python scripts or modules to the tox checks, you may very well be interested in multiple python versions. ↩ If you're being a purist, you could also invoke the tox runner from a Python script. I prefer the shell exec one-liner. ↩ In this case, for testing locally, we're not going to care about a specific installed Python version. We'll just make sure that the commit doesn't obviously break anything. In my humble opinion it's OK to catch version-specific issues in CI, but we shouldn't feed the CI code that's outright broken. ↩ This example assumes that you're either using shared GitLab runners using Docker, or a self-hosted runner on Kubernetes. ↩","tags":"hints-and-kinks","url":"resources/hints-and-kinks/universal-tox-tests-from-just-about-any-ci/","loc":"resources/hints-and-kinks/universal-tox-tests-from-just-about-any-ci/"},{"title":"On Contravictions","text":"This is a talk I submitted 1 to DevOpsDays Tel Aviv 2021, which used a non-anonymized CfP process via PaperCall . This submission was rejected. Title On Contravictions Elevator Pitch You have 300 characters to sell your talk. This is known as the \"elevator pitch\". Make it as exciting and enticing as possible. A contraviction is when a person firmly believes that two objectively mutually exclusive standpoints are simultaneously true. Being contravinced makes you extremely vulnerable to manipulation. Here's how to spot a contraviction, and what to do about them. Talk Format What format is this talk best suited for? Talk (~25-40 minutes) Audience Level Who is the best target audience for this talk? All Description The description will be seen by reviewers during the CFP process and may eventually be seen by the attendees of the event. You should make the description of your talk as compelling and exciting as possible. Remember, you're selling both the organizers of the events to select your talk, as well as trying to convince attendees your talk is the one they should see. \"Contraviction\" is a term I use for when a person is firmly convinced of two sides of an obvious contradiction. This may sound like it would be an unusual and rare occasion, and yet, once you start looking, they are all over the place. A few examples: At the core of Nazi ideology in Germany in the 1920s and 30s was the notion that Jews are engaged in a successful global conspiracy to subjugate all nations including the German nation, and that Jews were also, simultaneously, socially, intellectually, economically, and morally inferior to Germans. At the core of religious extremism is the belief that God is all-forgiving and merciful, and also that as long as a portion of humanity (\"infidels\") displeases God, all of humanity must suffer God's wrath. We see this in contemporary Islamic extremism, but Catholicism in the 16th century did no better in the conquest of Latin America, nor did Western Christianity do much differently during the medieval crusades. At the core of Trumpism is the notion that the United States of America is the greatest nation on Earth and that there is no better nation nor will there ever be, but also that America has been ruined by \"the liberals\" and has slipped into inferiority, so that it is necessary to \"make America great again.\" In all these examples, both statements can logically be false, or one of them could theoretically be true — but if one is true, the other one must be false. And yet, people ( millions of people!) hold or held both of these statements to be true, simultaneously. But being contravinced puts people in a very vulnerable position: if someone gets you to believe both sides of a contradiction, they can logically argue anything to follow from either one side, or the opposite. Which means they can convince you of anything. And that never ends well. This talk defines contravictions, highlights examples (even devopsy ones!) and provides suggestions on how to uncover and dismantle them. Because contravictions have the potential to poison and destroy discourse, and that's a cultural issue we all need to deal with — among friends, family, and coworkers. Notes Notes will only be seen by reviewers during the CFP process. This is where you should explain things such as technical requirements, why you're the best person to speak on this subject, etc… Given the fact that the nature of this topic is sensitive and emotional — yes I, an Austrian, will be talking about Nazi ideology, in Israel, consider me terrified — I'll need to submit this on the condition that I'd only want to deliver this talk in person. If that does not permit itself on account of the Covid-19 situation or of travel restrictions, and I would have to rely on a streamed talk and have no way of reading the room or scanning the audience for body language feedback (some more details on this topic here ), then I'd rather not give the talk at all, rather than stream it. If this disqualifies the talk (or if you just consider it too controversial to begin with), no hard feelings at all. Tags Tag your talk to make it easier for event organizers to be able to find. Examples are \"ruby, javascript, rails\". Culture, Communications If you're curious why this is here, please read this . ↩","tags":"talk-submissions","url":"talk-submissions/devopsdaystlv-2021-contravictions/","loc":"talk-submissions/devopsdaystlv-2021-contravictions/"},{"title":"The Review Review","text":"This is a talk I submitted 1 to DevConf.CZ 2022, which used a non-anonymized CfP process via Red Hat's CfP website . For that conference, it was selected as the lead talk in the Modern Software Development track. I had previously submitted this talk to DevOpsDays Tel Aviv 2021, which used a non-anonymized CfP process via PaperCall . That submission was rejected. Title The Review Review: comparing code review, testing, staging and deployment across development collaboration platforms Elevator Pitch You have 300 characters to sell your talk. This is known as the \"elevator pitch\". Make it as exciting and enticing as possible. GitHub, GitLab, Gerrit — what should I choose? What's the best review process, the best CI/CD integration, the best deployment facility? Which should I select for my startup, or consider migrating to? Which supports good collaboration practices, which bad ones? This talk gives the run-down. Talk Format What format is this talk best suited for? Talk (~25-40 minutes) Audience Level Who is the best target audience for this talk? Intermediate Description The description will be seen by reviewers during the CFP process and may eventually be seen by the attendees of the event. You should make the description of your talk as compelling and exciting as possible. Remember, you're selling both the organizers of the events to select your talk, as well as trying to convince attendees your talk is the one they should see. In DevOps, the process of collaborative review, testing, staging, and deployment to production constitutes a core element of the work we do. And we generally strive to make this process as effective, efficient, smooth, and transparent as possible. Achieving that partly comes from the work culture we shape and inhabit, partly from our selection of tools — and of course, work culture and work tools permanently and closely influence each other. This goes for both the tools that drive review, and the tools that drive CI/CD: the GitHub Pull Request process in combination with GitHub Actions ; the GitLab Merge Request process in combination with GitLab CI ; the Gerrit Review process in combination with Zuul . None of these is perfect, all of them have their advantages and disadvantages under particular circumstances. Some are meant to be used principally as a service, some are fine to self-host. Some are adamant about enforcing specific deployment practices, some follow a more relaxed approach. This talk is a summary of the current state of affairs with all these tools, and contains recommendations on what to use under which circumstances. Notes Notes will only be seen by reviewers during the CFP process. This is where you should explain things such as technical requirements, why you're the best person to speak on this subject, etc… My team and I have worked with all tools mentioned in a professional capacity, and I believe I've got a very good understanding of the relative merits of the systems presented. This does not include a hard-and-fast recommendation for one particular tool or platform. This is a talk that's suitable for both in-person and on-line events. Tags Tag your talk to make it easier for event organizers to be able to find. Examples are \"ruby, javascript, rails\". GitHub, GitLab, Gerrit, Zuul, CI/CD, Development, DevOps If you're curious why this is here, please read this . ↩","tags":"talk-submissions","url":"./talk-submissions/review-review/","loc":"./talk-submissions/review-review/"},{"title":"Getting out of Meeting Hell: As a top-level executive","text":"Please have a look at the introduction for background, for applicable disclaimers, and for information about the specific environments this series talks about. This part of this series is for you if you are a chief executive officer, a managing director, executive director, or whatever else the top-level role in your organization may be. This means that you have people who report to you, but you don't report to anyone in the day-to-day operations of the company — even though you may, of course, be answerable to your board of directors or your investors, or some oversight body. So, you realize that a lot of your company spends a lot of time in useless meetings, they're often forced to sit through 90 minutes of staring into cameras when they could instead have spent 5 minutes reading an email, people's productivity suffers badly because they are constantly being interrupted. And this is badly affecting you, personally, as well. So for the benefit of yourself and everyone else in the organization, you want to change things toward being less interrupt-driven, less synchronous, more productive, and healthier. Now, I've got some bad news for you. In contrast to your employees at any level, it is much harder for you to pull the Leave option than it is for them. In contrast to most of your mid-level managers, no matter how hard you try, you may never escape being in a lot of meetings, and being in a lot of unpleasant meetings to boot. Chances are, whenever stuff is exploding, boiling up, or otherwise going bonkers, you'll be roped in to calm things down, make a decision, or soothe a high-profile customer who is conniptiously trying to convince you that your SLA is akin to the Constitution and that an engineer guilty of causing a violation ought to hang for treason. But, and this is the good news to balance the bad, if you manage to pull the rest of the company out of meeting hell, life is going to get way better for you, too. So, obviously, there'll be a lot for you to Learn . If you've never led an organization that was distributed and asynchronous by default, there's a lot to unpack, understand, and overcome when it comes to turning one that isn't , into one that is. But your most important role at the top of the organization is this: Lead. And by that I mean lead by example, and also lead by policy. Here are a few ways you can lead by example: Write. Particularly when you want to communicate something to the whole company. All-hands video streams? [stage whisper] Everyone hates those. Meeting invites saying \"we'll anounce something important\"? Just write an email saying the important thing, and then ask people to send you questions by a deadline. And publicly commit yourself to a deadline by which they can expect answers. Take no shortcuts. If you're setting up communications rules for everyone, live by them yourself. There's a bad, hidden cost in skirting around them. Make a point of always giving context when pinging someone in chat. Quit calling people without warning — you can't convey the context of the call without them answering. A cold call is the ultimate naked ping . Generally, cut back on synchronous, realtime communications. They always interrupt people, and interruptions are expensive . Think about how much money you'll lose from someone not having a brilliant idea because you pinged them about some technicality in the company chat while they were deeply immersed in a complex problem. (Also, think about how much money you might have already lost that way.) Insist on agendas being drawn up and meeting notes being kept (and both circulated to anyone who needs the information) for every meeting you are asked to attend. Write an agenda for every meeting you call or chair. If you have a group that meets regularly, appoint a different person from the group as a scribe each time. Do not ask a person to volunteer. (If you do, chances are that one person in the group will be typecast as the scribe for all such meetings. I call such an unfortunate person a scrapegoat. You don't want scrapegoats.) Then, here are ways you can lead by policy: Hire professional writers. (In the software industry, tech writers come to mind.) Not because you want those people to be scrapegoats, but because professional writers can massively move your organization forward in efficiency of written expression, document organization, and clarity of communications. Make them a hiring priority. Pay them handsomely — good tech writers can make good money freelancing; you'll need to make them a pretty compelling offer to consider giving up some of that freedom. Ram a stake in the ground making clear to everyone you will not tolerate corporate surveillance or invasions of privacy. Any attempt to introduce an \"always on camera\" policy should be grounds for reprimanding the manager that instigated it. Insist on the procurement of tools that take distributed, asynchronous work into account. A video conferencing system that values surveillance over privacy is not one your company should throw money at. A hiring platform that requires specifying an \"office location\" for every role, and has no provisions for remote positions, isn't either. Neither is a chat platform hosted by a company whose sole chance at long-term success is to pull all corporate communications into synchronous chat. Yes, these judgments require technical expertise. If this is something you don't have because you consider yourself a \"non-technical\" person, I have something to read for you . There's another thing that you might be inclined toward doing: declaring meeting-free days, as in making it a policy that no meetings are to be scheduled on Wednesdays. I think of that as very much a stop-gap measure that's often done out of sheer despair. Sure, you want your people to have meeting-free days, but you actually want them to have meeting days, with meetingless days being the norm, rather than the exception. I think you're better off gradually replacing your meeting-addicted managers with ones that are accustomed to distributed and asynchronous work. Finally, and this may be a bitter pill to swallow: getting to distributed and asynchronous is infinitely harder if you have built a \"flat\" organization. If every one of your managers has 20-30 direct reports, they will feel utterly overwhelmed at the thought of staying in asynchronous communication with every one of them, not to mention the fact that it's damn near impossible for them to convince that many people at one time to adopt a new way of collaboration. This is one of the many, many ways in which flat organizations fail to scale , but this gives you the opportunity to fix two issues at the same time. If you've got one manager that's actively promoting meeting hell that is currently making life miserable for 20 people, you can split that team up into 4, and hire 3 team leads that know how to work asynchronously and distributed (or promote people who have that sort of experience and want to step up). Then, in the one remaining team with the unreformed meeting addict, things can go three ways: The one \"traditional\" team sees how things go in the other teams, and they quickly coax their team lead into doing things the new way, or they all ask to be transferred, or (the worst-case scenario) the one meeting-addicted manager continues to annoy everyone, and they all leave. That's bad, and a failure of judgment (that manager should probably have been let go first), but losing 4 good people, as difficult as that might be, is probably \"better\" than losing 20. As a final thought, please take a moment to put yourself in other people's shoes, and understand what options they have with being stuck in meeting hell. Whether it's your line managers or your regular employees , they both have the option to just chuck in their notice and leave, and leave they will, if they're sufficiently deep in meeting hell. They have plenty of opportunities. So lead by example, and lead by policy. Do things right, but more importantly do the right things. This article concludes the series — for now. I am guessing that people reading this will have opinions, air their grievances, and share feedback. Those usually give me good thinking material to dwell on, so I'll probably have an additional installment based on reader feedback at some point.","tags":"blog","url":"blog/2021/10/03/getting-out-of-meeting-hell-executives/","loc":"blog/2021/10/03/getting-out-of-meeting-hell-executives/"},{"title":"Getting out of Meeting Hell: As a mid-level manager","text":"Please have a look at the introduction for background, for applicable disclaimers, and for information about the specific environments this series talks about. This part of this series is for you if you are a manager at any level of your organization except the very top. In other words, you have reports, but you also report to someone. And as such you may be stuck in meeting hell in two ways: With your own team, that is to say, yourself and the people reporting to you. With your management peers, in other words, the people that manage other teams and report to the same director, VP, or whatever other fancy titles your company might use. And I'll cover both of those angles, but before I do, I should note that you, of course, have one option to improve your personal situation that is also available to your reports : Leave. It's not the only option you have, and it may not be your first option, but an option it is. And, for your personal development, taking on a management role in a company that does distributed work and asynchronous communications right might be an excellent career move. If you are a conscientious manager and you feel a sense of responsibility and obligation to your team, and this makes you hesitate to consider the option to leave, then that reflects nobly on your character but know this: that responsibility is contigent on everyone's employment in one organization, and it ends the day your employment contract (or theirs) terminates. That may sound harsh, but that's the breaks of the game, and you shouldn't let that hold you up, if leaving is the objectively most sensible option for you. There's no use burning out over an exaggerated sense of duty. In addition, I'd posit that you should leave your company if it employs or promotes employee surveillance, keeps voodoo \"meeting engagement metrics\", or engages in otherwise harmful or toxic behavior. I'd argue that in those cases you should also make sure your direct reports know why you're leaving, to the extent that your contractual obligations allow you to tell them. 1 However, if you've chosen not to leave your current organization, and you want to actually do the hard work for making work better for yourself and your distributed team, here is your paramount obligation: Learn. If there's anything that the coronavirus pandemic has shown about organizations making the (admittedly abrupt) transition from localized to distributed work, it's this: it's a challenge for managers much more than for non-manager employees. You have to make a conscious effort to learn how distributed work works, and how you manage a distributed team. But, chances are that you won't be limited to watching talks or listening to podcasts or reading articles (you might have read this one before you landed here). Instead, it's not unlikely that if you're doing software or technology development, some of the people on your team are already well versed in distributed collaboration and asynchronous communications. Yes, I am of course talking about open source contributors. It absolutely does not matter what kind of open source software some on your team have worked or helped out on, or whether it's in any way related to the products your company builds. Everyone who's ever landed a pull request probably has had to learn more about distributed, async communications than any office dweller did who never has. Find those people. Talk to them. Learn from them. You'll also have peers in the industry, outside your company. Managers who've already done the work. They can't do the learning for you, but they can you show a few tricks of the trade. And I'll give you one, straight away: in a company stuck in Meeting Hell, you change things for your team first, and don't even think about people at or above your level. I would not advise taking your ideas up the chain without the confidence of knowing that what you are suggesting works in your team. Mid-level management is unfortunately often full of \"great idea, but it'll never work here\" or \"that might sound nice, but I'm afraid it doesn't fit our culture.\" You want to run silent, run deep (if you permit me a strained metaphor). When your team lands its first great big success, that's when you casually want to drop something like \"we did this on one meeting a week.\" So, how can you get there? Here are a few ideas. Eliminate daily standups. Limit yourself to one scheduled team meeting a week. Keep it to approximately one hour, give or take a few minutes. Schedule it near the start of the work week (meaning, in most countries, on Monday). Have an agenda for your team meeting (and all other meetings) that is available in a collaboratively editable document, cross-referencing all open tasks. Have this agenda in a place where everyone can find it, and have it ready no later than 15 minutes prior to the meeting. During the meeting, transform the agenda into your meeting notes. This is something a collaboratively editable document (like a wiki page or Google Doc) makes easy. Appoint a responsible note-taker or scribe, whose job it is to compile the meeting notes, edit minor glitches and errors, and then let you know the notes are ready for consumption. You may decide that everyone who is in the meeting can co-edit the notes, and this can be very beneficial to make sure that even the quiet voices are heard. But it's the scribe's responsibility to bring them into good shape, and it's your responsibility as the team lead to make sure they're truthful and accurate. Spend the first 15-20 minutes of the weekly meeting on non-work topics. Ask something like \"how was your weekend\" or \"how are things going in your part of the world\" or anything else that you can think of that helps you feel the mood of your team. These things don't go on the meeting record. Always accept when people don't want to give an answer. They may have many reasons, none of which are necessarily your business. Trust that they'll share whatever they're comfortable sharing, even if some weeks that's nothing. Learn to keep each other appraised of your relative progress and status via a simple, central coordination facility, such as a virtual Kanban board. Learn to write to be understood , and encourage your team to do the same. Do not attempt to get back to people \"immediately\", and do not expect people to get back to you promptly. A reasonable default expectation for reply times is 24 hours. If you need an answer faster, you can always send an email (or better still, a comment on a card or ticket) with a request like \"if I could get this by 1600 UTC today, that'd be excellent.\" Bonus points if you add why you need that information quickly, as in \"… so I can put this together with Kim's performance data and make a decision on which load balancing strategy we'll use, which Alex needs to know by tomorrow.\" Establish a convention for marking things as urgent, when needed. The word URGENT , included in capital letters in an email subject or a chat message, usually works well. Only use this if things are on fire. 2 Never use the urgency marker just by itself. (That's a naked ping on steroids.) Also, when someone legitimately uses the marker in communication with you, honour it. When someone uses it for something other than a legitimate reason, take them to task. Get used to referring back to written communication, such as ticket comments or the record of a meeting, weeks or months later. This is not \"pulling out the receipts\" or \"quoting chapter and verse\" at someone. It simply serves to establish that yes, we have a written record of just about everything, and there is no need to keep these things stored in our heads under a banner saying \"must not forget\" (the latter being a major contributor to personal stress). Don't attempt to hide your own mistakes and misjudgements. They're on the record. That makes them a learning opportunity. Gradually establish firm communication rules with your team. Many of these won't need to be autocratically decreed, because they just make sense and people tend to intuitively agree on them (like the just-mentioned URGENT marker and the circumstances under which to use them). Then, make sure you stick to them . Stick to them yourself, and correct others who break them. 3 Importantly, let your team know that you're working on getting them all out of meeting hell. Don't sugarcoat things if you're struggling while swimming against the company tide, at least temporarily. If they know you're actively striving to make their work situation better, more productive and healthier it may well make the difference between them leaving and staying on board. Finally, keep an eye on corporate policy and the messages you get from company leadership. If they're obviously stuck in 1995, you might as well go and update your CV. If the people at the top do make an effort to implement distributed and async friendly policies , however, then things may not change overnight, but there's hope that change they will, and your effort will not be for naught. By the way: if your non-disparagement clause is so harsh that you can't talk about anything negative in the company, then that's probably an awful company to begin with. ↩ Note that the convention of the URGENT marker also establishes the convention that things are not urgent by default. ↩ Failure to enforce sensible collaboration rules may result in the inadvertent creation of an asshole filter . \"Oh but I can't enforce that rule on Bob, because Bob thinks rules aren't for him but he's brilliant!\" — No. Don't tolerate brilliant jerks . ↩","tags":"blog","url":"blog/2021/10/02/getting-out-of-meeting-hell-managers/","loc":"blog/2021/10/02/getting-out-of-meeting-hell-managers/"},{"title":"Getting out of Meeting Hell: As a regular employee","text":"Please have a look at the introduction for background, for applicable disclaimers, and for information about the specific environments this series talks about. So, you're a regular employee (sometimes called an Individual Contributor or IC), who doesn't have a team that reports to you. And you are in an organization that, although it does have remote employees or may even have switched to an all-distributed mode on account of the pandemic, does not adopt basic rules of distributed work and asynchronous communications. You spend more time in meetings than is healthy, your productivity is severely impacted, you're stressed out from frequent interruptions, you have trouble getting into a state of flow . You may be asked to keep your camera on for hours at a time, and you feel that this badly encroaches on your privacy or sense of personal space. Your manager or your peers like to naked-ping you or send you \"hey, I need you for a minute\" one-liners that disrupt your train of thought for half an hour. And your organization (including your peers and direct manager) tolerate or condone this kind of collaboration. If that sounds like your typical day, I have great news for you. You have at hand an excellent opportunity to improve your personal situation, be more productive, and communicate better with your peers: Leave. Hand in your notice. Quit. Hightail it outta there. If you want to truly improve your personal situation and work effectively in a distributed team, and neither your direct manager nor the top leadership of the organization takes any interest in optimizing for distributed and asynchronous collaboration, then the way to do that for yourself is to leave your current organisation behind, and go elsewhere. There's plenty of employment opportunities for you to pick up in this industry at this time, and there are a number of organizations that handle distributed work better than the one you're currently in. Sure, you could consider trying to change the system from within. And there is a small (I'd say minute, but nonzero) probability that you'll be successful, and drive real change in your organization. However, there is an inordinately larger probability that you'll burn out in the process — and endangering your health is never a gamble that's worth taking. You may think that that's a pessimistic view. I think the opposite is true. You're not \"failing\" to drive change in an organization that resists it, instead you can succeed at pushing change in an organization that embraces it. Throwing your energy into meaningful change for the better is the definition of optimism, in my book. And if the place for that is elsewhere from where you are now, seize that opportunity! However, note that the foregoing is all predicated on the entire company (including your direct manager) being fine with synchronous work and people having their calendar jammed with back-to-back meetings, with no apparent intent to change. If it's just you that wants change, or perhaps only you and people at your level in other teams that you can't reasonably join forces with, I'd argue that you just shouldn't be risking your health. In contrast, if your manager 1 and a couple of your peers are on your side — consider your manager may be in their own learning phase about distributed and async collaboration — then things are a lot less clean cut, and maybe you'd want to stay on for the ride. Particularly so if there's an active, supportive push from the top of the organization. Even if your manager and some of your peers are on your side, I can think of a few situations where you should still leave: like when your company has an \"always on camera\" policy, or doesn't allow people to go audio-only on calls, or promotes number crunching on employee \"engagement\" in a meeting app . Companies doing that are, in my view, beyond salvation and will never be able to attain the levels of trust required for a distributed organization to function. So: if you're stuck in meeting hell with nobody on your side, as a regular employee with no reports, your best way forward is probably the way to the exit. If you're stuck in meeting hell now but there's a gale blowing the company into async & distributed mode, and/or you have a very thick-skinned manager that gets it, and will deflect and absorb any pressure from meeting-addicted higher-ups, you may want to hang on for the journey. I'd posit that the only manager that really matters in this scenario is your direct line manager. That person would your make-or-break partner in any transformation of how your team collaborates. If you and they don't share views, then don't expect to be able to play four-dimensional chess by forming an alliance with some other manager who you then expect to influence your manager so that everyone gets better at distributed and async work. You don't want to trade being stuck in meeting hell for being in stuck in meeting hell and corporate politics. ↩","tags":"blog","url":"blog/2021/10/01/getting-out-of-meeting-hell-employees/","loc":"blog/2021/10/01/getting-out-of-meeting-hell-employees/"},{"title":"Getting out of Meeting Hell: What this is about","text":"In 2020, I presented a talk at FrOSCon titled No, we won't have a video call for that: Communications for distributed teams. In early 2021 I put together a full-length writeup of that talk, and published it here . And for no apparent reason that article then made it to the top of Hacker News on one day in September 2021, 1 and apparently resonated with quite a few people . And after that, I got a fair number of questions along the lines of \"okay, what you talk about is a spot-on description of how I want to work, but how do I get there?\" In other words, what can you do in order to transition from a team that's stuck in meeting hell, to one that actually goes fully distributed and embraces asynchronous communications? Note: I use the shorthand Meeting Hell for a situation in which people are forced to be in unnecessary and unproductive video meetings 2 for an unhealthy fraction of their work time. This is admittedly a mere symptom of not having adopted asynchronous and distributed ways of working, but it's such a tell-tale sign thereof that it counts as a dead giveaway. Thus, I think it's okay to say \"I'm stuck in meeting hell\" when what someone means is really \"I work in an organization that's failing badly at distributed and asynchronous work.\" So what I'm doing here is offer suggestions for getting out of that. If you're one of the people that has 25 meetings a week, or you spend 60% of your work week in standups and planning and retro, or the only time you have for doing what you actually signed up for is in overtime , and it's grinding you down, and you want to work differently, then this series might be for you. Personal strategies: one size does not fit all I think it's very important to differentiate personal strategies for getting to distributed & async, based on your position in the company or organization. So, I'm going to look at it from three angles: Your options if you are what some companies call an \"Individual Contributor\", or IC. In other words, these are for you if you are a regular employee (or contractor), and you're not personally responsible for other people — in other words, you have no reports. Your options if you are at some level of management that is not top organizational leadership. That is to say, you have people that report to you, but you also report to someone. Your options if you're a top-level executive , meaning you're a Chief Executive Officer or Managing Director or Executive Director or something of the sort. You have people that report to you, but you don't directly report to anyone — even though you may be answerable to a Board of Directors or some other oversight body, of course. These are my views, not self-evident truths Now, I'm only going to talk about my industry (software-driven technology) because that's the only industry I feel remotely qualified to talk about. Also, I have never worked in a company that had more than 3,000 employees, and I feel most at home in small outfits under 50. I'm reasonably confident that what I talk about is somewhat useful for companies from 3 to 3,000 people in the software industry. It may be applicable elsewhere — larger companies, other industries — but at any rate, I make no guarantees of any kind. Feel free to adopt an entirely contrarian position. And, you should also know I am not a scientist, so none of what I write is informed by rigorous empiricism. So, are we cool with that? My opinion, my thoughts, my views — not pronouncements of absolute truth. Let's get started. For context: it was at the top for like three hours on a Saturday morning. So it might have landed there just because enough people were simultaneously bored enough to give it a read… ↩ Meeting Hell might also apply to excessive in-person meetings in a shared work space, such as an office. However, I really don't believe my industry will ever go back to defaulting to office-based work, now that it has shown for nearly two years that it can function, in principle, in a default-distributed mode. Thus, I am using the word \"meeting\" as synonymous with \"video meeting\" in this series, and I wouldn't be surprised if this eventually became the norm. Think of this as akin to the transition undergone by the word call: before the advent of the telephone, a call was a visit you paid to someone's house. Then, if you metaphorically \"visited\" someone by telephone, that was a telephone call . Now a call is always a phone call, except when explicitly specified as a house call. ↩","tags":"blog","url":"blog/2021/09/30/getting-out-of-meeting-hell/","loc":"blog/2021/09/30/getting-out-of-meeting-hell/"},{"title":"No, We Won't Have a Video Call for That: Reader Feedback","text":"This is a short summary of selected reactions to the original article . Twitter, 2021-04-10 On Twitter, Michael K Johnson made an interesting point in response to this article: My way of thinking about DMs is a little different, or maybe we think differently about confidentiality. Work goes in public, meta-work is often about relationships, and I want my reports to be 100% confident they can bring any question or concern to me. So \"encrypted email\" is not the right metaphor or measuring stick in my view. DMs are a tool for saying what you aren't comfortable saying \"out loud\", but that shouldn't be about the work itself. Never \"how do I do this?\" But \"advice pls about how to work with fred?\" — yes! I entirely agree with the sentiments behind this; I still maintain that chat DMs are not necessarily a good approach for addressing this, for reason that some chat systems give merely an illusion of confidentiality. If both participants in a conversation use OTR encryption over a protocol like IRC or XMPP, inadvertent disclosure to a third party is highly unlikely. Slack DMs? I wouldn't be so sure . If your report confides in you, you don't want them to have to worry if their message is really just between you and them. Hacker News, 2021-09-25 On 2021-09-25, a link to this article ended up being the top post on Hacker News for a few hours. You're very welcome to read through the 200-odd comments in that thread, but here I'd like to respond to just a couple. I'm deliberately only picking out ones where I feel like clarification on my part is necessary; as far as differences of opinion are concerned I'll be happy to let those stand. The article also assumes every one is a native speaker who can write quickly and clearly in a chat – in a lot of international projects this is not the case. — comment from papaf My native language is German, and neither at the time of presenting the original talk nor at the time of writing this update did I work with anyone who is a first-language English speaker. I have an objection to the author's blanket disregard for \"pings\" in chat - while the request could/should be worded a bit more clearly than just \"ping\", IMHO they're a valid way of requesting if an opportunity for synchronous communication is available in the (not that rare!) cases where asynchronous communication would be worthless. — comment from PeterisP My blanket disregard is for naked pings, not for pings in general.","tags":"presentations","url":"resources/presentations/no-we-wont-have-a-video-call-for-that-reader-feedback/","loc":"resources/presentations/no-we-wont-have-a-video-call-for-that-reader-feedback/"},{"title":"Rules are rules","text":"I have a reputation among my colleagues that I am very strict and rigid about the communications rules I follow for myself, and for my team. That reputation is entirely deserved, and I am indeed not particularly flexible in my strong preference for asynchronous, non-interruptive communication methods. I have discussed elsewhere , and at length , what those are and why I have that strong preference. But I haven't really outlined why I very rarely allow myself, or others if I can help it, to deviate from it. But Florian, the complaint usually goes, can't you occasionally make an exception? Sometimes there's something you could easily do to unblock someone else, if they could only quickly chat you up and ask you to jump in. You'd be in and out of there in no time. Let's use an analogy here. Suppose you're out of an indispensable food product, say milk or flour or potatoes or eggs. So you make a quick run to the grocery store, and because you don't live in a country with proper cycling infrastructure and the store is out of walking distance, you drive. You arrive at the grocery store parking lot, and for some unfathomable reason it's chock full. Completely packed. Not a single spot available. Except those two spots right near the store entrance that are reserved for wheelchair users, which you are not. You don't see a single car around with a wheelchair plaque or decal. Not even one. Now. When you look at the situation, the objectively simplest and most practical solution is for you to park in a wheelchair spot. You know exactly how long it takes you to buy a carton of eggs; you'd be in and out of that place in two minutes. The chance that one car needing a wheelchair spot arrives in exactly that time is minute, and even then there would be another one available. The probability of two wheelchair users arriving in their cars, simultaneously, in those two minutes, is infinitesimal. There is an overwhelming probability that you will vacate the spot again, without it ever being needed by one of its intended users while you were occupying it. So, why not use it? The answer is that if it's OK for you to break the rule that that spot is for wheelchair users only, there is absolutely no reason why it shouldn't also be OK for everyone else. You're not special, the same rules apply to you as to everyone else — so if we were to decide that this rule doesn't apply to you this very minute, then it also needn't apply to anyone else under similar circumstances. And then, promptly, we're in a situation where everyone flouts the rules, and an actual wheelchair user can no longer do their grocery shopping. That's why you, if you are not a wheelchair user, shouldn't park there, and nobody else that isn't shouldn't either. And with interruptive communications — such as pinging someone in a chat when you could send them an email just the same — it's much the same way: if you needlessly ping me and I acquiesce, drop the thing I'm doing, and focus on your interruption instead, it would be unfair of me to not do the same for somebody else. And I know if everyone does this to me, my work day is purely interrupt driven and that's awful. The same goes for tolerating interruptive communications towards my team — or, worse, engaging in such interruptive communications myself. Are there exceptions to this rule? Ah but of course. Let me take you back to the grocery store. Suppose someone had a heart attack or other major health emergency that struck them down right as they were exiting the store and walking back to their car. Would anyone — including a wheelchair-using motorist that arrived just at that moment to do their shopping — complain if the ambulance parked across both wheelchair accessible spots, if that was the only practical way to get closest to the patient? I hope not. And caring for the patient and stabilising them for the trip to the hospital would surely take longer than your two-minute egg procurement dash that we discussed earlier. Again, this has a parallel in interruptive communications in a (much less dire) regular work situation: stuff is actually on fire? Or there's something that for some legitimate reason needs doing right now that only I or someone on my team can do, or is most comfortable with? Ping me in chat, use the back channel, give me a ring on my phone for cryinoutloud, whatever it takes to get my attention. Nobody will hold that against you, least of all me. And in this case that's a rule that I can also easily apply generally, treating everyone around me fairly and equitably: when stuff is urgent and seriously overrides the priority of what's currently being worked on, you get to interrupt me or, if necessary anyone on my team. (Though I would prefer that you interrupt me specifically, and I can decide whether we really need to mobilise another person.) Just don't abuse that. If you do, you'll just condition people into taking your sense of \"urgent\" with a big pinch of salt. Edit, 2021-09-21: My colleague Jean-Philippe Evrard has suggested that I refer to another, much more elaborate article on a similar subject: Siderea's The Asshole Filter . I recommend you give it a read if you're inclined.","tags":"blog","url":"blog/2021/09/16/rules-are-rules/","loc":"blog/2021/09/16/rules-are-rules/"},{"title":"How to write a decent job ad","text":"Over time I've come to accept that one of the things I'm apparently reasonably competent at is writing and publishing ads for open positions, and I've received questions and requests for advice from other folks who hire people. So I'm going to try and break down what I consider a decent job ad. Not a perfect one mind you, perhaps not even a particularly good one, just a decent one that people will want to read, pass on, and maybe apply to. A few general notes I try to write an ad in such a way that it answers most of the questions an applicant might have about the position. And I then structure it like an imagined conversation between a potential applicant (asking questions) and me, the hiring manager (answering them). That's why practically every subheading in the ad is a question. The structure In my career ads, I give answers to this list of questions: What's this gig about? — The ultra-concise summary of the role to be filled. One sentence. 1 What will I be working on? — Details of the systems/processes/responsibilities associated with the role. What should I know? — Prerequisite skills and knowledge. What can I learn? — Opportunities for acquiring new skills and knowledge. What communities would I engage with? — People and communities outside your organisation the employee would interact with. Who would be my direct manager? — Information about yourself. How's work at company? — Notes on organisational culture. What does my team look like? — Notes on team composition and culture. What does my work week look like? — Information on how the team organises its work on a daily/weekly basis. Where can I work from? — Information about preferences or restrictions regarding the physical location of prospective employees. Can I work from home? 2 — Information about your remote work policy. What timezone would I work in? — Most teams have preferred times-of-day when the majority of team members is awake and working, which tends to be when most work gets done. Or, else, your team may operate 24/7 in shifts, and you're looking to cover a particular shift. Is travel involved? — Possibly a non-issue in the middle of a pandemic, but you might want to establish expectations for when it's over. What employment conditions apply? — Have standard contractual clauses that apply to everyone, like vacation policies or specific packages? Might as well list them. When would I start? — Don't assume that your applicants are available immediately. If people have a notice period in their current job to work with, they'll want to know what's the earliest and latest date you want the role filled. What will I make? — Compensation. How do I apply? — Details and deadlines related to the application process. The details I have a few more details about several of these items, which I'll try to elaborate on as my time permits. So there should be more installments in this series, eventually. Hopefully. 🙂 Can't condense the role description into one sentence? You'll either have to work on your editing skills, or define the role better. ↩ If your answer to this question is \"no\", I bid you good luck! ↩","tags":"blog","url":"blog/2021/08/27/decent-job-ads/","loc":"blog/2021/08/27/decent-job-ads/"},{"title":"Audience feedback on online conference platforms: a speaker's view","text":"We are in year 2 of the Covid-19 pandemic, and open-source conferences are still, for the most part, online-only events. (And I find myself questioning the judgment of those that put on large in-person conferences.) I have spoken at several of such conferences, and I'd like to zero in on one aspect of my personal experience, as a speaker, on some of the conference platforms I worked with. Now, I should explain one thing up front. As a speaker, the thing I care the most about is: Is this talk useful to you? That's it. That's the paramount question. I want you to take something away from the talk that you find useful. Whether that's a technical insight, or a new angle on a problem, or even just entertainment, I want there to be something in the talk for you. And it's incredibly important for me to get a sense of that, as I am delivering the talk. Now in an in-person event, that's easy: all I need is a look across the room, and I permanently look across the room. I can tell if you're making eye contact, or listening intently, or nodding your head, or even putting on a face that makes it clear that you're violently disagreeing with me. Sure, if you raise your hand and ask a question, or heckle me, or laugh at a joke, that drives the point of your engagement home — but I don't need you to become so explicitly engaged, to know that you are engaged. And it is this kind of feedback (that you might not even realize you're giving me!) that makes the difference between delivering a talk, and just speaking into the void. It's also the difference between delivering a conference talk, even if it's a pre-recorded one, and just uploading the video on YouTube. If an conference platform doesn't give me that kind of feedback channel, the work that I put into writing, rehearsing, and recording/streaming the talk is better spent with a view toward upload to a video hosting platform, and engaging with viewers there. So, I as a speaker am foremost interested in a single thing about the conference platform: Is it easy for you to tell me if my talk is useful to you? And I'm not talking about you getting into a chat and saying \"this is useful.\" That's much too high of a threshold. How often do you sit in a talk and then tell the speaker, \"hey, that's useful\"? Quite rarely, and only if you find the content especially actionable or insightful. Because you normally don't have to tell me explicitly: if you're actively listening to me, I can tell. And I know you wouldn't be listening to me if I talked useless nonsense. And this is one of the reasons why I like Venueless so much. Venueless has implemented an extremely low-threshold feature of showing audience engagement. You get a handful of emoji like ❤️👏🤣👍🤔 that you, as a viewer, can click on, and then they appear in the event chat stream like \"emoji rain\" from the top of the screen. Just one click, which also gets anonymized and lost in the crowd — this last bit is important for people who are shy or reserved, and don't like to stick their head out. And this makes it so much easier for you to engage, than having to actually type a sentence (or even a word, or even typing an emoji) into a chat channel. I can not tell you enough how much of a difference this makes to the speaker experience. Add to this that the event chat is not shackled to Slack or any other horribly overgrown not-even-really-chat platform anymore, and you've got a simple, easy-to-use, no-unnecessary-frills experience that puts you directly in touch with your audience. I loved this at PyCon AU last year. For comparison, I also saw LoudSwarm at DjangoCon Europe . The way it was used in that conference it seemed very tightly tied to Slack, although the audience was very nice in using emoji reactions very generously. Still, it wasn't the same quality of feedback that Venueless provided. Voctoconf , which I saw at FrOSCon, allegedly predates Venueless and was excellent in terms of streaming, but in terms of audience interaction it's essentially BigBlueButton on steroids, meaning it's about on the same level as the LoudSwarm/Slack combination I saw at DjangoCon.","tags":"blog","url":"blog/2021/08/24/online-conferences-audience-feedback/","loc":"blog/2021/08/24/online-conferences-audience-feedback/"},{"title":"Add Depth! Stereoscopic imagery for everyone","text":"FrOSCon 2021 was again an online event due to the COVID-19 pandemic ( just like the previous year ), and had a track named Woodwork instead of IT , showcasing people's hobbies and interests they had discovered, or rediscovered, during lockdown. So I submitted a talk that has absolutely nothing to do with what I do for work, and instead covered stereography: making, and viewing, stereoscopic images and videos. The talk recording is available from YouTube . And, as always, you can also review my slides, with all my speaker notes: Rendered slides: GitHub Pages Slide sources (CC-BY-SA): GitHub","tags":"presentations","url":"resources/presentations/add-depth-stereoscopic-imagery-for-everyone/","loc":"resources/presentations/add-depth-stereoscopic-imagery-for-everyone/"},{"title":"Running (Almost) Anything in LXC: Applications Using Your Webcam","text":"One of the non-open-source applications I sometimes have to run for work purposes, and which out of principle I run in LXC containers, is Zoom. Now Zoom is of course an X application, so my previously shared considerations for those apply. It also needs to process input from my microphone, and feed sound into my headphones, so that'll have to work, too . But a thus-configured LXC container is still missing one other bit: it'll have to process the video feed from my webcam. Here's how to do that. LXC Configuration In the article on running X applications in LXC , I give the example of sharing a host directory, (the one that contains the X.org server sockets). For sharing a webcam, I need to do the same for a few files . Now, video capture devices like webcams are represented in Linux by character devices named /dev/video0 , /dev/video1 and so forth. Udev manages these and (on Ubuntu platforms) creates them as owned by the user root and the group video — but it helpfully also creates POSIX ACL entries for the user currently logged in on the X console. All I thus need to do is mount these files into the container (yes, LXC lets you \"mount\" individual files), like so: lxc.mount.entry = /dev/video0 dev/video0 none bind,optional,create=file lxc.mount.entry = /dev/video1 dev/video0 none bind,optional,create=file lxc.mount.entry = /dev/video2 dev/video2 none bind,optional,create=file lxc.mount.entry = /dev/video3 dev/video2 none bind,optional,create=file Here, the optional bit of course means that the container will start even in case a particular file does not exist in the host at the time the container receives its lxc-start command. That, in principle, is all there is to it. Things to consider Be aware that since early 2018 (in other words, in kernel 4.16 and later) the Linux kernel's uvcvideo subsystem will create two /dev/video devices for your webcam. One of them is the actual video capture device; the second one is a metadata device node. You can easily tell which is which, with v4l2-ctl : only a video capture device will have a non-empty list of supported formats. This is a video capture device: $ v4l2-ctl --list-formats --device /dev/video0 ioctl: VIDIOC_ENUM_FMT Type: Video Capture [ 0 ] : 'MJPG' ( Motion-JPEG, compressed ) [ 1 ] : 'YUYV' ( YUYV 4 :2:2 ) [ 2 ] : 'H264' ( H.264, compressed ) And this is the metadata device; note that it lists no video codecs: $ v4l2-ctl --list-formats --device /dev/video1 ioctl: VIDIOC_ENUM_FMT Type: Video Capture Normally, device nodes /dev/video0 and /dev/video1 will be occupied by a built-in webcam, your USB webcam will use /dev/video2 and /dev/video3 , and if you have another video capture device then that will be /dev/video4 and /dev/video5 . Thus, perhaps you want your container to see only your USB webcam, and you don't care about the metadata device. In that case, instead of the four lxc.mount.entry lines I gave above, you might use just one: lxc.mount.entry = /dev/video2 dev/video2 none bind,optional,create=file Also, the bind mounts occur at the time you start the container. Thus, if you plug in a USB webcam while the container is already running, it won't magically become available to the container. There are two ways to address this: You start (or restart) your container whenever you need to use a web cam (or other video device) that you have just plugged in, or you remove the optional keyword from your lxc.mount.entry line(s), so that the container will refuse to start unless the correct webcam is plugged in. Note further that for the same reason, if you disconnect your USB webcam while your container is running, you can't just plug it back in and expect it to work. In that case, udev in the host will have deleted the device node, so the bind mount in your container is now stale, and your containerized applications won't be able to use your capture device anymore. Under those circumstances, you'll simply have to restart your container.","tags":"hints-and-kinks","url":"resources/hints-and-kinks/lxc-webcam/","loc":"resources/hints-and-kinks/lxc-webcam/"},{"title":"Fixing powerline flicker on your webcam feed with a udev rule","text":"If you are spending a non-trivial amount of time in video calls every week (something that, at the time of writing, is true for a lot of people due to the COVID-19 pandemic), and also having to use mains-powered artificial lighting in your office (true at the time of writing for significant portions of the Northern Hemisphere, as it's winter there), then you may be dealing with an unpleasant effect where your web cam feed produces a permanent horizontal flicker that is due to the electronic rolling shutter interacting with the (otherwise imperceptible) 50 or 60Hz AC powerline frequency. The good news is that most webcams come with a facility to eliminate that effect, and on a Linux desktop it's not difficult to permanently do so. v4l2-ctl The utility you want to use for this purpose is v4l2-ctl , which on Ubuntu ships with the v4l-utils package . v4l2-ctl allows you to read and set a bunch of parameters for your webcam. Here's the set of parameters available for my Razer Kiyo , a piece of kit that I highly recommend: $ v4l2-ctl --list-ctrls --device = /dev/video0 brightness 0x00980900 ( int ) : min = 0 max = 255 step = 1 default = 128 value = 128 contrast 0x00980901 ( int ) : min = 0 max = 255 step = 1 default = 128 value = 128 saturation 0x00980902 ( int ) : min = 0 max = 255 step = 1 default = 128 value = 128 white_balance_temperature_auto 0x0098090c ( bool ) : default = 1 value = 1 gain 0x00980913 ( int ) : min = 0 max = 255 step = 1 default = 0 value = 0 power_line_frequency 0x00980918 ( menu ) : min = 0 max = 2 default = 2 value = 2 white_balance_temperature 0x0098091a ( int ) : min = 2000 max = 7500 step = 10 default = 4000 value = 4000 flags = inactive sharpness 0x0098091b ( int ) : min = 0 max = 255 step = 1 default = 128 value = 128 backlight_compensation 0x0098091c ( int ) : min = 0 max = 1 step = 1 default = 0 value = 0 exposure_auto 0x009a0901 ( menu ) : min = 0 max = 3 default = 3 value = 1 exposure_absolute 0x009a0902 ( int ) : min = 3 max = 2047 step = 1 default = 127 value = 127 exposure_auto_priority 0x009a0903 ( bool ) : default = 0 value = 1 pan_absolute 0x009a0908 ( int ) : min = -36000 max = 36000 step = 3600 default = 0 value = 0 tilt_absolute 0x009a0909 ( int ) : min = -36000 max = 36000 step = 3600 default = 0 value = 0 focus_absolute 0x009a090a ( int ) : min = 0 max = 255 step = 1 default = 0 value = 0 flags = inactive focus_auto 0x009a090c ( bool ) : default = 1 value = 1 zoom_absolute 0x009a090d ( int ) : min = 100 max = 140 step = 10 default = 100 value = 100 The value you're looking for is power_line_frequency . Its default is 2 (compensating for a 60Hz powerline frequency), which means the camera should work out of the box and without any powerline flicker in the Americas and parts of Asia. I am in Europe though, where the mains frequency is 50Hz, so I need to set this to 1 : v4l2-ctl --device /dev/video0 --set-ctrl = power_line_frequency = 1 However, it's rather tedious to run that command every time I want to use the webcam. udev Thankfully, this process can be automated with a simple udev rule: ACTION == \"add\" , SUBSYSTEM == \"video4linux\" , DRIVERS == \"uvcvideo\" , RUN += \"/usr/bin/v4l2-ctl --set-ctrl=power_line_frequency=1\" This way, any camera handled by the uvcvideo driver (meaning, practically any contemporary webcam) will have its power line frequency setting configured to the 50Hz value, eliminating the banding effect from the rolling shutter. Chuck that line into a file in /etc/udev/rules.d , run sudo udevadm trigger , and you should be good to go. Acknowledgments and further reading I got the udev rule suggestion from user telcoM 's answer on this StackExchange post . The discussion thread on that post has a few additional suggestions, including some not using udev.","tags":"hints-and-kinks","url":"resources/hints-and-kinks/webcam-rolling-shutter-udev/","loc":"resources/hints-and-kinks/webcam-rolling-shutter-udev/"},{"title":"Running (Almost) Anything in LXC: Sound","text":"Some of the X applications I run in LXC make sounds. Now, I find alert sounds horribly distracting so I turn them off, but for some containerized applications I want to actually play sound. Examples include the Spotify Linux client (which I run in its own LXC container because it's not open source), and occasionally things like the latest available Shotcut version for video editing. You'll notice that, on face value, that's a pretty similar problem compared to getting containerized applications to talk to my X server. It's just that rather than applications only being clients to my X server, I also want them to be clients to my PulseAudio daemon. LXC (Non-)Configuration In the article on running X applications in LXC , I give the example of sharing a host directory, which contains X.org server sockets. In principle, I could do the same thing with the Unix socket that PulseAudio runs. However, there's a small problem with that: the directory I would have to bind-mount into my container is /run/1000/pulse , and you see the difference to bind-mounting /tmp/.X11-unix : /tmp already exists in my container on system startup — but while /run also does, /run/1000 does not. I have experimented with making this work, and I'll spare you the details but it's not as simple as it initially looks. I eventually gave up on that approach, because there is a much simpler way to do this — and it doesn't even require any specific LXC container configuration. The trick is to use the PulseAudio native-protocol-tcp module. When I load it into my running PulseAudio configuration, like so: pactl load-module module-native-protocol-tcp … then a PulseAudio sound server starts listening on a TCP socket on port 4713. I can of course also add this line (minus its pactl prefix) to my PulseAudio configuration file, ~/config/pulse/default.pa . And then, all I need to do is attach to my container, export the PULSE_SERVER environment variable set to 10.0.3.1 (my IPv4 address of the host on the lxcbr0 bridge), and launch an application. I can do this all in one go, like so (using the Spotify client as an example): pactl load-module module-native-protocol-tcp && \\ lxc-start -n focal-spotify && \\ sleep 1 && \\ lxc-attach -n focal-spotify -- \\ sudo -Hu florian env PULSE_SERVER = \"10.0.3.1\" spotify && \\ lxc-stop -n focal-spotify … and as long as the application links to any PulseAudio client libraries, it will correctly parse the set PULSE_SERVER environment variable as an instruction to connect to the given IP address on its default port, and send its audio stream there. I am then still able to control my volume, control my mix, and mute the output from my host. Of course, you probably want to chuck that long command into a .desktop file, or wrap it in a script or function. By the way, no I don't really know why I need that 1-second sleep between starting the container and attaching to it, but it works for me and breaks without it. I presume there is some initialization going on in the container that needs just a few tenths of a second to complete. And I can deal with waiting for my music for one more second. Things to consider Your Ubuntu desktop will most likely run with ufw enabled. If your containerized applications are unable to connect to the PulseAudio server because your firewall blocks them, you won't get sound. Here's what I do: First, I create /etc/ufw/applications.d/pulseaudio , with this content: [pulseaudio] title = PulseAudio Native Protocol TCP description = PulseAudio Sound Server ports = 4713/tcp Then, I allow traffic incoming via the LXC bridge to connect to that server: sudo ufw allow in on lxcbr0 to any app pulseaudio Also do consider, of course, that once your system is set up in this way, not only will your LXC applications be able to play sound through your speakers, but they will also be able to pick up input from your microphone. So use this wisely, particularly if the application you are running does record and process sound. Sometimes you totally want your application to record sound, though, and indeed see the video stream from your webcam, too. Zoom calls come to mind as one such example. More on this in the next installment of this series, where I'll talk about letting your containerized app use host video input.","tags":"hints-and-kinks","url":"resources/hints-and-kinks/lxc-sound/","loc":"resources/hints-and-kinks/lxc-sound/"},{"title":"Running (Almost) Anything in LXC: X applications","text":"I occasionally want to run X applications in an LXC container. Sometimes that's because they're not open source and I need to run them for work, like Zoom. Sometimes it's an open source X application that doesn't work splendidly well on the Ubuntu release that I am using. It turns out that this isn't particularly hard to do — if you are running X.org. To the best of my knowledge, what I am describing here cannot be expected to work, reliably, on Wayland. To me that's no big loss, because there are several other things that I like to use (like Autokey and Plover ) that won't work on Wayland, either. So I run GNOME on X by default, anyway. LXC Configuration Compared to the basic LXC configuration that I have described before , there's only one line that you'll need to add: lxc.mount.entry = /tmp/.X11-unix tmp/.X11-unix none bind,optional,create=dir,ro Now let me explain what this does. /tmp/.X11-unix is where your X display sockets will live, and I map it to the same path in the container. If I look into this directory while I'm in an X session myself, I see one single socket file in there, named X0 , which is owned by my user account that owns the session. And since my standard configuration maps my personal user account (and only my personal user account) from the host to the container, that means that processes running as florian in the container will be able to use this socket just like processes owned by florian in the host can. Now, what's with the create=dir and ro options? create=dir tells LXC to create the mount point in the container if it does not exist. ro bars processes in the container from creating or deleting any files in the directory. You see, my X server always runs in my host OS, I only want applications running in the container to connect to it, as clients. So there's no need for applications in the container to ever modify this directory. However, you'll almost certainly be running something on your system that will sweep /tmp on system startup ( systemd-tmpfiles will, for example), and if that happened, you'd lose the socket. With all that set up, any application that runs in the container with a default $DISPLAY variable ( :0 ) in its environment, will connect to the socket in /tmp/.X11-unix/X0 which is a direct pass-through of the X server socket in the host. Things to consider Since my default configuration maps /home in the host to /home in the container, any application running in the container will happily apply the same configuration as in the host. So for example, if I start Firefox in the container, my Firefox profiles and configuration are all there. However, so are any application locks that my application creates. Sticking with the Firefox example, I won't be able to open a specific profile in the container that is simultaneously open in the host. I can, however, totally use two different profiles side-by-side, or the same profile sequentially in first the host, then the container or the other way round. On a highly customized desktop your application may look different in the container than it does in the host. For example, my desktop is configured to use Cantarell as its sans-serif and Hack as its monospace font. If I neglect to install the fonts-cantarell and fonts-hack Ubuntu packages in my container, containerized X applications will instead fall back to the system default fonts. The same consideration applies for GTK themes. I have yet to tell you about pushing sound from the container to the host, and about sharing the host's webcam and microphone with the container. More on that in future installments in this series.","tags":"hints-and-kinks","url":"resources/hints-and-kinks/lxc-x11/","loc":"resources/hints-and-kinks/lxc-x11/"},{"title":"Running (Almost) Anything in LXC: The Basics","text":"LXC is part of my standard Linux desktop toolbox, and I use it daily. I have done tutorials about this before, one of which you can find on YouTube (courtesy of linux.conf.au ) and GitHub , but it's about time I included this in a series of articles. My motivations for running LXC containers are manifold, but here are some of the most important ones: I want to keep my main system clean of anything that's not free and open source software. There is, however, the odd bit of non-free software that I do need to or want to use — Zoom for work, for example, or the excellent Spotify Linux client for pleasure. Even if a piece of Software is open source, it sometimes does not play nicely with the version of my main system that I currently use. A recent example is the somewhat premature inclusion of pre-release versions of Calibre in Debian and Ubuntu, which means that Calibre is currently not playing too nicely on Ubuntu Focal (the current LTS at time of writing), but runs just dandy on Bionic, which I can handily run in an LXC container. Sometimes the opposite is true as well, that is, some application comes in a version that I want to use, except it's only bundled with a future Ubuntu (or Debian) release that I am not yet prepared to use. Or else, it's available only on Fedora or openSUSE, which are perfectly fine desktop distributions but just not my preferred ones to use on a daily basis. In that case, LXC containers are exceedingly useful as well, and are much less hassle than building the application in question from source. Here are my general rules for running LXC containers: I run my containers as non-root, under my own user account. (If you are unfamiliar with this, and would like to learn more about how it works and how you need to tweak your system to enable it, please see the excellent LXC Getting Started guide.) I use UID and GID mapping rules so that all of the container's user accounts, including the container's root, are mapped to subgids and subuids of my account — all except my own user account and group, with uid and gid 1000. I bind-mount the /home directory into the container. Combined with the uid and gid passthrough of my own account, this means that florian in the container can access /home/florian in any container, just like in the host. I run all my containers in btrfs subvolumes. I maintain a basic container configuration for each Ubuntu release I run, and then I duplicate that configuration for a bunch of containers using snapshot cloning ( lxc-copy -s ), which in combination with btrfs makes the clones quite space-efficient. For example, this is the \"container specific configuration\" section in ~/.share/lxc/focal/config , the configuration for my current base container running Ubuntu Focal: # Container specific configuration lxc.include = /etc/lxc/default.conf lxc.idmap = u 0 100000 1000 lxc.idmap = g 0 100000 1000 lxc.idmap = u 1000 1000 1 lxc.idmap = g 1000 1000 1 lxc.idmap = u 1001 101001 64535 lxc.idmap = g 1001 101001 64535 lxc.mount.auto = proc sys cgroup lxc.rootfs.path = btrfs:/home/florian/.local/share/lxc/focal/rootfs lxc.uts.name = focal lxc.mount.entry = /home home none bind,optional 0 0 Of this, perhaps the lxc.idmap settings merit a bit of extra explanation: lxc.idmap = u 0 100000 1000 means \"map the uid 0 ( root ) in the container to uid 100000 in the host, and continue up until you've hit 1,000 mappings\". In other words, map uids 0 to 999 including, to 100000 to 100999. lxc.idmap = u 1000 1000 1 means \"map the uid 1000 in the container to uid 1000 in the host,\" (in my case, my user account named florian ) \"and follow this pattern for just one mapping\". In other words, make uid 1000 a pass-through. Finally, lxc.idmap = u 1001 101001 64535 means \"starting with uid 1001 in the container, map it to uid 101001 in the host and proceed until you've hit 64,535 mappings\". So in total, that's LXC-ese for \"map all possible uids from 0 to 65535 in the container to host subuids shifted by 100,000 except 1000, which you shouldn't map to any subuid. And the same is true for gids, for the g idmaps. It's a rather roundabout way of specifying this, but it works. Now by itself, this already gives me plenty of options for command-line applications. But since it's my main workstation that I run this on, I usually want my applications to be wired up to my desktop GUI. More on that in the next installment of the series.","tags":"hints-and-kinks","url":"resources/hints-and-kinks/lxc-basics/","loc":"resources/hints-and-kinks/lxc-basics/"},{"title":"Add depth! Stereoscopic imagery for everyone","text":"This is a talk I submitted 1 to linux.conf.au 2021. It was, unfortunately, rejected. If you run a conference or meetup (on-person or online) where you think this talk would be a good fit, please let me know! I'd still love to present it when the opportunity arises. Title Add depth! Stereoscopic imagery for everyone Target Audience User Abstract This will appear in the conference programme. Up to about 500 words. This field is rendered with the monospace font Hack with whitespace preserved Stereoscopic imagery (photography and videography) is a fascinating way to create 3-dimensional images of landscapes, unmoving and moving objects, and of course, people. In this talk, we'll cover the basics of stereoscopic imagery and projection, discover how stereoscopic vision works, and how we can trick our brains into perceiving depth from two flat images. We start with the principles of three-dimensional vision in humans: how our eyes use the combination of focus and vergence to signal two slightly different images of our surroundings to our brain, and how our brain then processes these images to give us the perception of depth. Then, we discuss the techniques available to play tricks on our brains in which two slightly (but cleverly) distinct two -dimensional images are presented to our eyes in such a way that our mind conjures up depth where there objectively is none. These techniques come in various forms, from very high tech (such as virtual reality goggles) to very low tech (like mechanical stereoscopic viewers), but some can deal without any projection technology at all: this is called freeviewing, and for most people it is a remarkably simple and low-cost way to enjoy stunning three-dimensional imagery. We'll cover the parallel-view and crossview freeviewing techniques. We'll then dive into the simple but highly effective steps of making stereoscopic images, using run-of-the-mill cameras (even cell phones), and some straightforward image processing in the GIMP. Finally, we talk about some neat little tricks to make stereoscopic videos, with minimal cost and investment. We'll look at how we can make 3D video with just a GoPro, or a simple drone camera — again using a free software tool, namely the OpenShot video editor, for processing. Private Abstract This will only be shown to organisers and reviewers. You should provide any details about your proposal that you don't want to be public here. This field is rendered with the monospace font Hack with whitespace preserved This talk does not cover a specific software project; the \"Project URL\" below is simply a Flickr album containing a set of stereoscopic images created with the technique I am describing. The fact that LCA is an online event this year would suit this talk particularly well: when I get to the point of explaining freeviewing to attendees, I would expect novices to have some difficulty with one of the freeviewing techniques, and some, with both. The latter would have the option of simply backing up the stream and re-watching the instructions and the test images provided, which is an option that would not exist in a live talk. Accessibility note: Regretfully, this talk will have limited accessibility for people with vision deficiencies. Specifically, the 3D effects presented will be inaccessible to people with complete loss of vision in one eye (or both), nystagmus, or strabismus. People with these conditions will still be able to learn from the techniques presented in the talk, but will likely be unable to perceive the demonstrated 3D effects themselves. People with intraocular lens (IOL) implants might also have difficulty following some of the examples in the talk. Project URL https://www.flickr.com/gp/77872933@N02/SSzK0w If you're curious why this is here, please read this . ↩","tags":"talk-submissions","url":"talk-submissions/lca-2021-stereoscopy/","loc":"talk-submissions/lca-2021-stereoscopy/"},{"title":"I Don't Think This Means What You Think It Means: Red Herrings in OpenStack","text":"This is a talk of which I had done a previous version at Open Infra Days Nordic in 2019, but where unfortunately the audio came out really messed up in the recording. This version is much better. Talk video: YouTube You can review my slides, including all speaker notes: Rendered slides: GitHub Pages Slide sources (CC-BY-SA): GitHub","tags":"presentations","url":"resources/presentations/i-dont-think-this-means-what-you-think-it-means-red-herrings-in-openstack/","loc":"resources/presentations/i-dont-think-this-means-what-you-think-it-means-red-herrings-in-openstack/"},{"title":"What I now know about HAproxied Django database connections, and wish I'd known sooner","text":"I've been wanting to speak at PyCon.au for a long time now, and finally did for the 2020 PyConline AU edition. This was a truly wonderful conference, in which the organisers and attendees collectively bent over backwards and put in a massive effort to run an event that was just as wonderful as an in-person event would have been. This is a purely technical talk, and covers some very interesting aspects of dealing with Galera high availability clusters for MySQL and MariaDB database servers, connecting though them from Django via an HAProxy load balancer, and then dealing with interesting side effects of that combination. Talk video: YouTube And, as always, you can also review my slides, with all my speaker notes: Rendered slides: GitHub Pages Slide sources (CC-BY-SA): GitHub","tags":"presentations","url":"resources/presentations/what-i-now-know-about-haproxied-django-database-connections-and-wish-id-known-sooner/","loc":"resources/presentations/what-i-now-know-about-haproxied-django-database-connections-and-wish-id-known-sooner/"},{"title":"No, We Won't Have a Video Call for That!","text":"FrOSCon 2020 was an online event due to the COVID-19 pandemic, and gave me the opportunity to present an extended and heavily updated version of my DevOpsDays 2019 talk. I normally make my talks available as a video, and a slide deck with full speaker notes. In this case though, I consider it fitting to write the whole thing out, so that you don't need to watch a full length video in 45 minutes, but can read the whole thing in 15. You'll still find links to the recording and deck downpage, as usual. No, we won't have a video call for that! Communications for distributed teams FrOSCon 2020 This presentation is a talk presented at FrOSCon 2020 Cloud Edition . It is CC-BY-SA 4.0 licensed, see the license for details. Hello and welcome, dear FrOScon people — this is my talk on communications in distributed teams. My name is Florian, this is the second time I‘m speaking at FrOScon, and you probably want to know what the hell qualifies me to talk about this specific issue. So: Why am I talking here? So, why am I talking about that ? Or rather more precisely, why am I talking about that? I turned 40 last year, have been in IT for about 20 years now (19 full-time), and out of that I have worked in 4 successive companies, all of which worked out of offices, for 11 years, in a completely distributed company, that I founded, for 6 years, and now, for about three years, I have been running a distributed team that is a business unit of a company that has existed for 15 years and throughout that time, has only ever worked from a single office. So I think I might have seen and become aware of some of the rather interesting challenges that come with this. What changed since last time? I originally wrote and presented this talk for the first time in December 2019. At the time, you probably had forgotten about SARS, had no idea what SARS-CoV2 or COVID-19 were, and many of you were probably working from offices. And then something like three months later, everything changed and suddenly, this talk became much more relevant to a much greater audience. And something else happened: a lot of people suddenly started talking about working from home and distributed teams, and a lot of those people who were talking very loudly, had themselves only been working with or managing distributed teams since March. And a fair amount of what you could about the subject then, and can still read now, is complete and utter bullshit. So there's one point I actually didn't make in the initial version of this talk, because I thought it was self-evident. But I have come to the conclusion that to a lot of people it is not, so to rectify this omission from last December — and with apologies for that omission to the wonderful DevOpsDays Tel Aviv crowd, who were my first audience for this talk, let me make this one thing very clear from the outset: Effective distributed collaboration is not pretending to be in an office while staring into a webcam all day. You will never be able to capitalize on work as a distributed team unless you kick some office habits. The key to distributed teams being effective is not that they happen to not be in the same place, as you'll see from the remainder of this talk. So to expect success from the approach that you take the habits of an office, simply remove the element of locality, replace every face to face meeting with a video call and carry on, is ludicrous. The good news is that if you do it right, you'll end up with a far better team than a local one would ever be, and everyone has a chance at far better work-life balance, and you don't waste awful amounts of time and energy and fossil fuels on your commute. What's in this talk? So you'll find a few general themes throughout this talk: What modes we have available for communications in teams; Why distributed teams always collaborate asynchronously, and what communication modes lend themselves to that particularly well; Why written communication is so important in distributed teams; And why meetings (like video calls) are a mode of communication that effective distributed teams hardly ever need to use — except for very specific reasons. But I do want to state one thing upfront: This is not science. Nothing of what I am talking about is steeped in any scientific rigour. I present anecdotes, not evidence. I might be mistaking correlation for causation, or the other way round. It's solely based on my personal experience, and the experience of others I have talked to, watched, or read. Everything I say here is subject to debate and rebuttal, or you can simply have a different opinion. But it's definitely not science. Now with all of that said, let me attempt to give a definition of a distributed team, according to my understanding: A distributed team is a professional group whose members do not rely on proximity in order to routinely collaborate productively. Now this is clearly not an ideal definition, not least because it defines something by a negative, and an outside factor to boot: it defines a distributed team by what it does not need to exist to function. But it's the best definition I've been able to come up with. Now there's a couple of key words in here: Professional. I'm talking about teams that work towards a professional goal. This doesn't necessarily mean that they all work in the same company. They could, for example, all work in different companies collaborating on a joint project, which is what frequently happens in open source software projects. But they're not pursuing their hobby, they're doing their jobs. Routinely. I'm talking about teams that habitually work in a distributed fashion, not the work that goes on in an office-based team when one person is having a work-from-home day. It is important to understand that that lack of proximity is not only spatial, it is temporal as well, because: Working in a distributed team means working asynchronously. If your team is distributed, this is equivalent to saying that it works in an asynchronous fashion, that is to say, that people will work on things in parallel, and a capable distributed team will have just as few synchronization points as absolutely necessary. The reason for this is not just working in different timezones, but also the fact that everyone will have their own daily routine, and/or have their individual times when they are most productive. Which you will not attempt to synchronize. (Doing so would mean setting the entire team up for failure.) Now, this doesn't come for free, nor does it fall in our lap: Being productive in a distributed team is a skill that most people must learn; it is not innate to us. People are not born with the ability to work in a distributed team. Humans function best in groups that collaborate in close proximity to one another; it is only very recently that technology has started to enable us to override that to an extent — giving us other benefits like the ability to work from home, or the ability to hire people residing anywhere, provided they have internet connectivity. So we now can work in teams despite being continental distances away from each other but we do have to acquire the skills to do that. And if we fail to do so, that has a rather grave disadvantage, which is that… Nothing has as dire an impact on productivity as poor communications. This is a truism that applies to both distributed and non-distributed teams. Having bad communications will wreck any project, blow any budget, fail any objective. Now note that the reverse is not true: having good communications does not guarantee success. But having bad communications does guarantee failure. And here is one thing to start with: A capable distributed team habitually externalises information. Information is generally far less useful when it is only stored in one person's head, as opposed to being accessible in a shared system that everyone trusts and can use. If you take important information out of your own head and store it in a medium that allows others to easily find and contextualise it, that's a win for everyone. And since we're all technology people, we typically have multiple facilities to externalise, share, and then access information at our disposal. So let's see how those compare. Modes of communication in distributed teams A distributed team will habitually use multiple modes of communication, relying mostly on those that make sharing, finding, and contextualising information easy, and avoiding those that make it difficult. In many teams, distributed or not, using chat as a default mode of communication is becoming the norm. Now with an important exception, which I'll get to near the end of the talk, this is not a symptom of having a particularly dynamic or efficient team; it's the opposite. Excessively using chat isn't being efficient. It's being lazy. It's a symptom of the worst kind of laziness (not malice!) : in an attempt to communicate quickly and easily, for yourself, you are really making things harder for everyone, including yourself. Share Find Contextualise Chat 🙂 😐 🙁 This is because, while sharing information in a chat is extremely easy, it is also a \"fire and forget\" mode of communications. Chat makes it difficult to find information after the fact. If you've ever attempted to scour a busy Slack or IRC archive for a discussion on a specific topic that you only remember to have happened a \"few months ago\", you'll agree with me here. It's even more difficult to read a Slack discussion in context, that is to say in relation to other discussions on the same topic, days or weeks earlier or later. Let's compare that to other communication modes: Share Find Contextualise Chat 🙂 😐 🙁 Email 😐 😐 😐 Email makes it easy to share information with a person or a group from the get-go, but quite difficult to loop people into an ongoing discussion after the fact. Finding information later is just as hard as with chat, and it's marginally better at contextualizing information than chat (because you get proper threading). Share Find Contextualise Chat 🙂 😐 🙁 Email 😐 😐 😐 Wiki 🙂 🙂 🙂 Issue tracker 🙂 🙂 🙂 A wiki and an issue tracker (provided you don't lock them down with silly view permissions), in contrast, both make it very easy to share, find, and contextualise information. Note that \"wiki\", in this context, is shorthand for any facility that allows you to collaboratively edit long-form documents online. That can be an actual wiki like a MediaWiki, but also something like Confluence, or even shared Google Docs. Likewise, \"issue tracker\" can mean RT, OTRS, Jira, Taiga, Bugzilla, whatever works for you. Share Find Contextualise Chat 🙂 😐 🙁 Email 😐 😐 😐 Wiki 🙂 🙂 🙂 Issue tracker 🙂 🙂 🙂 Video call 😐 🙁 🙁 Video calls are even worse than chat or email, because sharing information works but doesn't scale — you can't reasonably have more than 5-or-so people in a video call, and sharing the recording of a full video call is just pointless. So really, make your wiki and your issue tracker your default mode of communications, and use the others sparingly. (This isn't meant to be a euphemism for \"don't use them\", as we'll get to in a moment.) Text chat So. Let's talk about text chat. These days, that frequently means Slack , but what I am talking about also and equally applies to IRC , Mattermost , Riot , and anything similar. Is text chat universally useful? No. Is it universally bad? Not that, either. There is a very specific type of situation in which text chat is a good thing: Use chat for collaboration that requires immediate, interactive mutual feedback. Using interactive chat is a good idea for the kind of communication that requires immediate, interactive mutual feedback from two or more participants. If that is not the case, chat is not a good idea. This means that the only thing that chat is good for is communication that is required to be synchronous, and remember, in a distributed team asychronicity is the norm. So using interactive chat for communications needs to be an exceptional event for a distributed team; if it is instead a regular occurrence you'll make everyone on the team miserable. For any interaction that does not require feedback that is both immediate and interactive, email, a wiki, or an issue tracker are far superior modes of communication. The only reason to use DMs for collaboration is a need for immediate, interactive mutual feedback and confidentiality. Using chat direct messages (DMs) as the default means of communication is utterly braindead. In order for a chat DM to be useful, there is precisely one clearly delineated confluence of events that must occur: You need immediate feedback from the other person, you need mutual back-and-forth with the other person, you don't want others to follow the conversation. I can't emphasize enough that this combination is perfectly valid — but it is exceedingly rare. If you want just a private exchange of ideas with someone, encrypted email will do. If you want to work on something together with one person before you share it with others, restricted view permissions on a wiki page or an issue tracker ticket will work just fine. If you don't need confidentiality but you do need interactive and immediate feedback, chances are that you're working on something urgent, and it is far more likely you'll eventually need to poll other opinions, than that you won't. So just use a shared channel from the get-go, that way it's easier for others to follow the conversation if needed — and they might be able to point out an incorrect assumption that one of you has, before you end up chasing a red herring. A chat ping is a shoulder tap. \"Pinging\" someone in a chat (that is, mentioning their username, which usually triggers a visual or auditory notification), is exactly like walking up to a person, interrupting what they are doing, tapping them on the shoulder, and asking them a question. No matter whether it is your intention or not, they will feel compelled to answer, relatively promptly (the only exception is when you've done this so often that you have conditioned your colleagues to ignore you — congratulations). This means that you've broken their train of thought, yanked them out of a potentially complex task, forced them to redo what they did pre-interruption, or actually have them commit a mistake. So pinging someone in a chat is something you should only do if you are aware of exactly this risk, and you are convinced that whatever you're pinging about is more important. Otherwise, to be very blunt, you'll be seen as the asshole. Want people to hate you? Send naked pings. A \"naked ping\" is the action of sending someone a message consisting only of their username and a marker like \"ping\", \"hi\", \"hey\" or similar. 14 : 00 : 02 Z johndoe : florian : ping [...] 15 : 56 : 17 Z florian : johndoe : I hate you Don't. Just don't. Any person who is versed in the use of chat communications will, when subjected to this behavior, be inclined to flay you alive. Infinitely more so if it's a DM. Do not do this. Instead, always provide context. Always always always. Don't say \"can I ask you a question, instead, ask the question. If something isn't urgent, say something like \"no urgency.\" 14 : 00 : 02 Z johndoe : florian : can I get your eyes on PR # 1422 ? [...] 15 : 56 : 17 Z florian : johndoe : done ! ( was afk for a bit – sick kiddo ) 15 : 56 : 58 Z johndoe : florian : np , ty It should be self-evident why this is better than naked pings, but if to you it is not, then please read Naked Pings , courtesy of Adam Jackson and Mark McLoughlin. Video calls (Zoom, Hangouts, BlueJeans etc.) Next, I'd like to talk about video calls. Doesn't matter what technology you're using. Could be Zoom, Google Hangouts, BlueJeans, Jitsi, whatever. And I'd like to address this specifically, given the fact that in the current pandemic the use of video calls appears to have skyrocketed. There's a very good reason to use video calls: they give you the ability to pick up on nontextual and nonverbal cues from the call participants. But that's really the only good reason to use them. Video calls have a significant drawback: until we get reliable automatic speech recognition and transcription, they are only half-on-the-record. Hardly anyone goes to the trouble of preparing a full transcript of a meeting, and if anything, we get perhaps a summary of points discussed and action items agreed to. So even if we keep recordings of every video call we attend, it's practically impossible to discern, after the fact, what was discussed in a meeting before decisions were made. It is also practically impossible to find a discussion point that you only have a vague recollection of when it was discussed in a video call, whereas doing so has a much greater probability of success if a discussion took place on any archived text-based medium. Every video call needs an agenda. This is, of course, true for any meeting, not just those conducted by video call. A conversation without an agenda is useless. You want people to know what to expect of the call. You also want to give people the option to prepare for the call, such as doing some research or pulling together some documentation. If you fail to circulate those ahead of time, I can guarantee that the call will be ineffective, and will likely result in a repeat performance. Until machines get intelligent enough to automatically transcribe and summarise words spoken in a meeting, write notes and a summary of every meeting you attend, and circulate them. Just as important as an agenda to set the purpose of the meeting, is a set of notes that describes its outcome . Effective distributed teams understand that the record of a call is what counts, not the call itself. It is not the spoken word that matters, but the written one. From that follows this consequence: To be useful, the write-up of a call takes more time and effort than the call itself. If you think that video calls are any less work than chat meetings or a shared document that's being edited together or dicussed in comments, think again. The only way a video call is less work, is when everyone's lazy and the call is, therefore, useless. Every meeting needs notes and a summary, and you need to circulate these notes not only with everyone who attended the meeting, but with everyone who has a need-to-know. Here's the standard outline I use for meeting notes: Meeting title Date, time, attendees Summary Discussion points (tabular) Action items Putting an executive summary at the very top is extraordinarily helpful so people can decide if they should familiarise themselves with what was discussed, immediately, and possibly respond if they have objections, or only want to be aware of what was decided, or just keep in the back of their head that a meeting happened, that notes exist, and where they can find them when they need to refer back to them. Once you do meetings right, you no longer need most of them. The funny thing is that once you adhere to this standard — and I repeat, having a full and detailed record is the only acceptable standard for video meetings – you'll note that you can actually skip the meeting altogether, use just a collaboratively edited document instead of your meeting notes, and remove your unnecessary synchronization point. Video calls for recurring team meetings There is one thing that I do believe video calls are good for, and that is to use them for recurring meetings as as an opportunity to feel the pulse of your team. Obviously, a distributed team has few recurring meetings, because they are synchronization points, and we've already discussed that we strive to minimize those. So the idea of having daily standups, sprint planning meetings, and sprint retrospectives is fundamentally incompatible with distributed teams. Aside: in my humble opinion, this is also why using Scrum is a terrible idea in distributed teams — not to mention that it's a terrible idea, period. However, having perhaps one meeting per week (or maybe even one every two weeks) in a video call is useful precisely for the aforementioned reasons of being able to pick up on nonverbal clues like body language, posture, facial expressions, and tone. If people are stressed out or unhappy, it'll show. If they are relaxed and productive, that will show too. Note that these meetings, which of course do follow the same rules about agenda and notes, are not strictly necessary to get the work done. The team I run has one one-hour meeting a week, but whenever that meeting conflicts with anything we skip it and divide up our work via just the circulated coordination notes, and that works too. The meeting really serves the purpose of syncing emotionally, and picking up on nonverbal communications. Briefing people Whenever you need to thoroughly brief a group of people on an important matter, consider using a 5-paragraph format. Situation Mission Execution Logistics Command and Signal This is a format as it is being used by many armed forces; in NATO parlance it's called the 5-paragraph field order. Now I'm generally not a fan of applying military thinking to civilian life — after all we shouldn't forget that the military is an institution that kills people and breaks things, and I say that as a commissioned officer in my own country's army —, but in this case it's actually something that can very much be applied to professional communications, with some rather minor modifications: Situation Objective Plan Logistics Communications Let's break these down in a little detail: Situation is about what position we're in, and why we set out to do what we want to do. You can break this down into three sub-points, like the customer's situation, the situation of your own company, any extra help that is available, and the current market. Objective is what we want to achieve. Plan is how we want to achieve it. Logistics is about what budget and resources are available, and how they are used. Communications is about how you'll be coordinating among yourselves and with others in order to achieve your goal. Note that people always have questions on what they've just been briefed about. They just might not think of them straight away. Give people time to think through what you've just briefed them on, and they will think of good questions. So always have a follow-up round at a later time (2 hours later, the following day, whatever), for which you encourage your group to come back with questions. Also, use that same follow-up for checking how your briefing came across, by gently quizzing people with questions like \"by what date do we want to implement X?\", or \"Joe, what things will you need to coordinate with Jane on?\" This gives you valuable feedback on the quality of your briefing: if your team can't answer these questions, chances are that you weren't as clear as you should have been. Pinching the firehose Finally, I want to say a few words about what I like to call pinching the figurative firehose you might otherwise be forced to drink from: The amount of incoming information in a distributed team can be daunting. When you work in a distributed team, since everyone is on their own schedule and everything is asynchronous, you may be dealing with a constant incoming stream of information — from your colleagues, your reports, your manager, your customers. There is no way to change this, so what you need to do is apply your own structure to that stream. What follows is not the way to do that, but one way, and you may find another works better for you. But you will need to define and apply some structure, otherwise you'll feel constantly overwhelmed and run the risk of burning out. Consider using the \"4-D\" approach when dealing with incoming information. (Hat tip to David Allen) There's a defined approach for doing this, which I learned about from reading David Allen 's Getting Things Done . I don't know if Allen invented the 4-D approach or whether someone came up with it before him, but that's how I know about it. In his book, David Allen suggests to apply one of the following four actions to any incoming bit of information: Drop means read, understand, and then archive. It's what you use for anything that doesn't require any action on your part. Delegate is for things that do require action, but not from you. Make sure that it gets to the right person and is understood by them , and make a note for follow-up. Defer means it needs doing, and it's you who needs to do it, but it doesn't need doing immediately . Enter it into your task list (to use a very generic term, more on this in a bit), and clear it from your inbox. Do are the (typically very few) things that remain that need to be done by you, and immediately. Following this approach does not mean that you'll never be overwhelmed by the amount of information that you need to process. But it'll greatly reduce that risk. \"Drop\" rules \"Dropping\" things doesn't mean ignoring them. You still have to read and understand what's in them, and be able to find them later. So: Never delete things (except spam). Only archive them in a way that that keeps them retrievable in the future. If there something isn't understandable to you, think it through and look for clarification. \"Delegate\" rules Delegation obviously requires that there is a person you can delegate to. This is not necessarily someone who reports to you; indeed, it might be someone you report to. (You might be asked to deal with something that you have no control over, but your manager does.) So: Find the right person that can get the task done. Preemptively send them all the information that you think they might need (and that you have access to), rather than relying on them to ask. Ask them to acknowledge that they have received what they need. Make a note to follow up to see if they need anything else, and follow through by seeing the task to completion. Within your own team, you only ever delegate tasks, not responsibility. Tasks without follow-up and follow-through are a waste of people's time. Do not delegate, or even define, tasks that you are not prepared to follow through on. If you handwave \"everyone use encrypted email from now on,\" and you're not even prepared to make that work for your own email account, you might as well just leave it. And if you do proclaim an objective or rule and then you find yourself unable to see it through — this happens, and is no sign of ineptitude or failure — then loudly and clearly rescind it. It's far better for you to visibly backtrack, than to be perceived as someone whose pronouncements are safe to ignore. \"Defer\" rules Deferring simply means that because something you need to do doesn't need doing immediately, you can do it at a time that suits your schedule. This means that you'll need to add the task immediately to some sort of queue (for email, this can be a folder named \"Needs Reply\"), make sure to go through that queue at a later time to prioritize (ideally, right after you're done with your \"Do\" tasks, which we'll get to in a second), absolutely ensure that you make time to go back and actually do your prioritized tasks, at a time you consider convenient. \"Do\" rules And finally, there'll be your \"Do\" tasks — stuff that you need to do, and do immediately. Tell people that you're doing them, because you'll want to be uninterrupted. Update your chat status, put some blocked time in your calendar. Make sure you'll be uninterrupted. For email, turn off all your notifications. Plow through all the undropped, undelegated, undeferred items in your inbox until it's empty. But what about the watercooler? The entirety of this talk, up to this point, has focused on professional communications. And among people unfamiliar or unexperienced with work in a distributed team, it is often accepted that teams can communicate well \"professionally.\" However, they frequently ask, \"what about watercooler chats? What about the many informal discussions that happen at work while people are getting some water or coffee, or sit together over lunch? There's always so much communication happening at work that's informal, but is extremely beneficial to everyone.\" Office workers often don't habitually externalise information. A distributed team that tries that won't last a week. Firstly, many companies where information exchange hinges on coffee or cafeteria talk simply don't give a damn about externalising information. Sure, if 90% of your company's knowledge is only in people's heads, you're dead without the lunchroom. But if the same thing happens in a distributed team, it never gets off the ground. So, if you have a team that's functional and productive, because it habitually externalises information, the absence of chit-chat over coffee has zero negative impact on information flow. However, you may also be interested in the completely non-work-related talk that happens over coffee, that simply contributes to people's relaxation and well-being. People working in distributed teams are often introverts. Or they simply choose to have their social relationships outside of work. I know this might shock some people, but there are plenty of people who can make a terrific contribution to your company, but who dislike the \"social\" aspect of work. They might thrive when being left alone, with as little small-talk as possible, and ample opportunity to socialize with their friends and family, away from work. But if you do have people on your team that enjoy having an entirely informal conversation every once in a while, there totally is room for that even in a distributed team. All you need to do is agree on a signal that means \"I'm taking a break and I'd be happy to chat with anyone who's inclined, preferably about non work related things\" (or whatever meaning your group agrees on). This could be a keyword on IRC, a message to a specific channel, or (if you want to get fancy) a bot that updates your group calendar when it receives a message with a particular format. However, as a word of caution, I've actually done this with my team before, and it didn't catch on — for the simple reason that we almost never took breaks that happened to overlap. But that doesn't rule out that it works on your team, and also there's always the remote possibility that two or more people on your team might like to schedule their breaks concurrently. What you can also do, of course, is have a channel in which you can discuss completely random things that are not work related. And if the rule is that confidential or company-proprietary discussion topics are off-limits there, the channel might as well be public. It might even be Twitter. The antithesis: ChatOps I do want to mention one other thing for balance. There is a complete alternative framework for distributed teams working together, and it's what people refer to as ChatOps. To the best of my knowledge, the first company to run ChatOps on a large scale and talk about it publicly was GitHub, back in 2013 in a RubyFuza talk by Jesse Newland . If a distributed team operates on a ChatOps basis, the interactive text chat is where absolutely everything happens. Everyone lives in chat all the time, and all issues, alerts and events are piped into the chat. Everything is discussed in the chat, and everything is also resolved in the chat. Such a system relies on heavy use of chat bots. For example, if an alert lands in the channel, and the discussion then yields that the proper fix to the problem is to run a specific Ansible playbook, you send an in-chat bot command that kicks off that playbook, and then reports its result. And this is of course very laudable, because it resolves a major issue with using chat, which is the classic scenario of something being discussed in a chat, someone else then going away for a bit and then coming back saying \"I fixed it!\", and nobody else actually understanding what the problem was. If you make everything explicit and in-band, in becomes easy, in principle, to go back to a previously-solved problem that reappears, and replay the resolution. When does ChatOps make sense? Here's a hint: It's called Chat Ops . So can this make sense? Yes, absolutely. Under what circumstances though? I maintain that this is best suited for when your work tends to be inherently linear with respect to some dimension. For example, if your primary job is to keep a system operational versus the linear passage of time, ChatOps is an excellent approach. And keeping complex systems operational over time is the definition of, you guessed it, ops. So ChatOps may be a very suitable communication mode for operations, but it's highly unlikely to be efficient as a generic mode of communication across distributed teams. And even then I posit it's difficult to get right, since you'll have to curb channel sprawl and threading and other things, but's that's a whole ‘nother talk and indeed a talk for another speaker, because I don't lead an ops team. To summarize… So to summarize, here are my key points from this talk, in a nutshell — please make these your key takeaways. Distributed teams are better than localized teams — not because they're distributed, but because they're asynchronous. Avoid anything that makes a distributed team run synchronously. Use less chat. Have fewer meetings. Write. Things. Down.","tags":"presentations","url":"resources/presentations/no-we-wont-have-a-video-call-for-that/","loc":"resources/presentations/no-we-wont-have-a-video-call-for-that/"},{"title":"No, We Won't Have a Video Call for That: Slides and Recordings","text":"These are the slides and recordings of the original talk that ultimately became this article . The talk recording is available from two different sources: Video and audio in multiple formats, for viewing and download: CCC Media server Streaming: YouTube And, as always, you can also review my slides, with all my speaker notes: Rendered slides: GitHub Pages Slide sources (CC-BY-SA): GitHub","tags":"presentations","url":"resources/presentations/no-we-wont-have-a-video-call-for-that-slides-and-recordings/","loc":"resources/presentations/no-we-wont-have-a-video-call-for-that-slides-and-recordings/"},{"title":"Celery to Chew On","text":"Asynchronous Celery tasks that manipulate a MySQL/Galera database from a Django application can produce very interesting behavior when HAProxy is involved. Some basics When you're running a Django application, the following things are all pretty commonplace: You use MySQL or MariaDB as your Django database backend . You don't run a single standalone MySQL/MariaDB instance, but a Galera cluster. You run asynchronous tasks in Celery . This way, if you have a complex operation in your application, you don't necessarily have to handle it in your latency-critical request codepath. Instead, you can have something like this: from celery import Task class ComplexOperation ( Task ) \"\"\"Task that does very complex things\"\"\" def run ( self , ** kwargs ): # ... lots of interesting things … and then from your view (or management command, or whatever), you can invoke this like so: from .tasks import ComplexOperation def some_path ( request ): \"\"\"/some_path URL that receives a request for an asynchronous ComplexOperation\"\"\" # ... # Asynchronously process ComplexOperation ComplexOperation . delay ( pk = request . GET [ 'id' ]) # ... What this means is that the code defined in ComplexOperation 's run() method can run asynchronously, while the HTTP request to /some_path can immediately return a response. You can then fetch the asynchronous task's result in a later request, and present it to the user. (Note that there are other ways to invoke Celery tasks ; getting into those in detail is not the point of this article.) MySQL/Galera via HAProxy Now, let's inject another item into the setup. Suppose your application doesn't talk to your Galera cluster directly, but via HAProxy . That's not exactly unheard of; in fact it's an officially documented HA option for Galera. If you run a Django application against an HAProxyfied Galera cluster, and you have rather long-running Celery tasks, you may see occurrences of OperationalError exceptions that map to MySQL error 2013, Lost connection to MySQL server during query . Error 2013 means that the connection between the client and the server dropped in the middle of executing a query. This is different from error 2006, MySQL server has gone away , which means that the server has gracefully torn down the connection. 2013 is really an out-of-nowhere connection drop, which normally only occurs if your network has gone very wonky. With HAProxy however, that service may be your culprit. An HAProxy service sets four different timeout values: timeout connect : the time in which a backend server must accept a TCP connection, default 5s. timeout check : the time in which a backend server must respond to a recurring health check, default 5s. timeout server : how long the server is allowed to take before it answers a request, default 50s. timeout client : how long the client is allowed to take before it sends the next request, default 50s. Distilling the timeout problem If you have access to manage.py shell for your Django application, here's a really easy way for you to trigger an adverse effect of this default configuration. All you have to do is create an object from a model, so that it fetches data from the database, then wait a bit, then try to re-fetch. Like so: ./ manage . py shell [ ... ] ( InteractiveConsole ) >>> from time import sleep >>> from django.contrib.auth import get_user_model >>> User = get_user_model () >>> me = User . objects . get ( username = 'florian' ) >>> sleep ( 40 ) >>> me . refresh_from_db () >>> sleep ( 55 ) >>> me . refresh_from_db () Traceback ( most recent call last ): [ ... ] OperationalError : ( 2013 , 'Lost connection to MySQL server during query' ) So what happens here? I open a session to the database with the User.objects.get() call that populates the me object. Then I wait 40 seconds. That's comfortably short of the 50-second HAproxy timeout. Now when I run me.refresh_from_db() , the session is still alive and the call completes without error. The timeout clock resets at this stage, and I could keep going like this ad infinitum, as long as I sleep() (or keep busy) for less than 50 seconds. However, I next wait 55 seconds, causing HAProxy to terminate the connection. And then, refresh_from_db() breaks immediately with the 2013 error. Note that if I run refresh_from_db() — or any other operation that touches the database – again , I get a different error (2016, expected at this point), but I don't get my database connection back: >>> me.refresh_from_db() Traceback (most recent call last): [...] OperationalError: (2006, 'MySQL server has gone away') What I have to do instead is close my connection first: >>> from django.db import connection >>> connection . close () … and then, when I run anything else that requires a database query, Django will happily reconnect for me. >>> me.refresh_from_db() HAProxy timeouts getting in the way of your Celery tasks Now how does this relate to a real-world application? Suppose you have a long-running Celery task with database updates or queries at the beginning and end of something complicated, like so: from celery import Task from model import Thing class ComplexOperation ( Task ) \"\"\"Task that does very complex things\"\"\" def run ( self , ** kwargs ): thing = Thing . objects . get ( pk = kwargs [ 'pk' ]) do_something_really_long_and_complicated () thing . save () In this case, we retrieve data from the database into memory, populating our thing object, then we do something very complex with it — suppose this can take on the order of minutes, in the extreme, and finally, we take the modified data for our in-memory object, and persist it back to the database. So far, so simple. However, now assume that while you're executing the do_something_really_long_and_complicated() method, something bad happens to your database. Say you restarted one of your MySQL or MariaDB processes, or one of your nodes died altogether. Your database cluster is still alive, but your session , which was very much alive during the call that populated thing , is dead by the time you want to make the thing.save() call. Depending on what actually happened, you'd see one of the following two OperationalError instances: Either an immediate 2006, MySQL server has gone away — this is is what you'd see if the MySQL server was shut down or restarted. That's a graceful session teardown, and it's not what I want to focus on in this article. Or, and this is what I want to discuss further here, 2013, Lost connection to MySQL server during query . You normally don't get this as a result of something breaking at the other end of the connection, but rather in between. In our case, that would be HAProxy. Let's look at our code snippet with a few extra comments: from celery import Task from model import Thing class ComplexOperation ( Task ) \"\"\"Task that does very complex things\"\"\" def run ( self , ** kwargs ): thing = Thing . objects . get ( pk = kwargs [ 'pk' ]) # Right here (after the query is complete) is where HAproxy starts its # timeout clock # Suppose this takes 60 seconds (10 seconds longer than the default # HAProxy timeout) do_something_really_long_and_complicated () # Then by the time we get here, HAProxy has torn down the connection, # and we get a 2013 error. thing . save () So now that we've identified the problem, how do we solve it? Well that depends greatly on the following questions: Are you the developer, meaning you can fix this in code, but you can't change much in the infrastructure? Or are you a systems person, who can control all aspects of the infrastructure, but you don't have leverage over the code? If you have control over neither code nor infrastructure, you're out of luck. If you call all the shots about both, you get to pick and choose. But here are your options. Fixing this in code If it's your codebase, and you want to make it robust so it runs in any MySQL/Galera environment behind HAProxy, no matter its configuration, you have a couple of ways to do it. Keep connections shorter One way to do it is do keep your database connections alive for such a short time that you practically never hit the HAProxy timeouts. Thankfully, Django auto-reconnects to your database any time it needs to do something, so the only thing you need to worry about here is closing connections — reopening them is automatic. For example: from django.db import connection from model import Thing class ComplexOperation ( Task ) \"\"\"Task that does very complex things\"\"\" def run ( self , ** kwargs ): thing = Thing . objects . get ( pk = kwargs [ 'pk' ]) # Close connection immediately connection . close () # Suppose this takes 60 seconds. do_something_really_long_and_complicated () # Here, we just get a new connection. thing . save () Catch OperationalErrors The other option is to just wing it, and catch the errors. Here's a deliberately overtrivialized example: from django.db import connection from django.db.utils import OperationalError from model import Thing class ComplexOperation ( Task ) \"\"\"Task that does very complex things\"\"\" def run ( self , ** kwargs ): thing = Thing . objects . get ( pk = kwargs [ 'pk' ]) # Right here (after the query is complete) is where HAproxy starts its # timeout clock # Suppose this takes 60 seconds. do_something_really_long_and_complicated () # Then by the time we get here, HAProxy has torn down the connection, # and we get a 2013 error, which we'll want to catch. try : thing . save () except OperationalError : # It's now necessary to disconnect (and reconnect automatically), # because if we don't then all we do is turn a 2013 into a 2006. connection . close () thing . save () Now of course, you'd never actually implement it this way, because the one-time retry is far too trivial, so you probably want to retry up to n times, but with exponential backoff or some such — in detail, this becomes complicated really quickly. You probably also want some logging to catch this. In short, you probably don't want to hand-craft this, but instead rely on something like the retry() decorator from tenacity , which can conveniently provide all those things, plus the reconnect, without cluttering your code too much. Fixing this in infrastructure You may be unable to control this sort of thing in your code — because, for example, it's a codebase you're not allowed to touch, or you're less than comfortable with the idea of scouring or profiling your code for long-running codepaths between database queries, and sprinkling connection.close() statements around. In that case, you can fix your HAProxy configuration instead. Again, the variables you'll want to set are timeout server and timeout client . You'll probably want to set them to an identical value, which should be the maximum length of your database-manipulating Celery task, and then ample room to spare. The maximum reasonable value that you can set here is that of your backend server's wait_timeout configuration variable, which defaults to 8 hours . Careful though, while MySQL interprets timeout settings in seconds by default, HAProxy defaults to milliseconds. You'd thus need to translate the 28800 default value for MySQL's wait_timeout into a timeout server|client value of 28000000 for HAProxy, or else you set the HAProxy timeout to a value of 28800s (or 8h , if you prefer). Background research contribution credit for this post goes to my City Network colleagues Elena Lindqvist and Phillip Dale , plus Zane Bitter for the tenacity suggestion. Also, thanks to Murat Koç for suggesting to clarify the supported time formats in HAProxy.","tags":"hints-and-kinks","url":"resources/hints-and-kinks/chewy-celery/","loc":"resources/hints-and-kinks/chewy-celery/"},{"title":"Paying People","text":"In my now almost 10ish years of first running my own company, and then managing a team in the company I sold my company to, I have very frequently struggled with what's a \"fair\" way of paying people. I've come to the conclusion that when it comes to hiring distributed, and thus hiring people living in different countries, there is no such thing. At least not one that everyone agrees on to be fair. The best thing we can hope for is an approximation of fairness. How much do you earn? If I ask you how much you make, you'll either be very offended at the invasion of your privacy, and refuse to answer, or give me an answer straight away. Which it is, is likely going to be determined by your culture and your upbringing. I am Austrian, I have interacted a lot with Americans, and now I work with Swedes — I can tell you, there are very different views on this. Whether or not you choose to answer, you'll likely have a number in your head. And again, there are multiple ways you'll think of that. It could be your annual salary, before income tax. It could be your monthly salary, before income tax and public healthcare and pension payments. It could be really weird, as in Austria: you'd think of your \"monthly\" salary, but in reality that's 1/14th rather than 1/12th of your annual salary, because you're paid a double salary in June and November. But whichever it is, you'd probably think of a number, in your home currency. Equal work, equal pay. Right? I hope we generally agree that two people who do the same (or equivalent) work should be paid the same. And as long as you're comparing two people, living in the same place, whose salary is paid out in the same currency, that's easy. But what about people from different countries? We can even assume that those two countries use the same currency, to facilitate the discussion. (Talking about different currencies makes things even more murky, and is perhaps a topic for another article.) What's the actual \"worth\" of the money you make? There's no correct answer for this. There are two possible approaches, both of which are \"wrong\" in a way and \"right\" in another. Let's say we're talking about two people being paid 3,000 euros a month, one living in Finland, one in Greece. You could say that a euro is a euro. Thus, if you've got € 3,000 in your hand in Finland, that's the same as having € 3,000 in Greece. Or you could look at the question, what does that euro buy? If you've got € 3,000 in Finland, that will buy you things that, on average, you need to spend only about € 2,080 euros on in Greece. So look at two people, one living in Finland and one in Greece. You pay them both € 3,000 for the same work. Are you paying them equally, or not? I think you wouldn't. If I had a person working for me in Finland, and I paid that person € 3,000, and another person in Greece that also made € 3,000 for the same work, I'd be massively short-changing the person in Finland. But you can argue that € 3,000 is € 3,000 and that's the end of the story. I tend to think that economists are on my side — that's why the concept of purchasing power parity (PPP) exists —, but you can certainly be of the opposite opinion. 1 And then there's the problem that most PPP conversion rates are per-country. And they may be way off if you compare, say, someone living in Athens to someone living in Äteritsiputeritsipuolilautatsijänkä , to stick with the Greece and Finland example. The fact of the matter is, neither approach is perfect, and I can only choose one approach or the other. And the approach that I've chosen, in the distributed team that I run, is to make PPP adjustments. Others opt for a different approach. Neither of these is right, or wrong. They're both an attempt to treat your people equally, and neither is perfect. I will put forward one small thought though: if you are convinced that the money you earn is just that number in your currency (as in, € 3,000 is € 3,000), then I posit that you should also stick to that opinion in the face of inflation. Under that assumption, then € 3,000 next year is still the same € 3,000 it was this year, regardless of whether inflation was maybe 5%. ↩","tags":"blog","url":"blog/2020/02/04/paying-people/","loc":"blog/2020/02/04/paying-people/"},{"title":"Why do they always lie?","text":"Recently I came across a tweet from my Irish OpenStack community friend Dave Neary , in which he wondered aloud why a picture, which was very obviously (and poorly) doctored, made its way onto Twitter. As if, so goes the reasoning, the creator of the picture was ass enough to assume that no-one would notice. It's so obvious […] I just don't understand why you'd bother. More generally, you can summarize this befuddlement by rephrasing the question as follows: \"why would anyone, in a political campaign even, run with a lie that's so easily called out?\" This assumes that lying, deception, is something you'd prefer to go undetected. And generally that's true, human behavior works exactly that way: when we lie and deceive — and humans do this all them time for many reasons, some of them benign — the deception only works if it isn't caught. Why do they lie, when it's so easy to tell? Then, what makes humans lie and spread falsehoods, even when they're easily detected? I submit that to understand why, you should play a game. The game is called The Evolution of Trust , and its creator is Nicky Case . You can play it online, it's available in multiple languages. And it will take only about 15 minutes to complete. Go play it. No, really do. Did you play it? No? Well it's here . Please go play it. Done? OK. Let's carry on. What does The Evolution of Trust tell us? The game-theoretical concepts that The Evolution of Trust introduces tell you three things about its simple game of cooperation and defection (playing by the rules vs. cheating): If communications between players are perfect, then the most successful strategy is tit-for-tat (\"copycat\"). If communications are imperfect, then the most successful strategy is tit-for-two-tats (or \"tit for tat with forgiveness,\" or \"copykitten\") — up to a certain error rate in communications. If communications are imperfect beyond that error rate threshold, then the most successful strategy is to always cheat (\"cheater\"). Now, real human communications are always messy, so the perfect communications scenario is out the window. We're always dealing with imperfect communications, but we never know how high our error rate is. And most of us are brought up with the Golden Rule and a certain measure of forgiveness. We tend to be \"copykittens.\" But now put yourself in the shoes of someone who has decided they'll take the cheater role. They'll not play by the rules, they'll only ever fend for themselves and their own, everyone else be damned. They've chosen the asshole route. Their problem is, they can't win. Game theory literally tells us that the forgiveness strategy is superior. Except if the cheater manages to increase the error rate beyond the threshold. If you've decided you want to be an asshole, lie. It's your only chance. So once you've decided that you'll not play by the rules, your only shot at winning is to destroy communications — for everyone. And your best shot at that is to lie. Never tell the truth, contradict objective facts, say the stupidest, dumbest, most blatantly false things. Small lies, large lies, medium-sized lies. It does not matter if your lie is exposed, in fact, your lies must be exposed for your strategy to work. And now you know how to spot someone in politics who has decided to break the rules. And why you shouldn't assume they're stupid, just because they say objectively stupid things.","tags":"blog","url":"blog/2020/02/04/why-do-they-always-lie/","loc":"blog/2020/02/04/why-do-they-always-lie/"},{"title":"Salacious Salad and Omelette","text":"Sometime last year I came across a post on /r/food that I seem to be unable to dig up. However, I recall that it had an infographic asserting that for a salad to be perfect, it had to have something tangy, something sweet, something crunchy, some protein (egg or meat), some dairy (yoghurt or cheese). So I goofed around preparing breakfast one Sunday morning, and out came this. Ingredients Amounts are per person. Multiply as needed. Salad: A fistful of rocket leaves (arugula). Some grapes, the smaller the better. A few cherry tomatoes. Best fresh off the vine in the middle of summer, even better if you can mix red, yellow, and purple varieties. A few basil leaves. A small amount of your favorite cold cut, say a thin slice or two of prosciutto crudo or bresaola (or Bündnerfleisch , if you want to get super fancy). A few thin slices of Parmigiano or Grana Padano. Cutting them off the piece with a potato peeler works really well. Pinch of black sesame seeds. Vinaigrette: 1 teaspoon balsamic vinegar Pinch of salt A few turns of freshly ground black pepper Half a teaspoon of mustard Half a teaspoon of honey A dash of something spicy, if you're into that sort of thing. Sambal oelek or sriracha sauce will work just fine. 3 teaspoons olive oil Omelette: 1 egg Pinch of salt Butter (for frying) Equipment 1 medium-size bowl 2 small bowls Whisk Small frying pan Method Prepare the tomatoes: using a properly sharp kitchen knife, cut them into halves, quarters, or slices. Chuck them into a small bowl and sprinkle them rather liberally with salt and black pepper. Stack the basil leaves on top of each other and roll them up like tobacco leaves for a cigar, then cut the rolls into slices as thinly as you can. Throw the thin basil strands into the bowl, add some olive oil, and mix thoroughly using a spoon or your hands. Let the bowl sit on the countertop for five minutes or so, so that the tomatoes start oozing a bit of juice. Make the vinaigrette: in a wide-enough bowl, put in salt, pepper, mustard, honey, and balsamic vinegar, and optionally the spicy condiment. Whisk to mix nicely, then add olive oil and whisk vigorously for 30-60 seconds. The goal is to get an opaque emulsion, with the mustard and honey acting as natural emulsifiers . Throw the rocket leaves and grapes into the vinaigrette bowl and mix thoroughly with (cleanly washed) hands. Let sit for a few minutes so that the vinaigrette can infuse the leaves. Crack egg into a small bowl, beat thoroughly with the whisk so the mixture is homogenous. Add a bit of salt. Make sure your serving plate, pieces of meat, cheese shavings, and black sesame are close by (if you're a mise-en-place enthusiast, you've likely done this already) — once the omelette is ready, you want to serve things up quickly. Heat up some butter in the small pan. Pour the beaten egg in and cook your omelet. It will be very thin so this should take only a minute, perhaps two. Throw the omelet on the plate (don't fold it) and then start stacking: rocket and grapes in vinaigrette at the bottom, then tomatoes in olive oil and basil, meat, cheese shavings finally sesame on top. Serve. Nutrition facts No warranty of any kind on these. Values are per serving. Calories (kcal) 343 Total fat (g) 28.1 Saturated fat (g) 8.8 Total carbohydrates (g) 12.1 Sugars (g) 8.2 Protein (g) 12.9","tags":"blog","url":"blog/2020/01/01/salacious-salad-and-omelette/","loc":"blog/2020/01/01/salacious-salad-and-omelette/"},{"title":"My 2010s","text":"This is my look back at the decade we're just finishing up. 1 Some stats Besides managing to grow 10 years older, in this decade I founded 1 company, sold 1 company, left 1 company, joined 1 company, folded 0 companies, bankrupted 0 companies, raised precisely €0 of VC, provided an income to a growing family, kept the bank accounts in the black throughout, spent 836 days (2 years, 3 months, 2 weeks and 2 days) on the road, traveled 947,000 kilometers (that's about the length of a circumlunar free return trajectory), visited 144 cities in 32 countries (including airport stopovers), and gave about 72 talks (on average a little over one every two months) at conferences and events. hastexo More than half the decade (6 years and 1 month, to be precise) is occupied by my tenure at hastexo, the company I co-founded and led from inception to acquisition by City Network . Coming off an excellent stint at Linbit — where I worked for 4.5 years, and which I'm pleased to report is still around, alive, independent and kicking (a rare feat in this industry) — my co-founders Martin, Andreas and I bootstrapped a company that operationally broke even in its third month, and earned its founding cost back in six (while providing us a livelihood out of operational revenue). Though we never went through any kind of meteoric rise or exponential growth — hardly a thing in professional service companies devoid of hockey sticks — we did make good calls in gaining a foothold in the Ceph and OpenStack communities early on, and quickly established a reputation as technical experts. We parted ways three years in, and if we follow the analogy that that sort of thing is something like a divorce, this was a particularly amicable one. All of us are still on good terms, and even occasionally have the opportunity to collaborate. hastexo also enabled me to meet my brilliant colleagues Adolfo (with whom I still work) and Syed (who has since done a career pivot and works in a completely different part of our industry). City Network Making the decision to sell hastexo to City Network in 2017 is something that I've never regretted. I generally get along very well with the Scandinavian approach to work — something that I had learned 10 years earlier when I had the pleasure of interacting regularly with then-independent MySQL AB —, and City Network is no exception here. Obviously an integration into an acquirer is never entirely devoid of friction, 2 particularly when a fully distributed team meets a previously fully office-driven company, but our colleagues turned out to be an excellent bunch and I'm on a very good working basis with my CEO Johan. At City Network I've also finally succeeded in doing something I'd always failed at in the years prior, which is to have built a gender-balanced team. Namrata and Elena are fantastic assets in a highly professional, highly functional, and generally awesome group. People Apart from the excellent people I get to work with on a daily basis, I have met an astonishing number of utterly amazing folks in this decade. In fact, many of them have had so much influence on me that I find it hard to believe I didn't even know them a decade ago. It's straight-out impossible to list them all, so I've representatively picked three people here. Sage Weil of Ceph fame is possibly the strongest pairing of brilliance and humility you'll ever encounter. How many people do you know that made fuck-you money from something they thought up in their PhD thesis , then took some of that fuck-you money as a donation to their alma mater where it endows a professorship that their PhD advisor now holds ? 3 It's an absolute privilege to know this guy. Marco Ostini is, and will always be, the face of linux.conf.au for me, and I need to mention him here for his own sake and that of the community he is a part of. When I arrived for my very first LCA in 2011, as a completely inexperienced traveler, fatigued, jet-lagged and bleary-eyed as all hell, there was this wonderful Aussietalian with a beaming smile giving me the warmest of welcomes at half-past midnight and cheerfully gave me a lift to the accommodation. LCA 2011 was my best conference up to that point, ran what still appears to be my most popular conference talk (largely thanks to Tim Serong 's live cartooning), and kicked off a series of LCAs for me that were always a warm fuzzy shot-up-the-arm of Aussie and Kiwi hospitality in the middle of the dark European winter. Not to mention the talks. And Pac-Man. And hugs. Sharone Revah Zitzman has been my constant and unbroken link to the Israeli cloud, open source, and DevOps community. Sharone is basically a conference organizing committee on two feet, and an incredibly nice and welcoming person to boot. I've forged many friendships in the Israeli developer community that would never have happened were it not for her, and I love coming back to her neck of the woods for that reason. Me Honestly, I really dislike reading old emails (whether I sent them in private or to public mailing lists) from the early 2010s, because they remind me that more than occasionally I was abrasive to the point of being an outright jerk. I hope that that has improved somewhat. I think I did pretty well in the \"immersing myself in new technology and keeping current in it\" department. At the start of the decade I knew next-to-nothing about Ceph, OpenStack hadn't even started, and Open edX wasn't yet under the AGPL. Today I feel kinda-sorta-OK in two of those, and not-quite-an-idiot in the other, which is about as happy as I'll ever be with my limited knowledge of anything. I've also finally allowed myself to feel reasonably comfortable about what I do as a manager, even if it means deviating from conventional wisdom or established precedent. So let's see what the next decade holds. Happy new year, everyone! Yes, I know. It's arbitrary. Gregorian calendar yada yada, plus the discussion whether the decade ends at the end of 2019, or the end of 2020. I don't care. Now is as good as any time to look back and reflect. ↩ I should really do a conference talk for startup founders on what to expect when you're being acquired one day. ↩ That's pretty awesome academia bragging rights for the professor, too. ↩","tags":"blog","url":"blog/2019/12/31/my-2010s/","loc":"blog/2019/12/31/my-2010s/"},{"title":"Exceptional Pan Pizza","text":"I've found this to be the pan pizza to end all pan pizzas. It's my standard pizza dough recipe, plus some inspiration from this video . The trick with seasoning the pan is an absolute kicker and makes the crust taste oodles better than without that seasoning. Ingredients Amounts are for one 26 cm pan that will likely feed 4 adults. Make several to accommodate a larger crowd, or very hungry people. Crust: 200g plain spelt flour 1 150g wholemeal spelt flour 150ml warm water 2 10g fresh yeast (alternatively 7 grams dry yeast) ½ teaspoon honey (optional) 7g salt good splash of olive oil, about 1-2 tablespoons Sauce: 200g peeled and chopped canned tomatoes (alternatively a similar preparation from homegrown produce) Pinch of salt 1 tablespoon of olive oil Toppings: 100g cheese (mozzarella is canonical, but coarsely grated mild young Gouda works surprisingly well and you should give it a shot if you have it available) spicy sausage, bell peppers, mushrooms, ham, whatever you fancy Pan seasoning: Splash of olive oil Tablespoon of cornstarch Pinch of salt Ground black pepper Oregano Equipment Required: One large, thick-bottomed pan with a lid. Can be a cast iron skillet, or a non-stick pan, or a thick enamelled frying pan which is what I use. What's important is that the bottom is at least 1cm thick, and that the handle can stand being under your oven broiler/grill for about 5 minutes. Stovetop. Oven with broiler/grill. Optional: Stand mixer or kitchen appliance with a dough hook. If unavailable, your hands will work just fine. Method Prepare a poolish: dissolve the yeast (and honey, if you like) in half of the warm water, add flour until the mixture is something like porridge. Put in a warm spot 3 and let rise until the volume has doubled (15-30 minutes). Add salt and poolish to the remainder of the flour in a bowl, knead and add enough water to make a homogeneous dough. Might take the full remaining 150ml, or less, depending on flour. Pour olive oil into bowl and and work some in, leaving the sides of the bowl nicely greased. Chuck bowl back into warm place and let rise for another 15-20 minutes. While dough rises, season the pan. Pour in olive oil, add cornstarch, and rub the mixture with your fingers over the whole inside of the pan — both the bottom and the walls. Sprinkle salt, pepper, and oregano into the pan. Gently spread the dough into a flat round piece roughly the diameter of the bottom of the pan. You can use a rolling pin for this, but if you do, make sure to grease the rolling pin and countertop with olive oil, rather than dusting them with flour like you're perhaps used to. Let the dough rise one more time, about 15 minutes. Prepare the sauce. Simply mix chopped tomatoes with a good pinch of salt and some olive oil. Spread the sauce all across the dough , covering the whole diameter of the pan. Do not leave any uncovered crust on the perimeter. Repeat with cheese and finally, toppings. Turn burner on medium heat, put the pan on (cover it with a lid), and cook for approximately 8-10 minutes. The trapped steam will cook the sauce and toppings on top, while the bottom of the pan bakes the crust. Preheat your broiler to high heat. When the cheese on top has started to melt, throw the pan under the broiler/grill for about 3 minutes until the cheese gets nice patches of golden brown. Turn pizza out on a round pizza plate. It should easily come off the bottom of the pan, though the sides might require some scraping if cheese has melted and run down the sides. Cut into slices. Dig in. Nutrition facts No warranty of any kind on these. Values are per serving, counting one serving as one-quarter of the whole pizza. Calories (kcal) 479 Total fat (g) 17.0 Saturated fat (g) 6.7 Total carbohydrates (g) 64.6 Sugars (g) 3.5 Protein (g) 21.1 In case you've never baked with spelt flour before: tastes about like wheat, but takes on less water. You can modify this recipe to use wheat flour, in which case you'll need about 375ml of water. Also, while a wheat dough normally benefits from a long or slow rise, I've found that not to be true for spelt. ↩ If you're from the U.S.: yes I know, we Europeans are a bit weird in that we customarily give some quantities by weight, others by volume. You'd think it'd be straightforward that we do solids by weight and liquids by volume, but it isn't. (Some recipes specify sugar by weight, for example, others say use so-and-so-many tablespoons of sugar.) ↩ My oven has a leavening mode in which I can let a dough rise at approximately 39°C and near 100% humidity, which is glorious, but this is in no way a requirement. I've let dough rise in a bowl placed on the floor (we have floor heating), on the running laundry dryer, or out on the countertop. ↩","tags":"blog","url":"blog/2019/12/30/exceptional-pan-pizza/","loc":"blog/2019/12/30/exceptional-pan-pizza/"},{"title":"DevOpsDays Tel Aviv 2019","text":"This year, DevOpsDays Tel Aviv accepted two of my submitted talks: No really, don't chuck everything in Slack: Communications for distributed teams This is a 40-minute talk that I presented after keynotes on day 2. It deals with the specific challenges that distributed teams face and solve, and has a bunch of ready-to-go suggestions to communicate better as a distributed team. I had two surprises in this talk: A large number of people still appear to be unfamiliar with the term naked ping , even though just about everyone is very familiar with the antipattern itself. It resulted in an \"oh so that's what that's called!\" reaction from a significant share of the audience. I usually try to not throw shade in my talks. But if and when I do it's usually about Scrum, which I continue to consider a patently ludicrous idea. I did mention Scrum in a negative manner in my talk, and got a rather unexpected round of mid-talk applause. (My talks generally tend to be rather matter-of-factly; mid-sentence applause is not something I'm used to.) Speaking to a crowd that skewed hard toward the software engineering profession, this always gets me thinking: engineers understand that Scrum is horrible; when will their managers catch on? The video for this talk is forthcoming, but for now you can find my slides (with full speaker notes) on GitHub . Five is Fine: A case for small teams This was a 5-minute talk delivered as part of a round of \"Ignite\" talks. I use scare quotes because DevOpsDaysTLV uses a somewhat relaxed Ignite format: you must deliver your talk in 5 minutes, but you're not restricted to the exact number of 20 slides, and your slides also don't auto-advance every 15 seconds. I did not know this, so I did follow the original Ignite format, using reveal.js autoSlide to advance my slides every 15000 ms. These talks were also recorded, and the recording should become available relatively shortly (I will update this post when they do). As with the other talk, the slides are available on GitHub and include my notes. I'd like to thank Rachel for suggesting that I write this talk. Know a conference that might like these talks? If you organize a conference that might be interested in including these talks, or you've attended one that you think might, please find me on Twitter and let me know. I'll be happy to submit one of them for consideration. I could definitely expand the Ignite talk into a full standard-length talk — doing the reverse for the other one might be a bit challenging, though.","tags":"presentations","url":"resources/presentations/devopsdays-tel-aviv-2019/","loc":"resources/presentations/devopsdays-tel-aviv-2019/"},{"title":"Slidecraft updates","text":"I've been doing public talks and presentations rather frequently for the last 10-or-so years, but this year I made significant changes to my process for creating, rehearsing, and presenting talks. What I used to do When I started doing talks a while back, I followed an approach that many of us were at some point either taught, or adopted by emulating our peers: I would roughly sketch an outline, then I'd create slides (usually on a company or conference template), then I'd add some speaker notes in bullet-point fashion, then I'd rehearse the talk, and finally I'd refine my content based on whether I was short or long in terms of the available time slot. That's a very conventional approach, and it tends to focus very much on getting the slides right. Today I do things differently. What I do now I've come to the conclusion that what I really want my audience to focus on is what I am talking about. So now, I do this: I still start with a rough — very rough — outline. Next though, I write my speaker notes. All of them. Yes, that means I write out my entire talk, and this may well take days. Then, I do a first practice run. Is there a good natural flow? Will it make sense to someone completely unfamiliar with my topic? Is the story I'm telling logical? Then, I edit. This process of alternating rehearsal and edits continues until I'm reasonably happy with the whole talk, and I have timed it and am happy with my pacing, too. Only then do I start creating my slides, and I usually completely disregard conference templates, for reasons I'll get to in a moment. Yes, that means that when I ultimately deliver the talk, I'm actually reading from my notes. Except when I'm just riffing and ad-libbing over them. Chances are, unless you know me very well, you'll be unable to tell. That's because I write my notes like I talk, and I pay more attention to flow and stress and rhythm than I do to grammar and exactitude. This talk , just like this , and this were all delivered from fully written speaker notes. Fully writing out my talk has also enabled me to greatly reduce my use of fillers (like \"ah\" and \"um\"), which I used to say excessively and which would make me cringe at my talk videos. Accessibility Now to explain why I normally disregard conference templates: In preparing and delivering my talks, I try to put a greater emphasis on accessibility that I used to before. my slides are now all high-contrast. I default to using black text on a white background (this is a good default for when I have reason to assume the projection equipment will be less than perfect), alternatively I use white text on a black background (for darkened rooms). I try to accommodate people with the most common types of color blindness: I use blue — as opposed to red or green – as my highlight color, and in charts , I differentiate by both color and line stroke. I also try to refrain from using animations. As for slide transitions, I use only fade, no motion. I publish my slides ahead of time, include a QR code at the very beginning, and I use reveal.js Multiplex so that my slides advance in unison with those one people's phones, tablets, or laptops. This way, people with less-than-perfect vision (or simply seated in an unfortunate spot, at the back of the room) can follow along easily and and their own preferred zoom level. Since my speaker notes are fully written out, this means that I can also include them in the published material, so that my notes can act as subtitles for my speaking. This can come in handy to people who are hard of hearing, or who are simply unaccustomed to my accent or manner of speech. I've rolled these accessibility considerations into my opinionated, Cookiecutter -based reveal.js presentation generator . A request If you happen to be visually impaired, or color blind, or hard of hearing, or you work with people who are – in other words, if you can make a suggestion for me to improve the accessibility of my slides, please file an issue against my Cookiecutter and I'll try to work that in as best I can. Thank you!","tags":"blog","url":"blog/2019/12/13/slidecraft/","loc":"blog/2019/12/13/slidecraft/"},{"title":"Ceph Erasure Code Overhead Mathematics","text":"So you're running a Ceph cluster, and you want to create pools using erasure codes , but you're not quite sure of exactly how much extra space you're going to save, and whether or not that's worth the performance penalty? Here's a simple recipe for calculating that space overhead. Suppose a RADOS object has a size of $S$, and because it's in an EC pool using the jerasure or isa plugin, 1 Ceph splits it into $k$ equally-sized chunks. Then the size of any of its $k$ chunks will be: $$S \\over k$$ In addition, we get $m$ more parity chunks, also of size $S \\over k$. Thus, the total amount of storage taken by an object of size $S$ is: $$k \\cdot {S \\over k} + m \\cdot {S \\over k}$$ This of course we can rearrange and reduce to $$S + S \\cdot {m \\over k}$$ or $$S \\cdot (1 + {m \\over k})$$ In other words, the overhead (that is, the additional storage taken up by the EC parity data) is $$S \\cdot {m \\over k}$$ or when expressed as a proportion to $S$, simply $$m \\over k$$ As an example, an EC profile with $k = 8, m=3$ comes with a storage overhead of $3 \\over 8$ or 37.5%. One with $k=5, m=2$ has an overhead of $2 \\over 5$, or 40%. And finally, a replicated (conventional, non-EC) pool with 3 replicas can be thought of as having a degenerate EC profile with $k=1, m=2$, resulting in an overhead of $2 \\over 1$, or 200%. On a parting note, you should realize that the space utilization overhead is only one factor by which you should weigh erasure code profiles against one another. The other is performance. Here, the general (deliberately oversimplified) rule is that the more chunks you define — in other words, the higher your $k$ — the higher the performance penalty you suffer, particularly on reads. 2 This is due to the fact that in order to reconstruct the object and serve it to the application, your client must collect data from $k$ different OSDs and assemble it locally. 3 Thanks to Lars Marowsky-Bree for reminding me that slightly different arithmetics apply to the lrc , shec , and clay plugins. ↩ Thanks to Lenz Grimmer for pointing out that the post should make this clear. ↩ If you want to know more about erasure codes and their history, not limited to their use in Ceph, Danny Abukalam did an interesting talk on the subject at OpenStack Days Nordic 2019. ↩","tags":"hints-and-kinks","url":"resources/hints-and-kinks/ceph-ec-math/","loc":"resources/hints-and-kinks/ceph-ec-math/"},{"title":"The Little Bag Of Tricks: 10 Things You Might Not Know You Can Do With OpenStack","text":"My presentation from the Open Infrastructure Summit 2019 in Shanghai. Video: YouTube Slides: GitHub Use the arrow keys to navigate through the presentation, hit Esc to zoom out for an overview, or just advance by hitting the spacebar.","tags":"presentations","url":"resources/presentations/the-little-bag-of-tricks-10-things-you-might-not-know-you-can-do-with-openstack/","loc":"resources/presentations/the-little-bag-of-tricks-10-things-you-might-not-know-you-can-do-with-openstack/"},{"title":"Using ftrace to trace function calls from qemu-guest-agent","text":"When you are using functionality that is buried deep in the Linux kernel, ftrace can be extremely useful. Here are some suggestions on how to use it, using the example of tracing function calls from qemu-guest-agent . What's this about? Recently I used, for the first time, libvirt 's functionality to indicate to a virtual guest that it is about to have a point-in-time copy of its disks — a snapshot — taken. In doing so, it can tell the virtual machine (VM) to freeze I/O on all its mounted filesystems. The rationale behind this is, I hope, obvious: you want the VM to momentarily stop I/O to its virtual disks, so that you can take a snapshot when no I/O is in-flight, and the snapshot image can thus be expected to be internally consistent. The snapshot itself will only take a second or so, and the minor interruption is a small price to pay for the added consistency guarantee you get. You might be wondering how this works and it is, indeed, a bit involved. First, you'll need a virtual serial console that allows the hypervisor (in the host) to communicate with the guest. This will be defined in your libvirt domain XML , and in OpenStack Nova , this automatically pops up if you are booting your instance off an image which has the hw_qemu_guest_agent=yes property set . Then, you'll need a daemon within the guest that listens for commands received over the serial port. This daemon is called qemu-guest-agent , or qemu-ga for short. All you'll need for it to run is to install the package of that name, which you can do in various ways ( apt-get install qemu-guest-agent being the simplest, on Ubuntu guests). One of the many commands that said daemon supports is guest-fsfreeze-freeze . When it receives that command over the virtual serial link, the daemon will loop over your mounted filesystems , and issue the FIFREEZE ioctl on all of them. This happens in reverse order, meaning your root ( / ) filesystem is frozen last. That ioctl then calls the freeze_super() kernel function , which flushes each filesystem's superblock, blocks (\"freezes\") all new I/O to the filesystem, and syncs (flushes) all I/O that is currently in flight on that filesystem. The combined net effect of all of the above is that you get a virtual machine that is temporarily read-only, with pending I/O piling up, until you are done taking your snapshot. When that happens, there are a few more actions that happen: The hypervisor sends the guest-fsfreeze-thaw command over the virtual serial link. Now, the daemon will loop over all your mounted filesystems again , and issue the FITHAW ioctl on them. This time, it is taking the mounts in forward order, thawing the root filesystem first. That ioctl then calls the thaw_super() kernel function , which unblocks (\"thaws\") all new I/O to the filesystem, and allows the VM to continue normal operations. Now there's a bit of an issue with that. All of the aforementioned kernel functions only write printk 's on error , but they don't tell you when they succeed. So you can try a snapshot, then type dmesg in the guest, and you'll have no way of telling whether the whole freeze/thaw dance succeeded, or was never even attempted. But fear not, there's a way that you can trace exactly what the kernel is doing! tracefs, and configuring ftrace If your guest runs any modern kernel, then chances are that it will, by default, mount a virtual tracefs filesystem to the /sys/kernel/debug/tracing mount point (although as of kernel 4.1, this is nominally an alias, with /sys/kernel/tracing being the canonical mount point). Regardless of its path, tracefs exposes the kernel's ftrace functionality . So the first thing you'll tell ftrace, in your guest VM, is the process for which you'll want to do function tracing. In our case, that's your guest's qemu-ga . So, you can do: pidof qemu-ga > /sys/kernel/debug/tracing/set_ftrace_pid Then, you'll want to instruct ftrace to trace kernel function calls: echo \"function\" > /sys/kernel/debug/tracing/current_tracer And, you'll want to make sure that we don't trace only function calls from qemu-ga itself, but also from its child processes: echo \"function-fork\" > /sys/kernel/debug/tracing/trace_options Let's see what's happening! Now you have a guest that's properly instrumented for tracing kernel function calls that originate with qemu-ga . So now, go ahead and take a snapshot. On OpenStack Nova, you'd do that with: openstack server image create --name <image-name> <instance-name> Then, shell back into your guest, and interrogate your trace for ioctl calls: grep -E '(freeze|thaw)_super.*ioctl' /sys/kernel/debug/tracing/trace And voilà: qemu - ga -14574 [ 001 ] .... 264.059109 : freeze_super <- do_vfs_ioctl qemu - ga -14574 [ 001 ] .... 265.837955 : thaw_super <- do_vfs_ioctl qemu - ga -14574 [ 001 ] .... 265.855048 : thaw_super <- do_vfs_ioctl qemu - ga -14574 [ 001 ] .... 265.855084 : thaw_super <- do_vfs_ioctl So that's the FIFREEZE ioctl that maps to freeze_super() , and the FITHAW ioctl that maps to thaw_super() . And that's how you know that your guest is freezing and thawing I/O as you expect it to! Where to go from here Feel free to dig further into your trace file ( cat or less will help), and play with other ftrace options. There's a massive amount of things you can do with it, as the documentation explains. You'll probably also find this blog post from Julia Evans useful for exploring ftrace . Also, thank Steven Rostedt when you see him! He is the primary author of the ftrace framework.","tags":"hints-and-kinks","url":"resources/hints-and-kinks/ftrace-qemu-ga/","loc":"resources/hints-and-kinks/ftrace-qemu-ga/"},{"title":"Learn Complex Skills, From Anywhere: Combining Django, Ansible and OpenStack to teach any tech skill","text":"This is a talk I submitted 1 to three separate conferences: PyCon AU 2018 , via an anonymized CfP process using PaperCall . This submission was rejected. linux.conf.au 2019 , which used a non-anonymized CfP process on a custom platform that, I think, is built on Symposion . That submission was accepted, and the talk ran in the main conference programme . PyCon DE 2019 , via a non-anonymized CfP process using pretalx . This submission was rejected. It's the linux.conf.au submission that is reflected in this page. Title Learn Complex Skills, From Anywhere: Combining Django, Ansible and OpenStack to teach any tech skill Target Audience Community Abstract This will appear in the conference programme. Up to about 500 words. This field is rendered with the monospace font Hack with whitespace preserved Professional skill-building is challenging, particularly when the skill to acquire is about distributed, scalable platform technology. In this talk, I cover an open-source skill-building platform that is 100% Python: built on Open edX and heavily involving Django, Ansible, and OpenStack. The information technology industry is currently dealing with an interesting challenge in professional skill-building: almost every new technology developed in recent years has been complex, distributed, and built for scale: Kubernetes, Ceph, and OpenStack can serve as just a few representative examples. Loose coupling, asynchronicity, and elasticity are just some of the qualities frequently found in such systems that were entirely absent in many of the systems we built only a few years ago. As a result, people comfortable with building and operating these complex systems are hardly found in abundance, and organisations frequently struggle to adopt these technologies as a direct result of this scarcity: we are dealing with a skills gap, not a technology gap. This means that we need novel ways to educate professionals on these technologies. We must provide professional learners with complex, distributed systems to use as realistic learning environments, and we must enable them to learn from anywhere, at any time, and on their own pace. One excellent way of doing this is to use the capabilities of the Open edX platform to integrate a learning management system with hands-on, on-demand lab environments that can be just as complex, and just as distributed, as production systems. This allows anyone interested to develop a professional skill set on novel technology at minimal cost, and without the need for costly hardware platforms for evaluation. In this talk, I will give a rapid technical introduction to the core components of this free and open source (AGPL 3/ASL 2) all-Python platform: edx-platform, the core learning management system (LMS) and content management system (CMS), built on Django; edx-configuration, the automated deployment facility to roll out the Open edX platform, built on Ansible; and finally, the Open edX XBlock extension system and its integration with OpenStack, also itself an all-Python cloud platform, in order to provide on-demand lab environments from both private and public cloud environments. Private Abstract This will only be shown to organisers and reviewers. You should provide any details about your proposal that you don't want to be public here. This field is rendered with the monospace font Hack with whitespace preserved I come from a background in technical consulting and instructor-driven professional education, and together with my team have been building and deploying Open edX based platforms as described in the talk since 2015. I believe I have a good understanding on why instructor-driven training, while desirable, is not accessible to everyone in need of keeping abreast with technology development, and that a self-paced, learn-from-anywhere alternative is needed. I am extremely grateful for the fact that we have an very well-suited platform for that purpose, and since it has a completely open-source, Python codebase, it might be of interest to LCA attendees. I have done a talk on a similar topic at the LCA 2018 Education miniconf (video link included below). In the 2018 talk, I focused primarily on the educational aspects of self-paced, on-line training. This time, and I think more appropriately for the main conference track, I would like to dive into the nuts and bolts of the platform that drives this. As such, the talk should still be appealing to people engaged in professional education (be it as learners, tutors, or instructional designers), but will also be insightful to Python and OpenStack developers, and heavy Ansible users. Video URL https://www.youtube.com/watch?v=E8BhTAjMwa4 If you're curious why this is here, please read this . ↩","tags":"talk-submissions","url":"talk-submissions/lca-2019-openedx/","loc":"talk-submissions/lca-2019-openedx/"},{"title":"Team meetings","text":"I've run a distributed team — nominally the same team, though through people joining and leaving I am the sole original band member at this point — for almost 8 years. And about 3 years in was the first time we were sufficiently spread and had enough money to spare to warrant flying everyone to the same place for one week per year. We have been doing that ever since. Every year, it's an exceedingly enlightening and pleasant experience. And this year was the first time that we did not one but two team meetings back to back: first with my core team running education services at City Network, in the Hellerup neighborhood in Copenhagen , and then as part of the complete City Network all-hands meeting on the island of Tjärö in the Blekinge archipelago . So here is how we do these things. Accommodation Except for the very first team meeting we did in 2014 when we put everyone up in hotel rooms, we've always rented a house. I'm a believer in small team sizes — 5 people being the maximum number of direct reports a leader can realistically have —, so that means that we can still find houses where everyone has a room to themselves, and nobody has to queue for a shower. I consider both of these things extremely important. Many, many people working in tech are introverts, even more so for people working from home in tech. Many of us find the experience of constantly being around people emotionally draining, and we need solitude to recharge. Also, privacy. Our personal lives don't stop while we're having a team meeting, so you might want to have a room in which you can talk about your kid's health issue with your spouse, without concern about colleagues overhearing the conversation. Price-wise, since this puts us in an accommodation category that can qualify as a penthouse or mansion, this won't be significantly cheaper than hotel accommodation — but it won't be more expensive either, and it'll be a much nicer experience. Particularly if you also have the joy of interacting with an exceedingly pleasant, nice, and helpful person for a host. As for who scouts the accommodation, my rule has always been this: In case we have a person on the team who lives, or has lived, in the city we're going to, and who thus is very familiar with the locality and surroundings, I delegate the search to them. You can never beat local personal experience. Otherwise, I do the scouting on the web, and I usually run it by the team — after I have made the booking, but while we can still cancel or change. Arrival Whoever did the scouting for the accommodation travels a day early, gets the key, settles in, and reports back. This has multiple purposes: If they are local and so is the host, of course that'll facilitate matters a lot. Particularly if we're in a country where that one team member speaks the language, and the rest of us don't. If there's something seriously wrong with the property, they can still cry foul and we can make other arrangements while we're not all congregated in the same spot. (This has never happened to us, but just in case.) That person is our support backstop who can field issues and customer questions, while everyone else is in the air. That person can also meet and collect people at the airport or train station, if getting to our accommodation is nontrivial or difficult. Also, this is particularly helpful when we have a new team member who is perhaps less travel experienced. First day I normally don't schedule any formal work sessions for the first day. People will be jet-lagged and fatigued from travel, we are often re-meeting in person for the first time in a year, and there is a lot of catching up to do about family, hobbies, recent travel, and all sorts of things that are not work. Somebody might be new and we might see them for the first time ever, in person. And then of course people might be delayed in travel, may have had flights cancelled, or may have missed connections. So that means if you're actually planning the first day to be full of \"work\" sessions, you ran a significant chance of your schedule getting wrecked by a flight delay. So we just don't do that. Instead, we try to make the first day as relaxed and as enjoyable as possible, including a nice first group dinner. And then sometimes, as happened this year, a remarkably serious and productive discussion over work issues ensues over a glass of wine in the evening. But that's not part of the expectation. Work sessions We tend to have half-day work sessions these days, where we focus on one topic for 3-4 hours straight. These can get intense, and sometimes heated, but they are nearly always very, very productive. We typically use a place like our house's kitchen (if it's roomy, and has a table), or patio (if it's bearable outside), or sitting room (if it's cozy) for work sessions. They are usually quite analog, with frequently just one person — the assigned record-keeper, often me — with a laptop open to take notes and record the discussion and its outcomes. I've found that on occasion, when discussing complex issues, working with a roll of brown paper and thick felt-tip markers (\"sharpies\" for you Americans out there) can be much more useful than with anything pushing bits. Food I enjoy food, and I've never worked with anyone who doesn't. So we make that part enjoyable in whatever way we fancy. We might go for lunch to the neighborhood bagel store that our host recommended for their excellent pastrami. We might jump on a train to get to a street food spot, or head out for pizza or tacos or curry. And I'm buying. These meetings are for work, my team is traveling for just that purpose, so whenever we eat together (and we practically always do), the tab is on me. What I can put on the company and what comes out of my own pocket is my job to sort out later, but we're definitely not going Dutch. Interesting things Our meetings are usually in an interesting city with art, architecture, and history, and not seeing any of that would be a bit of a waste. So there's usually maybe two to three things that we just go and do. It could be a harbor or river cruise, a visit to a castle or palace, a bicycle tour criss-crossing the city, or a museum visit. Depends a bit on the weather and a bit on individual interest. Epilogue: going larger So what works for a 5-person team clearly doesn't work for a whole company, not least because you'll be hard pressed to find a rental home with 40 bedrooms — I guess such a dwelling would be appropriately referred to as a palace, and last I checked the Queen wasn't on Airbnb. But you can take a page out of City Network's playbook and do something else, which is to book an island. Yes, you read that right, immediately after our team meeting we packed up and boarded a train to join our company all-hands meeting, in which we had an island practically to ourselves.","tags":"blog","url":"blog/2019/06/06/team-meetings/","loc":"blog/2019/06/06/team-meetings/"},{"title":"Learn Ceph — For Fun, For Real, For Free!","text":"My lightning talk from Cephalocon 2019. Video: YouTube","tags":"presentations","url":"resources/presentations/learn-ceph-for-fun-for-real-for-free/","loc":"resources/presentations/learn-ceph-for-fun-for-real-for-free/"},{"title":"Geographical Redundancy with rbd-mirror","text":"My presentation from Cephalocon 2019. Video: YouTube Slides (with full speaker notes): GitHub Use the PgUp / PgDown keys to navigate through the presentation, hit Esc to zoom out for an overview, or just advance by hitting the spacebar.","tags":"presentations","url":"resources/presentations/geographical-redundancy-with-rbd-mirror/","loc":"resources/presentations/geographical-redundancy-with-rbd-mirror/"},{"title":"I Don't Think This Means What You Think It Means: Red Herrings in OpenStack","text":"This is a talk I proposed 1 for OpenInfra Days Nordics , via a non-anonymized CfP process using PaperCall . Title I Don't Think This Means What You Think It Means: Red Herrings in OpenStack Elevator Pitch You have 300 characters to sell your talk. This is known as the \"elevator pitch\". Make it as exciting and enticing as possible. OpenStack's complexity comes with operational challenges. And in situations where OpenStack misbehaves, it is frequently non-trivial to find the actual cause of an issue. This talk includes several examples of red herrings in OpenStack, and suggestions for spotting and avoiding them. Talk Format Talk (>30-45 minutes) Audience Level All Description This field supports Markdown. The description will be seen by reviewers during the CFP process and may eventually be seen by the attendees of the event. You should make the description of your talk as compelling and exciting as possible. Remember, you're selling both the organizers of the events to select your talk, as well as trying to convince attendees your talk is the one they should see. When working with OpenStack, you deal with an environment that is inherently complex. As with all complex environments, things sometimes go wrong or behave unexpectedly. And when that happens, your immediate goal is to locate, pinpoint, and then troubleshoot the issue. And then, sometimes, you go down the dead-wrong path, and end up chasing a red herring for some time, before you find the real problem. This talk contains examples of such red herrings, enabling you to recognize and avoid them. This talk is both for those who run an OpenStack cloud, and those who consume its functionality as a service. It talks about both red herrings in OpenStack operations, and red herrings in operating applications on OpenStack. Notes This field supports Markdown. Notes will only be seen by reviewers during the CFP process. This is where you should explain things such as technical requirements, why you're the best person to speak on this subject, etc… I've been working on OpenStack since 2012, have consulted on lots of private and public cloud deployments using OpenStack, and I work for the operator of a multi-region global OpenStack Cloud. \"I've seen things you people wouldn't believe. Attack ships on fire off the shoulder of Orion…\" In addition to what I have seen, others have seen other things, which is why I am crowdsourcing the content of this talk. That being so, the talk proposal is public , and I am asking people on Twitter to send me their stories, which I will add to and mix with my own, with due attribution. Just to give one example of what I would like to cover, see this article on my web site, which talks about how you can run into what looks like a quota issue in Neutron, but whose cause is in fact buried deep in RFC 5798 . Tags Tag your talk to make it easier for event organizers to be able to find. Examples are \"ruby, javascript, rails\". OpenStack, Operations If you're curious why this is here, please read this . ↩","tags":"talk-submissions","url":"talk-submissions/oidn-2019-red-herrings/","loc":"talk-submissions/oidn-2019-red-herrings/"},{"title":"One For All: Using Terraform to manage OpenStack and Kubernetes resources","text":"This is a workshop I proposed 1 for OpenInfra Days Nordics , via a non-anonymized CfP process using PaperCall . Title One For All: Using Terraform to manage OpenStack and Kubernetes resources Elevator Pitch You have 300 characters to sell your talk. This is known as the \"elevator pitch\". Make it as exciting and enticing as possible. A hands-on introduction to Terraform in an OpenStack and Kubernetes context. Get the basics (of Terraform), then spin up a Kubernetes cluster in an OpenStack public cloud (with Terraform), and manage resources on it (with Terraform). Talk Format Workshop (>60 minutes) Audience Level Intermediate Description This field supports Markdown. The description will be seen by reviewers during the CFP process and may eventually be seen by the attendees of the event. You should make the description of your talk as compelling and exciting as possible. Remember, you're selling both the organizers of the events to select your talk, as well as trying to convince attendees your talk is the one they should see. If you are interested in deployment automation for arbitrarily complex containerized microservice applications, this is for you! In this workshop, you will get to know the basics of Terraform and Terraform configurations , spin up a Kubernetes cluster with Terraform, using the OpenStack provider and interfacing with OpenStack Magnum in a public cloud, start managing Kubernetes resources from Terraform, using the Kubernetes provider . You'll walk away with a solid understanding of Terraform's capabilities, enabling you to make an informed decision of whether Terraform is a suitable deployment automation facility for your organization's needs. Prior Terraform knowledge is not required. Notes This field supports Markdown. Notes will only be seen by reviewers during the CFP process. This is where you should explain things such as technical requirements, why you're the best person to speak on this subject, etc… There are no technical requirements other than internet connectivity, and a web browser (preferably on a laptop, though a reasonably-sized tablet with a modern browser should work as well). Tags Tag your talk to make it easier for event organizers to be able to find. Examples are \"ruby, javascript, rails\". Terraform, OpenStack, Kubernetes, Magnum If you're curious why this is here, please read this . ↩","tags":"talk-submissions","url":"talk-submissions/oidn-2019-terraform/","loc":"talk-submissions/oidn-2019-terraform/"},{"title":"Configuring CLI output verbosity with logging and argparse","text":"In a Python command-line interface (CLI) utility, you will want to inform your users about what your program is doing. Your will also want to give your users the ability to tweak how verbose that output is. Now there is a de-facto standard convention for doing that, which most CLIs — Python or otherwise — tend to adhere to: By default, show messages only about errors and warning conditions. Define a -v or --verbose option that makes your program also show messages that are merely informative in nature. Optionally, allow users to repeat the -v option, making the program even more verbose (to include, for example, debug output). Conversely, also define a -q or --quiet (alternatively -s / --silent ) option that, when set, makes the program suppress warnings and show only errors — i.e. the stuff that your program shows if it exits with a nonzero exit code. Log output that tells users about what the program is doing, as it goes along, to the standard error (stderr) stream, whereas the output related to the program's results goes to standard output (stdout). This gives your users the ability to pipe stdout to a file or another program, and your progress or status messages won't interfere with that. And in Python it's not at all difficult to do that! argparse options First, we'll want to define a couple of options for our argparse.ArgumentParser object , which in the following snippet I've named parser . Define two options, like so: 1 parser . add_argument ( '-v' , '--verbose' , action = 'count' , dest = 'verbosity' , default = 0 , help = \"verbose output (repeat for increased verbosity)\" ) parser . add_argument ( '-q' , '--quiet' , action = 'store_const' , const =- 1 , default = 0 , dest = 'verbosity' , help = \"quiet output (show errors only)\" ) From this, we get two command-line options: -v or --verbose , which can be repeated, sets verbosity , which defaults to 0. action='count' means that if you invoke your CLI with -v , verbosity is 1, -vv sets verbosity to 2, etc. -q or --quiet also sets verbosity , but to a constant value, -1, via store_const . Setting up the logging subsystem What we'll want to do is use the logging subsystem to send our status, progress, and error messages to stderr. First, you can translate verbosity into a logging level understood by the logging module. Here's a little convenience method that achieves that: def setup_logging ( verbosity ): base_loglevel = 30 verbosity = min ( verbosity , 2 ) loglevel = base_loglevel - ( verbosity * 10 ) logging . basicConfig ( level = loglevel , format = ' %(message)s ' ) Now what does this do? Python log levels go from 10 ( logging.DEBUG ) to 50 ( logging.CRITICAL ) in intervals of 10; our verbosity argument goes from -1 ( -q ) to 2 ( -vv ). 2 We never want to suppress error and critical messages, and default to 30 ( logging.WARNING ). So we multiply verbosity by 10, and subtract that from our base loglevel of 30. With -v , that sets our effective log level to 20 ( logging.INFO ); with -vv , to 10 ( logging.DEBUG ). And with -q (i.e. verbosity==-1 ), our log level becomes 40 ( logging.ERROR ). Now we can use logging.basicConfig() to configure the logging subsystem to send unadorned log messages with the desired loglevel or above, to stderr: basicConfig() , by default , sets up a StreamHandler whose output stream is sys.stderr , so it already does what we want here. And setting format='%(message)s' strips the LEVEL:logger: prefix that basicConfig() would otherwise include in the log line (and which is helpful for log files, but not so much for CLI output). From then on, every time your program should write an informational message to stderr, you just use logging.info() , for a debug message, logging.debug() , and so on. Adding an environment variable In some circumstances you might always want debug output, and invoking your CLI with -vv all the time might not be practical. (CI systems are an example — you generally want your build logs as verbose as possible.) You can make your users' lives easier by optionally fixing up your logging subsystem with an environment variable, like so: def setup_logging ( verbosity ): base_loglevel = int ( os . getenv ( 'LOGLEVEL' , 30 )) verbosity = min ( verbosity , 2 ) loglevel = base_loglevel - ( verbosity * 10 ) logging . basicConfig ( level = loglevel , format = ' %(message)s ' ) This way, if you invoke your CLI with LOGLEVEL=10 in its environment, it will always use debug output. Perhaps you'd like to make this even easier, allowing your users to also set LOGLEVEL to debug , INFO , erRoR and whatever else. That you could do like this: 3 def setup_logging ( verbosity ): base_loglevel = gettattr ( logging , ( os . getenv ( 'LOGLEVEL' , 'WARNING' )) . upper ()) verbosity = min ( verbosity , 2 ) loglevel = base_loglevel - ( verbosity * 10 ) logging . basicConfig ( level = loglevel , format = ' %(message)s ' ) Parting thought One of the many ways in which using logging comes in handy in a CLI is in a catch-all exception handler: if __name__ == '__main__' : try : main () except Exception as e : logging . error ( str ( e )) logging . debug ( '' , exc_info = True ) try : sys . exit ( e . errno ) except AttributeError : sys . exit ( 1 ) This way, unhandled exceptions will show merely the exception message by default, but if and only if debug logging is enabled, your users will also see a stack trace. This is used here . ↩ There is, to the best of my knowledge, no way to limit the number of repeats for an argument with action='count' . Hence the construct with the min() built-in function. ↩ A variation of this is used here . ↩","tags":"hints-and-kinks","url":"resources/hints-and-kinks/python-cli-logging-options/","loc":"resources/hints-and-kinks/python-cli-logging-options/"},{"title":"Nonexisticon","text":"I have no experience with, and presently no plans for, running or organizing a grassroots knowledge-sharing conference. But if I did run one, and I could shape it exactly as I wanted, this might be what it would look like. Please be advised that I have no idea what I am talking about. Let's talk about Nonexisticon, the non-existent conference. Just to benefit your reading flow, dear reader, I am using the indicative rather than the subjunctive mood, in other words, I use phrases like \"Nonexisticon is \" as opposed to \"Nonexisticon would be \" 1 . I trust that you are not confused by this, as the name very clearly implies that the conference does not exist. Unifying Theme Nonexisticon's theme is freely shared knowledge work. Nonexisticon brings together open-data researchers, open-source software and hardware engineers and designers, Creative Commoners, documentarians, writers, artists, educators. Basically, if you create something with your mind and you make your creation freely available for anyone to use, enhance, modify and build upon, you're welcome at Nonexisticon. Nonexisticon is also a mutually supportive conference, where corporate-funded or financially secure attendees can commit to supporting less-privileged ones. Location and Reach Nonexisticon is a regional conference. \"Region\" in this context means an area from which the conference attendees can reasonably travel to the conference location, using environmentally friendly transportation modes (such as high-speed rail). Attendance from outside the region is welcome, but for environmental reasons it is not encouraged. Nonexisticon is typically held on a university campus, or other suitable facility, in a location well accessible by public transportation. Cheap accommodation is typically available on-campus. Supporting Organization Nonexisticon is organized by the Nonexisticon Association, a not-for-profit organization that allows only individual, not corporate membership. The organization is staffed by volunteers, that is to say, while it is authorized to reimburse members of the conference team for expenses incurred and income lost as a direct result of conference work, it does not pay salaries to its staff. Nonexisticon registration includes membership in this organization, with full voting rights, for a period of two years. Recurrence and Length Any regional Nonexisticon has its own recurrence schedule, but the conference is never held in the same region more than once per year. Each Nonexisticon has a general theme. Themes can be quite diverse and are generally broadly defined, so as to attract individuals from many disciplines and backgrounds (as long as they relate to the unifying theme of freely shared knowledge work). Examples for conference themes are \"cancer research\", \"the Python programming language\", \"high-energy physics\", or \"the arts in education.\" Conference Committee Nonexisticon is run by a Conference Committee, which is a group of 5 individuals headed by a Conference Director. The Conference Committee serves for the run-up, duration, and wind-down of one Nonexisticon, oversees the appointment of the succeeding Committee, and acts as advisors to the incoming Committee. Would-be Conference Committees prepare a bid for the next Nonexisticon. Bids specify the conference location, dates, venue, capacity, theme, proposed sponsorship opportunities, and budget. If a bid is uncontested, the prior Conference Committee merely assesses the bid for plausibility and compliance with formal criteria, and accepts the bid (thereby also appointing the new Conference Committee). If more than one bid exists, the outgoing Conference Committee organizes an on-line vote among the Association membership, using a preferential voting system. The winning bid then results in the appointment of the new Conference Committee. Presentations Nonexisticon is a single-track conference, with one presentation slot length available: 30 minutes. Nonexisticon typically runs over the course of three days, with talks scheduled between 09:00 (9am) and 18:00 (6pm) local time. This means that after deducting conference opening and closing remarks, keynotes, and breaks, Nonexisticon has 38 talks 2 in total. Q&A time is at the speaker's discretion; forgoing Q&A entirely is acceptable. All presentations are open for rating by attendees for a short time period of 5 minutes prior to, and 15 minutes after scheduled conclusion. Presentations are recorded by a professional A/V team, and publicly released under a permissive license. Presentation Proposals Nonexisticon uses an anonymized call for proposals (CFP), conducted online, using an open-source conference platform. The Conference Committee defines the format of the CFP proper, including the questions posed at submitters. The Conference Committee reviews all submitted proposals, anonymized, for formal compliance with the CFP only. Nonexisticon attendance requires the submission of a presentation proposal. Thus, conference registration and presentation submission are one and the same process. The conference registration fee must be paid in full at the time of registration/submission. Nonexisticon limits proposals to one per speaker/attendee. All presentations are solo; multi-presenter talks and panel discussions are not permitted. 3 Underprivileged Attendee Fund Prospective attendees unable to accommodate the registration fee, conference travel, or accommodation may apply, upon registration, to the Underprivileged Attendee Fund. 4 If accepted (per decision by the Conference Committee), the speaker/attendee is invited to attend the conference free of charge, and their submitted presentation is included in proposal review. The Underprivileged Attendee Fund is endowed by corporate conference sponsorship, donations to the Nonexisticon Association, profits carried over from prior conferences, donations from regular speaker-attendees who voluntarily put up double the regular application fee upon their own registration/proposal submission. Review process and talk selection Talks are selected by all attendees/speakers during a time-limited selection period using a modified Borda count (MBC) ranking process, as pioneered by the scientific community for allocating observation time on astronomical telescopes . In a nutshell, every attendee is assigned a small, randomly selected set of proposals (about 10, and excluding their own submission) to review. They then rank these submissions not in an order of subjectively \"best\" to \"worst\", but from most beneficial to the overall attendee community to least beneficial to the overall attendee community. This results in an overall preliminary ranking of submissions, which is then compared to each attendee's individual ranking. A high degree of agreement of an individual attendee's ranking with the overall preliminary tally results in additional points for the attendee's own proposal; the contrary, in subtracted points. Ultimately, this produces a final, definitive ranking of all received proposals. 5 The entire ranking process is automated using open-source software, and both the preliminary and the final ranking result are publicized to all attendees/submitters. The submitters of the 48 top-ranked presentations (38 plus 10 backup/waitlist presentations) are refunded their registration fee upon acceptance. If an accepted speaker needs to withdraw their talk, the next-ranked talk automatically moves up, and the speaker's registration is simultaneously canceled. 6 Budget and Sponsoring Nonexisticon's budget calls for a barebones conference (infrastructure only, no catering, no childcare) to break even solely on registration fees equivalent to two-thirds of the venue capacity. In case of registrations being in excess of this threshold, Nonexisticon funds childcare, refreshments, and catered lunch, in that order. 7 Nonexisticon is open to sponsoring. Sponsoring, however, does not buy presentation slots, nor does it have any bearing on keynote selection. Sponsors can choose to contribute to infrastructure, catering, childcare, the Underprivileged Attendee Fund, and social events. Of these, social events are the only category open exclusively to sponsor funding; Nonexisticon does not spend registration fee revenue on social events. If registrations do not meet the two-thirds threshold, and the budget shortfall cannot be compensated by sponsor contributions or profits carried over from prior conferences, the conference is cancelled and registration fees refunded. Keynotes Nonexisticon has one keynote, which opens the conference. The Conference Committee extends the keynote speaker invitation by consensus. The closing \"keynote\" is a reprise of the highest-rated presentation in the conference. Conference run-up timeline Nonexisticon's attendee-visible run-up cycle is 6 months. Assuming a Nonexisticon is scheduled to run from May 15-17, the following schedule applies: Date Time to conference Event Nov 15 6 months Conference Committee appointed. Date, location, and sponsorship opportunities announced. Dec 15 5 months Registration period / CFP commences. Jan 15 4 months Registration period / CFP ends, also first sponsorship commitment deadline. Jan 22 3 months, 3 weeks Conference go/no-go call, based on registration and committed sponsorship. Jan 29 3 months, 2 weeks Deadline for rejection of submissions, by the conference committee, on formal grounds. Final decision on Underprivileged Attendee Fund applications. First stage of submission review process (anonymized free-form comments on talk submissions) commences. Feb 15 3 months First stage of submission review process ends, second stage (randomized-subset review and ranking) commences. Mar 1 2 months, 2 weeks Second stage of review ends, final ranking available. Selected speakers for rank 1-38 in final ranking receive notification of acceptance, as do speakers with submissions ranked 39-48 for waitlist/backup talks. Mar 15 2 months Final conference schedule published. Second and final deadline for sponsorships. Disclaimer and acknowledgments I'd like to reiterate that I have no experience whatsoever in running or putting on a conference, since the only time I've contributed to them as something other than a mere speaker, I've sat on proposal selection committees. So take all of what I wrote above with a mountain of salt, and consider it nothing more than semi-elaborate handwaving full of glaring omissions. But if you do want to give me some feedback, even it is simply telling me why my ideas are nuts — as opposed to just that they are — I'd be most grateful. Find me on Twitter or Mastodon . That said, thanks to Tom Eastman for prompting me to put this in writing, to Professor Mike Merrifield for introducing me to the MBC approach , and to Brady Haran for making the Numberphile YouTube channel where I learned about it. Dear grammar stickler, I am acutely aware that a phrase including would + infinitive is not a true subjunctive mood, but the use of a modal verb. Feel free to replace all such instances with a true subjunctive in your head. ↩ Don't bother to check the talk arithmetic. It doesn't matter whether it's really 36 talks or 41 or 42. I just picked 38 as a reasonable, concrete number to work with. ↩ Disallowing multiple submissions from one person, and presentations with multiple speakers, is a necessary consequence of the talk selection process. Allowing only one submission per submitter also has the added benefit that prospective speakers can focus on one single talk and give the proposal their very best shot. ↩ Is there a better name for this? ↩ If you find this summary insufficient to explain the process but also don't feel like plowing through the paper, here's a video explanation , plus additional information . ↩ It is admittedly harsh to only be able to pull out of an accepted talk by pulling out of the conference altogether. I consider this a necessary evil to ensure that no attendee/submitter submit their proposal without genuine intent to present. ↩ Of these, I am most on the fence about childcare. Meaning it would probably be a good idea to always budget for child-care cost, even if that means a higher registration fee for everyone, and thus a slightly elevated risk of conference cancellation. ↩","tags":"blog","url":"blog/2019/04/27/nonexisticon/","loc":"blog/2019/04/27/nonexisticon/"},{"title":"No, really, don't chuck everything in Slack: communications for distributed teams","text":"This is a talk I submitted 1 to linux.conf.au 2019. The conference uses a non-anonymized CfP process on a custom platform that, I think, is built on Symposion . This talk was accepted as a standby talk, meaning it was slated to fill the gap if any other talk had to be cancelled on short notice. I did prepare the full talk, though it did not end up presenting it at the conference. I also submitted the talk to DjangoCon Europe 2019 , where it was rejected. Title No, really, don't chuck everything in Slack: communications for distributed teams Target Audience Business Abstract This will appear in the conference programme. Up to about 500 words. This field is rendered with the monospace font Hack with whitespace preserved This is a personal story. It does not claim to be rooted in statistical analysis or scientific rigour, and the evidence presented is anecdotal. But it might be insightful to anyone joining, leaving, or interacting with a remote team. From 2011 to 2017, I ran a company that had no office. Everyone worked from home, and apart from an annual one-week face to face meeting, all our communications were remote. In 2017, I sold my company and integrated my team into a company that had previously been working exclusively out of a single office. As one would expect, the integration was not without friction (they never are), but what emerged from the experience was a better understanding of the challenges that come with a mixed office/remote work environment, and some rules to address them. In this talk, I'll cover: Typical misconceptions that remoties have about office-workers, and vice versa Using the right tools for the right type of communications: interactive chat, email, wiki, issue trackers, Kanban boards Timezones, and communications around scheduling The 5-paragraph format, a simple tool I habitually use to make sure everyone is on the same page Follow-up and follow-through, and how to make sure neither you, nor your team, nor your boss loses sight of what needs doing Private Abstract This will only be shown to organisers and reviewers. You should provide any details about your proposal that you don't want to be public here. This field is rendered with the monospace font Hack with whitespace preserved It's probably good for me to reiterate that this is not a scientific study. :) If you're curious why this is here, please read this . ↩","tags":"talk-submissions","url":"talk-submissions/lca-2019-slack/","loc":"talk-submissions/lca-2019-slack/"},{"title":"Talk submissions","text":"I have spoken at tech conferences for the better part of a decade, and up to this point I have only ever published my talks after I've actually presented them. I'll change that from here on out, and I'll instead record any talk that I submit to a conference instead, regardless of whether it ultimately gets accepted or not. I do this for several reasons. I'd like to have a record of my talk submissions for my own reference purposes. It's rather remarkable how many conference talk submission systems exist that make it rather difficult to retrieve your submitted abstract a few months or years after the conference. Some event websites exist specifically for one instance of a particular conference, so they might be taken down a few months after, and unaccepted abstracts are usually not accessible publicly — so even getting them via the Internet Archive is not an option. Others record talk submissions via Google Forms, into a private Google Sheet, and don't send an autoreply containing the full submission. So, I use this site for having my own record of talks I submit. I don't want to reinforce the illusion that just because I'm an experienced speaker, all talks I submit anywhere get accepted. I get talks rejected all the time, particularly from conferences I'd really enjoy speaking at. This is normal, and if any of you reading this are less experienced and find rejections discouraging, I want you to know that they happen to all of us. I am curious about other people's thoughts. As a speaker, it is pretty hard to get good feedback on a talk submission. Very few conferences send you detailed feedback on rejected talks. And none at all, to the best of my knowledge, send you qualitative feedback on accepted ones (other than \"congrats, you're in!\"). So I figure that if I publish my submissions here, perhaps a few people might take a look and chip in some valuable thoughts. And even though this site doesn't do comments , I am counting on Twitter and other social networks to spark some. If you run a small conference or meetup that I don't know about, I want to give you the opportunity to reach out to me if there's a topic you'd like to hear me talk about. There's generally far more topics that I'd like to do talks on — and feel reasonably qualified to — than what I often get selected to speak about. In addition, there are just so many conferences and meetups out there that it's impossible to keep track of all of them. So, if you find a topic here that you'd really like at your event that I absolutely don't have on my radar, do drop me a line. If you find any of my talk ideas useful, go ahead and submit your own talk like it. Like almost all content on this site, the talk submissions I record here are CC-BY-SA licensed, so as long as you include an acknowledgment, and reciprocate by sharing your own talk, please do consider yourself encouraged to build your own talk ideas from mine. I'll make one exception from this rule: some conferences use an anonymized talk selection process, where submissions must be devoid of any information that might be remotely likely to identify the speaker. If I submit a talk to such a conference, I'll only put the abstract up here when the selection process is over, the schedule stands, and I have received a definitive acceptance or rejection notice. However, in case I am re-submitting a talk previously given at (or previously submitted to) a different conference, I won't be removing that article. You can find my (continuously updated) list of talk submissions here , and there's also an Atom feed, here .","tags":"blog","url":"blog/2019/04/23/talk-submissions/","loc":"blog/2019/04/23/talk-submissions/"},{"title":"Writing for learners: best practices for creating, developing, and maintaining self-paced learning resources","text":"This is a talk I submitted 1 to Write The Docs Prague, September 15-17, 2019. The conference uses a non-anonymized CfP process with a simple Google Form. The CfP page is here . Talk title Writing for learners: best practices for creating, developing, and maintaining self-paced learning resources. Talk abstract More information here is better. Submitting a single paragraph won't give us much to go on, but please no walls of text. This presentation talks about a special kind of tech writing: creating and maintaining self-paced technical training content. This encompasses both prose for theoretical background information, and instructions for hands-on labs. In this talk, I'll go over special challenges (and advantages!) of self-directed over instructor-driven training video content, and why we don't do it our rules for structuring theoretical content our approach for interactive lab instructions This is rooted in 4 years' experience in writing courseware used for learning complex technical topics (like OpenStack, Kubernetes, Ceph, and others) on an Open edX platform. You'll find the concepts discussed in this talk useful if you write courseware (of course), but I'd say they equally apply whenever you find yourself writing any content that is instructive, rather than descriptive. Who and why Who is this talk for? What background knowledge or experience do you expect the audience to have? What is the take away from the talk? This talk is for anyone who, in a technical context, uses imperatives. I've been at least a part-time tech writer for the better part of the last 10 years, but I've written software documentation for only 4 of those – the majority of the remainder I've written courseware content instead. Writing prose training content and lab instructions comes with its own unique challenges and its own (explicit or implicit) style guide. Since as courseware authors we communicate with our learners exclusively in writing, I believe I have good practices to share with other documentarians who might occasionally find themselves in the situation of writing lab instructions and test scenario descriptions, and I think there are equally many things that can learn from other talks and speakers. Other Information Any other information that might be interesting for us to know about you? Give a lightning talk last year, speak at a WTD meetup, or anything else interesting? Add it here. I've never spoken at any Write The Docs conference, though I have done talks and workshops at multiple instances of linux.conf.au, LinuxCon (now Open Source Summit), OpenStack Summit (now Open Infrastructure Summit), the Open edX Conference, and others. My team and I maintain City Cloud Academy (academy.citycloud.com), which runs on Open edX. If you're curious why this is here, please read this . ↩","tags":"talk-submissions","url":"talk-submissions/wtd-prague-2019/","loc":"talk-submissions/wtd-prague-2019/"},{"title":"If you're a leader in tech, \"non-technical\" is not a free pass","text":"The excellent Josh Simmons recently implored people on Twitter to stop using the term \"non-technical\" when talking about another person's skill set. And as far as that term is often used as a put-down of others, I am completely with Josh. Belittling someone because they work in documentation, corporate leadership, marketing, middle management, PR, advocacy, legal (etc. etc.), and because you consider yourself somehow superior because you're in a \"technical\" role — that has got to stop, yesterday. However, I would like to amend his plea to also decry the use of the phrase about oneself, as a cop-out. I am talking about people in leadership roles saying \"I'm not technical\" or \"I'm not a tech person\" to allege that they have bigger fish to fry, and cannot be reasonably expected to understand technical detail. And that has to stop yesterday, too. Leadership roles exist that require understanding of technical detail. If you're the CEO of a tech company, you need to understand the technology your company makes. If you're in charge of a product, you need to understand the technology that makes up that product. And even if your company's core business has nothing to do with technology, but you are in charge of something that does, you need to understand that technology. Now of course it's nigh impossible to understand every bit of technology that you need to make decisions on, in its every intricate detail. But you will be faced with decisions that do boil down to specific technology details. And then, it is incumbent on you to know as much as you need to know, to make an informed decision. This is a core element of a leadership position, and it makes up at least part of your income differential versus a person who is a subject matter expert in their field, but doesn't manage other people. Whenever you have an issue to decide on that you don't understand, get someone to explain it to you. It's perfectly fine to say \"I know nothing about this bit of technology, please give me your simplest explanation that will enable me to make a decision.\" But you don't get to say \"I'm not a technical person\" and use that as a pass for, and perpetuation of, your self-inflicted ignorance. (By the way, the same is obviously true in reverse. Say you've got the tech-person perspective and someone from legal comes up to you with a question on licensing or patents or international contract law that you know zilch about? Same thing. \"Please give me your simplest explanation of this matter that will enable me to make a decision.\") There is a recently popular spin on the \"it's OK to be non-technical\" cop-out argument, which is to over-emphasize communication skills over tech skills. It starts with a truism — a person's technical skills provide little benefit unless paired with communications proficiency. But then this is frequently flipped to claim that technical understanding can be replaced with communications skills, because the former allegedly pales in importance versus the latter. Let me break something to you: except in the rare case of a job where someone works entirely on their own and also has no customers, 1 communication skills are every person's most important skills. Yes of course your communication skills are more important than your tech skills, because they're literally more important than anything. However, if you're a great communicator but you don't know what you're talking about, all that makes you is a bullshit peddler. And, if you're actually incapable of listening to experts who are able and usually very willing to explain a complex matter to you, maybe you're not such a grand communicator after all, either. So if you're in a leadership position that is even remotely tech-related, and you've ever used \"I'm not technical\" as a free pass to not understand things, stop. It isn't. Yes I am aware that this is exceedingly rare. ↩","tags":"blog","url":"blog/2019/04/21/non-technical/","loc":"blog/2019/04/21/non-technical/"},{"title":"Why upload filters don't work (really simple math!)","text":"\"I can't figure out how upload filters should work, but I'm not a technical person — surely someone who is can sort it out!\" That is a misconception. I'll be happy to explain, requiring — I promise! — no technical understanding of what an upload filter is, or how it works. The current draft of the EU Directive \"on copyright and related rights in the Digital Single Market\", available here , (PDF in English), also known as the EU Copyright Directive , requires in its Article 17 (formerly Article 13), clause 4, that the service provider undergoes, in accordance with high industry standards of professional diligence, best efforts to ensure the unavailability of specific works and other subject matter for which the rightholders have provided the service providers with the relevant and necessary information. It is obvious that the only way any platform hosting user-generated content would thus have to intercept any such content on upload, failing which it would immediately become potentially liable for a copyright violation. This would require a technical facility commonly called an upload filter. We don't need to talk about how upload filters work Now, an upload filter is immensely complex and there are tons of technical difficulties — the only time this has been attempted on a large scale is YouTube's Content ID , and it is exceedingly unreliable and prone to overblocking. But for the purposes of this discussion, it doesn't matter whether implementing an upload filter is difficult to do. 1 Assume for a moment someone has built a magnificent upload filter. Something that operates on magic pixie dust that catches all copyright violations. Interactions Now, let's call every instance of someone uploading content to the internet an \"interaction\". Every tweet, every Facebook post and comment, every comment on your favorite news site, every blog post you write, every picture that you take and post to a WhatsApp group of 50 people or more, every YouTube video and comment — let's call all of those \"interactions.\" And let's make an outrageously overblown assumption: suppose that on the internet today, 1% of such interactions infringe someone's copyright. Again, let me reiterate that this is ludicrously high. The vast majority of internet interactions today are either completely trivial and thus irrelevant to copyright, or works of your own, or a perfectly legal fair-use way of using someone else's work, such as when you quote a passage of a book. But purely for the sake of this discussion, let's say it's 1%. So then let's look at 10,000 interactions that completely random people make on the internet. Those would then break down like so: Total Perfectly legal 9,900 Infringing copyright 100 Catching copyright violations. Or non-violations. OK. Now, suppose we built a perfect upload filter, i.e. one that catches all copyright infringements. Remember, the Directive calls for \"best effort to ensure the unavailability \" (emphasis mine) of potentially infringing content. It does not allow providers to balance for freedom of expression or the like, so to err on the side of caution, they must strive to over- rather than underblock. So a perfect filter is one that has no false negatives — meaning if content infringes, it is always caught. Now, suppose further that the filter mis-identifies content (meaning, flags content as infringing when it is not) with a rate of only 2%. That means it has 2% false positives. That, now, is ridiculously low for any automated screening procedure. 2 So that means that out of our 10,000 interactions tracked by our \"perfect\" content filter, the numbers break down like this: Total Flagged as legal Flagged as infringing Perfectly legal 9,900 9,802 198 Infringing copyright 100 0 100 Overall 10,000 9,802 298 Congratulations, a coin toss beats your upload filter. That leads us to the question: if you upload something and it gets flagged, how likely is it that it is actually infringing any copyright? Answer: 100 in 298. Roughly one in three. Yes, that is worse than a coin toss. And remember, this is assuming an implausibly high rate of infringements overall, and a ludicrously low false-positive and false-negative rate on your filter. Go ahead and play with the numbers, tweak the false-negatives and false-positives, whatever. As long as what you're looking for is exceedingly rare, automated filters detect it with poor accuracy. And if you leave all parameters the same, but consider a probably much more realistic infringement rate of 1 in 1000, that is, 0.1%, then things look like this: Total Flagged as legal Flagged as infringing Perfectly legal 9,990 9,800 200 Infringing copyright 10 0 10 Overall 10,000 9,800 300 Now there's a one-in-thirty chance that an upload block is legitimate. Assuming there is an appeals process, and all false positives get appealed, then that means the human going through the appeals will have to undo a block 29 times out of 30. A cheap optimization I'd like to propose an optimization here: any website seeking to implement a content filter should consider to just use a random number generator to reject your upload, comment, tweet, or post with a certain probability that is demonstrably larger than that of an upload filter. I'd posit that that would be by far the safest, cheapest way to comply with the directive — if it becomes law. Of course, everyone who is now in favor of this directive (including its Article 17) will hate that. Footnotes It's also easy to dismiss with a \"try harder\" retort, which is completely disingenuous, because it's akin to saying, doc, this patient has terminal pancreatic cancer, but you must cure her. Inoperable? Terminal? No there's got to be a way. Sometimes there is no way, and it's OK when an expert tells you that. ↩ I don't believe YouTube releases numbers on its ContentID error rate, but it's apparently pretty bad for a system that cost $100M to build. ↩","tags":"blog","url":"blog/2019/03/25/upload-filter-math/","loc":"blog/2019/03/25/upload-filter-math/"},{"title":"Article 17: The time to act is now.","text":"On Tuesday, March 26 at half-past noon Central European Time, the European Parliament is due to vote on an issue that will definitely impact your life, no matter if you live in or outside the EU. No, it has nothing to do with Brexit. Brexit has just managed to monopolise your attention. The Tuesday vote has much more far-reaching consequences. What's going on here? On Tuesday, the EP will vote on a Directive \"on copyright and related rights in the Digital Single Market\", the draft of which you can look up here (PDF in English), also known as the EU Copyright Directive . Now this thing attempts to be a 21st-century copyright law, which is laudable, but it does absolutely awful things: go take a look at this video to get the quick run-down. It'll only take 4 minutes of your time. And as you'll see from the video, this law will have a devastating global impact on society at large. (While completely failing to achieve its ostensible goals, mind you.) What is apparent from the discussion around this directive is that it isn't being pushed by exceptionally clueful people. One MEP in favor of the directive once publicly surmised that he was dealing with a concerted campaign from Google, because lots of the email in his inbox came from gmail.com addresses . The rapporteur on the directive hasn't subscribed to any YouTube channel and thinks that there is a Memes section on Google , and when properly roasted on Twitter, someone representing his party publicly doubled down on his behalf. In the words of Janus Kopfstein: It's no longer OK to not know how the internet works. Yes, he wrote that in 2011, and addressed at the U.S. Congress, but here we are, with our European representatives still needing that reminder 8 years on. They even tried the oldest trick in the book: once all of Europe started screaming about Article 13, they renumbered. So what used to be Article 13 is now Article 17. Don't get confused; this is just a ploy by people who print out emails. Now luckily, the backers of Article 17 are opposed by a good bunch of people who are clued in, and have pledged to strike this Directive down in the EP on Tuesday. OK. So what do I do? You can sign a change.org petition . It already has 5 million supporters, and there will likely be many more. But supporting that petition is not enough. You can check out your own MEPs for their status at Pledge 2019 . And you can lean on them: Pledge 2019 lets you call the people whose job it is to represent you in the EP, and if they haven't pledged to reject the Directive in its current form, you can let them know (in no uncertain terms) that you won't be voting for them or their party in the upcoming EP elections in May. Also, there's Save Your Internet . The primary spin your pro-upload filter reps are trying to put on the discussion is that the legislation isn't opposed by an appreciable number of real people, and that only astroturfing bots want this law struck down. You can write an email, send a letter (yes, a letter, as in, snail mail), or call them. And finally, Save The Internet (yes, everyone's, not just yours). March 23 is a day of pan-European protest against Article 17. Find a rally and go.","tags":"blog","url":"blog/2019/03/21/copyright-reform/","loc":"blog/2019/03/21/copyright-reform/"},{"title":"Using coverage with multiple parallel GitLab CI jobs","text":"If you ever write unit tests in Python, you are probably familiar with Ned Batchelder 's coverage tool . This article explains how you can use coverage in combination with tox and a GitLab CI pipeline, for coverage reports in your Python code. Running coverage from tox Consider the following rather run-of-the mill tox configuration (nothing very spectacular here): [tox] envlist = py{27,35,36,37},flake8 [coverage:run] parallel = True include = bin/* my_package/*.py tests/*.py [testenv] commands = coverage run -m unittest discover tests {posargs} deps = -rrequirements/setup.txt -rrequirements/test.txt [testenv:flake8] deps = -rrequirements/flake8.txt commands = flake8 {posargs} In this configuration, coverage run (which, remember, replaces python ) invokes test auto-discovery from the unittest module. It looks for unit tests in the tests subdirectory, runs them, and keeps track of which lines were hit and missed by your unit tests. The only slightly unusual bit is parallel = True in the [coverage:run] section. This instructs coverage to write its results not into one file, .coverage , but into multiple, named .coverage.<hostname>.<pid>.<randomnumber> — meaning you get separate results files for each coverage run. Subsequently, you can combine your coverage data with coverage combine , and then do whatever you like with the combined data ( coverage report , coverage html , etc.). GitLab CI Now there's a bit of a difficulty with GitLab CI, which is that your individual tox testenv s will all run in completely different container instances. That means that you'll run your py27 tests in one container, py35 in another, and so forth. But you can use GitLab CI job artifacts to pass your coverage data between one stage and another. Here's your build stage, which stores your coverage data in short-lived artifacts: image : python py27 : image : 'python:2.7' stage : build script : - pip install tox - tox -e py27,flake8 artifacts : paths : - .coverage* expire_in : 5 minutes py35 : image : 'python:3.5' stage : build script : - pip install tox - tox -e py35,flake8 artifacts : paths : - .coverage* expire_in : 5 minutes py36 : image : 'python:3.6' stage : build script : - pip install tox - tox -e py36,flake8 artifacts : paths : - .coverage* expire_in : 5 minutes py37 : image : 'python:3.7' stage : build script : - pip install tox - tox -e py37,flake8 artifacts : paths : - .coverage* expire_in : 5 minutes And here's the test stage, with a single job that combines your coverage data, runs coverage report and parses the output — this is what goes into the coverage column of your GitLab job report, runs coverage html and stores the resulting htmlcov directory into an artifact that you can download from GitLab for a week. coverage : stage : test script : - pip install coverage - python -m coverage combine - python -m coverage html - python -m coverage report coverage : '/TOTAL.*\\s+(\\d+%)$/' artifacts : paths : - htmlcov expire_in : 1 week","tags":"hints-and-kinks","url":"resources/hints-and-kinks/coverage-gitlab-ci/","loc":"resources/hints-and-kinks/coverage-gitlab-ci/"},{"title":"Building a nested CLI parser from a dictionary","text":"If you've ever built a command-line interface in Python, you are surely familiar with the argparse module, which is part of the Python standard library. It contains the ArgumentParser class, instances of which are typically invoked from the CLI's main() method. The canonical way of doing this is explained in considerable detail in the standard library documentation . However, the standard way is quite repetitive, and you end up invoking parser.add_argument() a lot, as you populate your parent parser and subparsers with options. Here's a more concise way: # If you must run this on Python 2. You really shouldn't! from __future__ import print_function from argparse import ArgumentParser import yaml import sys # Using YAML here only for illustrative purposes, as it's a bit # easier to read. You probably just want to use a dictionary outright. # # More at the bottom of this article. # Yes, go read the bottom of this article. # # Want to just blindly copy and paste this snippet? Fine, this is for you. assert ( False ) PARSER_CONFIG_YAML = \"\"\" options: - 'flags': ['-V', '--version'] action: version help: 'show version' version: '0.01' subcommands: - foo: options: - 'flags': ['-c', '--config'] 'help': 'YAML configuration file' dest: config - bar: options: - 'flags': ['-o', '--output'] 'help': 'output file' dest: output - baz: subcommands: - 'spam-eggs': options: - 'flags': ['-i', '--input'] 'help': 'input file' dest: input \"\"\" class CLI (): def __init__ ( self ): def walk_config ( dictionary , parser ): \"\"\"Walk a dictionary and populate an ArgumentParser.\"\"\" if 'options' in dictionary : for opt in dictionary [ 'options' ]: args = opt . pop ( 'flags' ) kwargs = opt parser . add_argument ( * args , ** kwargs ) if 'subcommands' in dictionary : subs = parser . add_subparsers ( dest = 'action' ) for subcommand in dictionary [ 'subcommands' ]: for cmd , opts in subcommand . items (): sub = subs . add_parser ( cmd ) walk_config ( opts , sub ) config = yaml . safe_load ( PARSER_CONFIG_YAML ) parser = ArgumentParser () walk_config ( config , parser ) self . parser = parser def foo ( self , config ): print ( \"This is the foo subcommand, \" \"invoked with '-c %s '.\" % config ) def bar ( self , output ): print ( \"This is the bar subcommand, \" \"invoked with '-o %s '.\" % output ) def baz ( self ): print ( \"This is the baz subcommand\" ) def spam_eggs ( self , input ): print ( \"This is the baz spam-eggs subcommand, \" \"invoked with '-i %s '.\" % input ) def main ( self , argv = sys . argv ): opts = self . parser . parse_args ( argv [ 1 :]) getattr ( self , opts . pop ( 'action' ) . replace ( '-' , '_' ))( ** opts ) if __name__ == '__main__' : CLI () . main () And now, if you want to add a new option, you add it to the top-level or the subcommand's options list, and add it to your subcommand method. And if you want to add a new subcommand, you just add that at the level you like, and add a method that is named like your subcommand — with any hyphens in the subcommand being replaced with underscores in the method name. Notes When using PyYAML, do not use versions affected by CVE-2017-18342 . Really, you shouldn't be using YAML at all for this purpose; you should just use a straight-up dictionary. If you want something just a little more readable, you might also consider JSON (for which there is a parser in the standard library), or perhaps TOML . Also, yes there are smarter ways to define your program's version; more on that perhaps in a later post.","tags":"hints-and-kinks","url":"resources/hints-and-kinks/python-argparse-from-dictionary/","loc":"resources/hints-and-kinks/python-argparse-from-dictionary/"},{"title":"Learn Complex Skills, From Anywhere: Combining Django, Ansible and OpenStack to teach any tech skill","text":"My presentation from linux.conf.au 2019. Video: YouTube , Linux Australia (MP4) , Linux Australia (WebM) Slides (with full speaker notes): GitHub Use the arrow keys to navigate through the presentation, hit Esc to zoom out for an overview, or just advance by hitting the spacebar.","tags":"presentations","url":"resources/presentations/learn-complex-skills-from-anywhere-combining-django-ansible-and-openstack-to-teach-any-tech-skill/","loc":"resources/presentations/learn-complex-skills-from-anywhere-combining-django-ansible-and-openstack-to-teach-any-tech-skill/"},{"title":"1,000 routers per tenant? Think again!","text":"Neutron quotas As with all other OpenStack services, Neutron uses a fairly extensive quota system. An OpenStack admin can give a tenant 1 a quota limit on networks, routers, port, subnets, IPv6 subnetpools, and many other object types. Most OpenStack deployments set the default per-tenant quota at 10 routers. However, nothing stops an admin from setting a much higher router quota, including one above 255. When such a quota change has been applied to your tenant, you're in for a surprise. HA routers Way back in the OpenStack Juno release, we got high-availability support for Neutron routers. This means that, assuming you have more than one network gateway node that can host them, your virtual routers will work in an automated active/backup configuration. In effect, what Neutron does for you is that for every subnet that is plugged into the router — and for which it therefore acts as the default gateway — the gateway address binds to a keepalived-backed VRRP interface. On one of the network nodes that interface is active, and on the others it's in standby. If your network node goes down, keepalived makes sure that the subnets' default gateway IPs come up on the other node. The keepalived configuration is completely abstracted away from the user; the Neutron L3 agent happily takes care of all of it. In addition, in case a network node is up but has lost upstream network connectivity itself, whereas another is still available that retains it, HA routers also fail over in order to ensure connectivity for your VMs. The catch: one HA router network per tenant In order to enable HA routers, Neutron creates one administrative network per tenant, over which it runs VRRP traffic. In order to tell apart all the keepalived instances that it manages on that network, it assigns each an individual Virtual Router ID or VRID. And here's the problem: RFC 5798 defines the VRID to be an 8-bit integer. That means that if you use HA routers, then setting a router quota over 255 is useless — Neutron will run out of VRIDs in the administrative network, before your tenant can ever hit the quota. And this is a hard limit; there's really not much that Neutron can do about this — apart from starting to spin up additional administrative networks once it runs out of VRIDs in the first one, but that likely would be a pretty involved change. Thus, at least for the time being, if you want more than 255 highly-available virtual routers, you'll have to spread them across multiple tenants. What's more is that Neutron is not very forthcoming about this limitation itself: an attempt to create an HA router beyond the limit simply leads to an Unknown error from the Neutron API endpoint. Wait, what if I really don't need HA routers? Well, firstly you probably do want them, really. But that aside, let's assume for a moment that you actually don't. Or rather, that it's more important for you to have more than 255 routers in a single tenant, than for any of them to be highly available. So you create routers with the ha flag set to False , simple, right? It turns out that you probably won't be able to do that. And that's not because you can't change a router's ha flag without first temporarily disabling it — that's not going to hurt you much if you've already decided you don't need HA; in such a case a brief router blip will be acceptable. Instead, it's because (at the time of writing) the default Neutron policy restricts setting the ha flag on a router to admins only. So if you want to be able to disable a router's HA capability, you'll first need to convince your cloud service provider to override the following default entries in Neutron's policy.json : { \"create_router:ha\" : \"rule:admin_only\" , \"get_router:ha\" : \"rule:admin_only\" , \"update_router:ha\" : \"rule:admin_only\" , } … and instead set them as follows: { \"create_router:ha\" : \"rule:admin_or_owner\" , \"get_router:ha\" : \"rule:admin_or_owner\" , \"update_router:ha\" : \"rule:admin_or_owner\" , } If your cloud service provider deploys Neutron with OpenStack-Ansible , they can define this in the following variable : neutron_policy_overrides : \"create_router:ha\" : \"rule:admin_or_owner\" \"get_router:ha\" : \"rule:admin_or_owner\" \"update_router:ha\" : \"rule:admin_or_owner\" Once the policy has been overridden in this manner, you should be able to create a new router with: openstack router create --no-ha <name> And modify an existing router's high-availability flag with: openstack router set --disable <name> openstack router set --no-ha <name> openstack router set --enable <name> Is my router HA, really? In relation to what I described above, you may want to find out whether one of your routers is configured to be highly available in the first place. You'd expect to easily be able to do this with an openstack router show command: Alas, what you see in the example above is indeed a highly-available router, so why does it clearly report its ha flag as being False ? Well, that's another consequence of that default Neutron policy, in combination with rather unintuitive behavior by the openstack command line client. You see, this part of the aforementioned policy { \"get_router:ha\" : \"rule:admin_only\" , } … means you're not even allowed to query the ha flag if you're not an admin, and when the openstack client is asked to display a boolean value that the user is not allowed to even read, then it always displays False . I'm very sorry, I still can't force myself to call a tenant it a \"project\", as I find that term profoundly illogical: the proper term for the concept being discussed here is multitenancy, not multiprojectcy. ↩","tags":"hints-and-kinks","url":"resources/hints-and-kinks/1000-routers-per-tenant-think-again/","loc":"resources/hints-and-kinks/1000-routers-per-tenant-think-again/"},{"title":"The Little Bag O'Tricks: 10 Things You Might Not Know You Can Do With OpenStack","text":"My presentation from OpenStack Days Nordic 2018 in Stockholm. Actually talks about 11, not 10 OpenStack capabilities you might not know about. Video: YouTube Slides: GitHub Use the arrow keys to navigate through the presentation, hit Esc to zoom out for an overview, or just advance by hitting the spacebar.","tags":"presentations","url":"resources/presentations/the-little-bag-otricks-10-things-you-might-not-know-you-can-do-with-openstack/","loc":"resources/presentations/the-little-bag-otricks-10-things-you-might-not-know-you-can-do-with-openstack/"},{"title":"Working from home, with little kids in the house","text":"I've worked exclusively from home 1 for the last 6½ years, and when I first started, my older kids were 7 and 6 years old. For the last 3 years after the arrival of our two younger children, though, I've worked from home with infants/toddlers in the house, and that's a wholly different ball game. The basic rules of working from home If you've dealt with the idea of working from home, or have done it before, then you're surely familiar with a few basic rules to follow: Make sure that you have a home office, that is a working space to yourself that allows you to close a door and be undisturbed when you need it. Set aside some time to go outside. Office workers have a commute that ensures they get out of the house; as a home worker you're running a certain risk of turning into a hermit. Make sure you run errands, take walks, go for runs. Keep tabs on your working hours. Home workers are frequently at risk of working too much or too long, which puts a strain on yourself, your significant other, and your family. Maintain relationships with others who work from home. Chat with them, call them, invite them over for dinner if they live close by. Working from home is still not a common thing to do in general (however common it might be in the tech industry), so exchanging thoughts with people in the same boat is a good thing. Be ready to try out working-from-home strategies that have worked for others. Fortunately for all of us, there are quite a few people who talk about the experience publicly — for example, John Dalton did so in a 45-minute talk at linux.conf.au 2018. Now, the challenging part: little kids If anyone reading this wants to pre-emptively freak out and berate me for being an irresponsible or unloving parent, you are politely asked to chill and if incapable of doing so, to GTFO. I absolutely love being around my children, and I wouldn't even think to trade my job for an office-dwelling one. Being able to work from home is generally a wonderful privilege, and so is being around my kids. You need to be aware of two things: one, working from home with a family is different from working from home when you're alone or living with an adult partner or roomie. Two, working from home with a family with little children is very different from a family with school-age kids. School-age children don't permanently need an adult in the room, they spend a significant amount of their time outside the house, they're usually not particularly prone to tantrums, and in the event that you really, really need some quiet, they can be asked to give you that for a limited amount of time. Kids under two? Good luck with that. If you're not striking a balance here, here's what's likely to happen: you'll think that you are a completely inadequate parent, or a terrible employee, or both. So, never lose sight of the fact that you're probably a wonderful parent and a highly productive employee — but that you can do anything, but not everything. 2 My most important piece of advice here is that you should not attempt to simultaneously be a primary parent and an employee. (Please note the emphasis on \"simultaneously.\") I think it's fundamentally impossible to be a mindful parent of your kids, and at the same time a productive knowledge worker. And the repeated attempt will lead to frustration and burnout. Of course, you can split your day in half with your significant other, if for example you happen to be a night owl and they are not, and you can put in a full day of work between 1pm and 9pm (but do ensure you get enough sleep, which is probably a topic for another post). Or you could rise early and start working at 6 while your other half drops your kids off at daycare, from whence you pick them up at 2. But whatever it is that you decide, please don't fool yourself into thinking that you can work productively, and be a good parent for your kids at the very same time. Productivity tips during work hours So, we've established that work time and parenting time shouldn't overlap. So what can you do while you're at work, and your kids are in the house (under the supervision of your spouse or partner, or other trusted guardian)? Spoiler: \"please, kids, be quiet for a bit\" isn't going to work. Not with toddlers. And sound-proofing your room is quite expensive, and also remarkably ineffective for the deep bass whooomp of an exuberant kid jumping off a couch upstairs. I find myself being able to focus extremely well if I play pink noise through my headphones. 3 It drowns out practically all background noise, and is uniform enough that my brain eventually drops it as an input, and I barely notice it. I also habitually close my door when I need to work. Obviously, my family knows that they can call on me when something urgent comes up, but little kids rightfully take an open door as an invitation. And as much as I would like to be able to tolerate interruptions of my thinking flow to play with a little one for a few minutes, and then immediately pick up where I left off, I am not. 4 So, this is a classic trade-off: I can either enjoy the interaction with my kids and then deal with the frustration that comes with not getting things done, or I can get things done and regret that I didn't spend more time with my kids. There's no right answer. The wrong answer that I choose is that my door stays closed. Striking a balance (or trying to) There are a few things I do to try and balance my in-house family absence (I'm dead serious, that's what working from home is): When I'm home, I don't miss lunch. 5 I can't say that I've never missed lunch because I have done so when a customer's or colleague's environment had just reached the HCF instruction , but I won't miss lunch for a meeting that can also be scheduled to an hour earlier or later. Also, tucking the little ones in is my job every night. I don't travel when it would mean being absent for a child's birthday. Which is actually not that easy to do when you're on the conference circuit, travel about 100 days per year, and you have a big family — thus far, if my math is right I've successfully navigated 28 kids' birthdays. 6 Am I succeeding? Let's be real: I have no way of knowing. Whether I'm being a decent parent will ultimately be judged by my children when they've grown up. Whether I've had a successful career is something I'll decide upon my retirement. But hey, such is life. And thus far it looks like I don't suck at it. I hope. 🙂 Footnotes This article originally appeared on my blog on fghaas.github.io (now defunct). Well, not just from home. Throughout the last few years I've traveled rather extensively, so I've also worked from planes, airport lounges, trains, hotel rooms, customer offices, cafes and parks. What I mean is that I haven't had to go to an office on a daily basis, since 2011. ↩ Hat tip to David Allen , whose Getting Things Done is not the ultimate fount of all self-management wisdom, but definitely a good source to have read at least once. ↩ On a Linux box with SoX , you can generate an Ogg Vorbis file containing 25 minutes of pink noise with sox -n pinknoise.ogg synth 25:00 pinknoise — the 25-minute length may come in handy if you're using Pomodoro . As with music, I play my pink noise from Audacious with the MPRIS -2 plugin enabled, which allows me to pause and play from gnome-shell with the Media Player Indicator extension. ↩ I do not consider myself a programmer. But the issue described in the cartoon applies to any knowledge worker. In fact, it probably applies to any worker, it's just that I've been doing knowledge work for the entirety of my career, and can't comment authoritatively on anything else. ↩ For those of you who grew up outside central Europe: over here, lunch (not dinner) is traditionally considered the \"big\" meal of the day. ↩ I also took uninterrupted travel breaks of 6 and 10 weeks, respectively, when our younger kids were born. (When the older ones arrived, I was traveling way less overall, so those travel schedule adjustments were easy at the time.) ↩","tags":"blog","url":"blog/2018/02/18/working-from-home-with-little-kids/","loc":"blog/2018/02/18/working-from-home-with-little-kids/"},{"title":"More recommendations for Ceph and OpenStack","text":"A few months ago, we shared our Dos and Don'ts , as they relate to Ceph and OpenStack. Since that post has proved quite popular, here are a few additional considerations for your Ceph-backed OpenStack cluster. Do configure your images for VirtIO-SCSI By default, RBD-backed Nova instances use the virtio-blk driver to expose RBD images to the guest – either as ephemeral drives, or as persistent volumes. In this default configuration, VirtIO presents a virtual PCI device to the guest that represents the paravirtual I/O bus, and devices are named /dev/vda , /dev/vdb , and so forth. VirtIO block devices are lightweight and efficient, but they come with a drawback: they don't support the discard operation. Not being able to use discard means the guest cannot mount a filesystem with mount -o discard , and it also cannot clean up freed blocks on a filesystem with fstrim . This can be a security concern for your users, who might want to be able to really, actually delete data from within the guest (after overwriting it, presumably). It can also be an operational concern for you as the cluster operator. This is because not supporting discard also means that RADOS objects owned by the corresponding RBD image and never removed during the image's lifetime – they persist until the whole image is deleted. So your cluster may carry the overhead of perhaps tens of thousands of RADOS objects that no-one actually cares about. Thankfully, there is an alternative VirtIO disk driver that does support discard : the paravirtualized VirtIO SCSI controller, virtio-scsi . Enabling the VirtIO SCSI controller is something you do by setting a couple of Glance image properties, namely hw_scsi_model and hw_disk_bus . You do so by running the following openstack commands on your image: openstack image set \\ --property hw_scsi_model = virtio-scsi \\ --property hw_disk_bus = scsi \\ <name or ID of your image> Then, if you boot an instance from this image, you'll see that its block device names switch from /dev/vdX to /dev/sdX , and you also get everything else you expect from a SCSI stack. For example, there's /proc/scsi/scsi , you can extract information about your bus, controller, and LUs with lsscsi command, and so on. It's important to note that this image property is inherited by the instance booted from that image, which also passes it on to all volumes that you may subsequently attach to that instance. Thus, openstack server add volume will now add /dev/sdb , not /dev/vdb , and you will automatically get the benefits of discard on your volumes, as well. Do set disk I/O limits on your Nova flavors In a Ceph cluster that acts as backing storage for OpenStack, naturally many OpenStack VMs share the bandwidth and IOPS of your whole cluster. When that happens, occasionally you may have a VM that's very busy (meaning it produces a lot of I/O), which the Ceph cluster will attempt to process to the best of its abilities. In doing so, since RBD has no built-in QoS guarantees ( yet ), it might cause other VMs to suffer from reduced throughput, increased latency, or both. The trouble with this is that it's almost impossible for your users to calculate and reckon with. They'll see a VM that sustains, say, 10,000 IOPS at times, and then drop to 2,000 with no warning or explanation. It is much smarter to pre-emptively limit Ceph RBD performance from the hypervisor, and luckily, OpenStack Nova absolutely allows you to do that. This concept is known as instance resource quotas , and you set them via flavor properties. For example, an you may want to limit a specific flavor to 1,500 IOPS and a maximum throughput of 100 MB/s: openstack flavor set \\ --property quota:disk_total_bytes_sec = $(( 100 << 20 )) --property quota:disk_total_iops_sec = 1500 m1.medium In the background, these settings are handed through to libvirt and ultimately fed into cgroup limitations for Qemu/KVM, when a VM with this flavor spins up. So these limits aren't specific to RBD, but they come in particularly handy when dealing with RBD. Obviously, since flavors can be public, but can also be limited to specific tenants, you can set relatively low instance resource quotas in public flavors, and then make flavors with higher resource quotas available to select tenants only. Do differentiate Cinder volume types by disk I/O limits In addition to setting I/O limits on flavors for VMs, you can also influence the I/O characteristics of volumes. You do so by specifying distinct Cinder volume types . Volume types are frequently used to enable users to select a specific Cinder backend — say, to stick volumes either on a NetApp box or on RBD, but it's perfectly OK if you define multiple volume types using the same backend. You can then set characteristics like maximum IOPS or maximum throughput via Cinder QoS specifications. A QoS specification akin to the Nova flavor decribed above — limiting throughput to 100 MB/s and 1,500 IOPS would be created like this: openstack volume qos create \\ --consumer front-end --property total_bytes_sec = $(( 100 << 20 )) \\ --property total_iops_sec = 1500 \\ \"100MB/s-1500iops\" You would then create a corresponding volume type, and associate the QoS spec with it: openstack volume type create \\ --public \\ \"100MB/s-1500iops\" openstack volume qos associate \\ \"100MB/s-1500iops\" \\ \"100MB/s-1500iops\" Again, as with Nova flavors, you can make volume types public, but you can also limit them to specific tenants. Don't forget about suspend files When you suspend a Nova/libvirt/KVM instance, what really happens is what libvirt calls a managed save : the instance's entire memory is written to a file, and then KVM process shuts down. This is actually quite neat because it means that the VM does not consume any CPU cycles nor memory until it restarts, and it will continue right where it left off, even if the compute node is rebooted in the interim. You should understand that these savefiles are not compressed in any way: if your instance has 16GB of RAM, that's a 16GB file that instance suspension drops into /var/lib/nova/save . This can add up pretty quickly: if a single compute node hosts something like 10 suspended instances, their combined save file size can easily exceed 100 GB. Obviously, this can put you in a really bad spot if this fills up your /var (or worse, / ) filesystem. Of course, if you already have a Ceph cluster, you can put it to good use here too: just deep-mount a CephFS file system into that spot. Here's an Ansible playbook snippet that you may use as inspiration: --- - hosts : - compute-nodes vars : ceph_mons : - ceph-mon01 - ceph-mon02 - ceph-mon03 cephfs_client : cephfs cephfs_secret : \"{{ vaulted_cephfs_secret }}\" tasks : - name : \"install ceph-fs-common package\" apt : name : ceph-fs-common state : installed - name : \"create ceph directory\" file : dest : /etc/ceph owner : root group : root mode : '0755' state : directory - name : \"create cephfs secretfile\" copy : dest : /etc/ceph/cephfs.secret owner : root group : root mode : '0600' content : '{{ cephfs_secret }}' - name : \"mount savefile directory\" mount : fstype : ceph path : /var/lib/nova/save src : \"{{ ceph_mons | join(',') }}:/nova/save/{{ ansible_hostname }}\" opts : \"name={{ cephfs_client }},secretfile=/etc/ceph/cephfs.secret\" state : mounted - name : \"fix savefile directory ownership\" file : path : /var/lib/nova/save owner : libvirt-qemu group : kvm state : directory Got more? Do you have Ceph/OpenStack hints of your own? Leave them in the comments below and we'll include them in the next installment. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/more-recommendations-ceph-openstack/","loc":"resources/hints-and-kinks/more-recommendations-ceph-openstack/"},{"title":"Importing an existing Ceph RBD image into Glance","text":"The normal process of uploading an image into Glance is straightforward: you use glance image-create or openstack image create , or the Horizon dashboard. Whichever process you choose, you select a local file, which you upload into the Glance image store. This process can be unpleasantly time-consuming when your Glance service is backed with Ceph RBD, for a practical reason. When using the rbd image store, you're expected to use raw images, which have interesting characteristics. Raw images and sparse files Most people will take an existing vendor cloud image, which is typically available in the qcow2 format, and convert it using the qemu-img utility, like so: $ wget -O ubuntu-xenial.qcow2 \\ https://cloud-images.ubuntu.com/xenial/current/xenial-server-cloudimg-amd64-disk1.img $ qemu-img convert -p -f qcow2 -O raw ubuntu-xenial.qcow2 ubuntu-xenial.raw On face value, the result looks innocuous enough: $ qemu-img info ubuntu-xenial.qcow2 image: ubuntu-xenial.qcow2 file format: qcow2 virtual size: 2 .2G ( 2361393152 bytes ) disk size: 308M cluster_size: 65536 Format specific information: compat: 0 .10 refcount bits: 16 $ qemu-img info ubuntu-xenial.raw image: ubuntu-xenial.raw file format: raw virtual size: 2 .2G ( 2361393152 bytes ) disk size: 1000M As you can see, in both cases the virtual image size differs starkly from the actual file size. In qcow2 , this is due to the copy-on-write nature of the file format and zlib compression; for the raw image, we're dealing with a sparse file: $ ls -lh ubuntu-xenial.qcow2 -rw-rw-r-- 1 florian florian 308M Feb 17 10 :05 ubuntu-xenial.qcow2 $ du -h ubuntu-xenial.qcow2 308M ubuntu-xenial.qcow2 $ ls -lh info ubuntu-xenial.raw -rw-r--r-- 1 florian florian 2 .2G Feb 17 10 :16 ubuntu-xenial.raw $ du -h ubuntu-xenial.raw 1000M ubuntu-xenial.raw So, while the qcow2 file's physical and logical sizes match, the raw file looks much larger in terms of filesystem metadata, as opposed to its actual storage utilization. That's because in a sparse file, \"holes\" (essentially, sequences of null bytes) aren't actually written to the filesystem. Instead, the filesystems just records the position and length of each \"hole\", and when we read from the \"holes\" in the file, the read would just return null bytes again. The trouble with sparse files is that RESTful web services, like Glance, don't know too much about them. So, if we were to import that raw file with openstack image-create --file my_cloud_image.raw , the command line client would upload null bytes with happy abandon, which would greatly lengthen the process. Importing images into RBD with qemu-img convert Luckily for us, qemu-img also allows us to upload directly into RBD. All you need to do is make sure the image goes into the correct pool, and is reasonably named. Glance names uploaded images by their image ID, which is a universally unique identifier (UUID), so let's follow Glance's precedent. export IMAGE_ID = ` uuidgen ` export POOL = \"glance-images\" # replace with your Glance pool name qemu-img convert \\ -f qcow2 -O raw \\ my_cloud_image.raw \\ rbd: $POOL / $IMAGE_ID Creating the clone baseline snapshot Glance expects a snapshot named snap to exist on any image that is subsequently cloned by Cinder or Nova, so let's create that as well: rbd snap create $POOL / $IMAGE_ID @snap rbd snap protect $POOL / $IMAGE_ID @snap Making Glance aware of the image Finally, we can let Glance know about this image. Now, there's a catch to this: this trick only works with the Glance v1 API, and thus you must use the glance client to do it. Your Glance is v2 only? Sorry. Insist on using the openstack client? Out of luck. What's special about this invocation of the glance client are simply the pre-populated location and id fields. The location is composed of the following segments: the fixed string rbd:// , your Ceph cluster UUID (you get this from ceph fsid ), a forward slash ( / ), the name of the pool that the image is stored in, the name of your image (which you previously created with uuidgen ), another forward slash ( / , not @ as you might expect), and finally, the name of your snapshot ( snap ). Other than that, the glance client invocation is pretty straightforward for a v1 API call: CLUSTER_ID = ` ceph fsid ` glance --os-image-api-version 1 \\ image-create \\ --disk-format raw \\ --id $IMAGE_ID \\ --location rbd:// $CLUSTER_ID / $POOL / $IMAGE_ID /snap Of course, you might add other options, like --private or --protected or --name , but the above options are the bare minimum. And that's it! Now you can happily fire up VMs, or clone your image into a volume and fire a VM up from that. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/importing-rbd-into-glance/","loc":"resources/hints-and-kinks/importing-rbd-into-glance/"},{"title":"Replacing the built-in Open edX forum with a suitable alternative","text":"Open edX comes with a built-in discussion forum service. Many Open edX users find this service less than optimal: it is the only edX service to require Ruby, it depends on a Ruby version that is outdated and no longer receives security updates (although a fix for that is on the way), it and generally feels like overkill to many users. Thankfully, since the Open edX Eucalyptus release it's been quite easy to replace the course forum with an alternative. Here at hastexo, we're fans of Disqus (you may have noticed we also use it around out web site), so let's see what we can do to drop the Open edX Forum and replace it with Disqus. Step 1: Locate your course's policy.json file If you keep your course materials in Git or some other version-controlled repository, you'll already be familiar with the directory structure of an OLX course tree. If you're not, just use edX Studio to export your course into a compressed archive, download it, and extract it on your local machine. Locate the policies/_base directory. Find the policy.json file located therein. It might look like this: { \"course/201702\" : { \"language\" : \"en\" , \"invitation_only\" : true , \"start\" : \"2017-02-01T00:00:00Z\" , \"advertised_start\" : \"2017-02-01T00:00:00Z\" , \"end\" : \"2017-02-28T23:59:59Z\" , \"is_new\" : true , \"catalog_visibility\" : \"both\" , \"max_student_enrollments_allowed\" : null , \"due\" : null , \"giturl\" : null , \"course_image\" : \"images_course_image.jpg\" , \"advanced_modules\" : [ \"hastexo\" ], \"hide_from_toc\" : false , \"ispublic\" : false , \"rerandomize\" : \"never\" , \"show_calculator\" : false , \"showanswer\" : \"attempted\" , \"days_early_for_beta\" : null , \"discussion_topics\" : { \"General\" : { \"id\" : \"i4x-hastexo-hx212-course-201702\" } }, \"tabs\" : [ { \"name\" : \"Courseware\" , \"type\" : \"courseware\" }, { \"name\" : \"Course Info\" , \"type\" : \"course_info\" }, { \"name\" : \"Textbooks\" , \"type\" : \"textbooks\" }, { \"name\" : \"Discussion\" , \"type\" : \"discussion\" }, { \"name\" : \"Wiki\" , \"type\" : \"wiki\" }, { \"name\" : \"Progress\" , \"type\" : \"progress\" } ] } } Note the tabs list. It contains the list of course tabs ( which edX Studio, confusingly, calls \"pages\" ). Step 2: Remove the default Discussion tab You can now edit policy.json , and drop the Discussion entry from the tabs list, to make it look like so: \"tabs\" : [ { \"name\" : \"Courseware\" , \"type\" : \"courseware\" }, { \"name\" : \"Course Info\" , \"type\" : \"course_info\" }, { \"name\" : \"Textbooks\" , \"type\" : \"textbooks\" }, { \"name\" : \"Wiki\" , \"type\" : \"wiki\" }, { \"name\" : \"Progress\" , \"type\" : \"progress\" } ] Maybe you also want to remove the course wiki. Just keep whichever tabs you'd like to keep. Step 3: Add a \"static\" tab In place of the old Discussion tab (which, you may have noticed, was of a special type conspicuously named discussion ), you can now put a tab of different, simpler type: static_tab . Like so: \"tabs\" : [ { \"name\" : \"Courseware\" , \"type\" : \"courseware\" }, { \"name\" : \"Course Info\" , \"type\" : \"course_info\" }, { \"name\" : \"Textbooks\" , \"type\" : \"textbooks\" }, { \"name\" : \"Discussion\" , \"type\" : \"static_tab\" , \"url_slug\" : \"discussion\" }, { \"name\" : \"Wiki\" , \"type\" : \"wiki\" }, { \"name\" : \"Progress\" , \"type\" : \"progress\" } ] Note that a static_tab type tab also requires a value url_slug . What's that one about, you ask? Step 4: add static content Whatever you put into url_slug tells Open edX to go look into the tabs subdirectory of your course root, and find a properly named file there. In our case, that file needs to be named discussion.html , because we defined \"url_slug\": \"discussion\" . So, head over to Disqus and grab the generated code from there, and then stick it into tabs/discussion.html . Something like this: < div id = \"disqus_thread\" ></ div > < script > // <![CDATA[ ( function () { // DON'T EDIT BELOW THIS LINE var d = document , s = d . createElement ( 'script' ); s . src = '//<your Disqus site domain name>/embed.js' ; s . setAttribute ( 'data-timestamp' , + new Date ()); ( d . head || d . body ). appendChild ( s ); })(); // ]]> </ script > < noscript > Please enable JavaScript to view the < a href = \"https://disqus.com/?ref_noscript\" > comments powered by Disqus. </ a ></ noscript > < p ></ p > Step 5: deploy Re-compress your tarball, upload to Studio or run manage.py import , and you're done! This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/replace-edx-forum/","loc":"resources/hints-and-kinks/replace-edx-forum/"},{"title":"The Dos and Don'ts for Ceph for OpenStack","text":"Ceph and OpenStack are an extremely useful and highly popular combination. Still, new Ceph/OpenStack deployments frequently come with easily avoided shortcomings — we'll help you fix them! Do use show_image_direct_url and the Glance v2 API With Ceph RBD (RADOS Block Device), you have the ability to create clones. You can think of clones as the writable siblings of snapshots (which are read-only). A clone creates RADOS objects only for those parts of your block device which have been modified relative to its parent snapshot, and this means two things: You save space. That's a no-brainer, but in and of itself it's not a very compelling argument as storage space is one of the cheapest things in a distributed system. What's not been modified in the clone can be served from the original volume. This is important because, of course, it means you are effectively hitting the same RADOS objects — and thus, the same OSDs — no matter which clone you're talking to. And that, in turn, means, those objects are likely to be served from the respective OSD's page caches, in other words, from RAM. RAM is way faster to access than any persistent storage device, so being able to serve lots of reads from the page cache is good. That, in turn, means, that serving data from a clone will be faster than serving the same data from a full copy of a volume. Both Cinder (when creating a volume from an image) and Nova (when serving ephemeral disks from Ceph) will make use of cloning RBD images in the Ceph backend, and will do so automatically. But they will do so only if show_image_direct_url=true is set in glance‑api.conf , and they are configured to connect to Glance using the Glance v2 API. So do both. Do set libvirt/images_type = rbd on Nova compute nodes In Nova (using the libvirt compute driver with KVM), you have several options of storing ephemeral disk images, that is, storage for any VM that is not booted from a Cinder volume. You do so by setting the images_type option in the [libvirt] section in nova‑compute.conf : [libvirt] images_type = <type> The default type is disk , which means that when you fire up a new VM, the following events occur: nova‑compute on your hypervisor node connects to the Glance API, looks up the desired image, and downloads the image to your compute node (into the /var/lib/nova/instances/_base directory by default). It then creates a new qcow2 file which uses the downloaded image as its backing file. This process uses up a fair amount of space on your compute nodes, and can quite seriously delay spawning a new VM if it has been scheduled to a host that hasn't downloaded the desired image before. It also makes it impossible for such a VM to be live-migrated to another host without downtime. Flipping images_type to rbd means the disk lives in the RBD backend, as an RBD clone of the original image, and can be created instantaneously. No delay on boot, no wasting space, all the benefits of clones. Use it. Do enable RBD caching on Nova compute nodes librbd , the library that underpins the Qemu/KVM RBD storage driver, can enable a disk cache that uses the hypervisor host's RAM for caching purposes. You should use this. Yes, it's a cache that is safe to use. On the one hand, the combination of virtio-blk with the Qemu RBD storage driver will properly honor disk flushes. That is to say, when an application inside your VM says \"I want this data on disk now,\" then virtio‑blk , Qemu, and Ceph will all work together to only report the write as complete when it has been written to the primary OSD, replicated to the available replica OSDs, acknowledged to have hit at least the persistent journal on all OSDs. In addition, Ceph RBD has an intelligent safeguard in place: even if it is configured to cache in write-back mode, it will refuse to do so (meaning, it will operate in write-through mode) until it has received the first flush request from its user. Thus, if you run a VM that just never does that — because it has been misconfigured or its guest OS is just ages old — then RBD will stubbornly refuse to cache any writes. The corresponding RBD option is called rbd cache writethrough until flush , it defaults to true and you should never disable it. You can enable writeback caching for Ceph by setting the following nova-compute configuration option: [libvirt] images_type = rbd ... disk_cachemodes = \"network=writeback\" And you just should. Do use separate pools for images, volumes, and ephemeral disks Now that you have enabled show_image_direct_url=true in Glance, configured Cinder and nova-compute to talk to Glance using the v2 API, and configured nova-compute with libvirt/images_type=rbd , all your VMs and volumes will be using RBD clones. Clones can span multiple RADOS pools, meaning you can have an RBD image (and its snapshots) in one pool, and its clones in another. You should do exactly that, for several reasons: Separate pools means you can lock down access to those pools separately. This is just a standard threat mitigation approach: if your nova-compute node gets compromised and the attacker can corrupt or delete ephemeral disks, then that's bad — but it would be worse if they could also corrupt your Glance images. Separate pools also means that you can have different pool settings, such as the settings for size or pg_num . Most importantly, separate pools can use separate crush_ruleset settings. We'll get back to this in a second, it'll come in handy shortly. It's common to have three different pools: one for your Glance images (usually named glance or images ), one for your Cinder volumes ( cinder or volumes ), and one for your VMs ( nova-compute or vms ). Don't necessarily use SSDs for your Ceph OSD journals Of the recommendations in this article, this one will probably be the one that surprises the most people. Of course, conventional wisdom holds that you should always put your OSD journals on fast OSDs, and that you should deploy SSDs and spinners in a 1:4 to 1:6 ratio, right? Let's take a look. Suppose you're following the 1:6 approach, and your SATA spinners are capable of writing at 100 MB/s. 6 spinners make 6 OSDs, and each OSD uses a journal device that's on a partition on an enterprise SSD. Suppose further that the SSD is capable of writing at 500 MB/s. Congratulations, in that scenario you've just made your SSD your bottleneck. While you would be able to hit your OSDs at 600 MB/s on aggregate, your SSD limits you to about 83% of that. In that scenario you would actually be fine with a 1:4 ratio, but make your spindles just a little faster and the SSD advantage goes out the window again. Now, of course, do consider the alternative: if you're putting your journals on the same drive as your OSD filestores, then you effectively get only half the nominal bandwidth of your drive, on average, because you write everything twice, to the same device. So that means that without SSDs, your effective spinner bandwidth is only about 50 MB/s, so the total bandwidth you get out of 6 drives that way is more like 300 MB/s, against which 500 MB/s is still a substantial improvement. So you will need to plug your own numbers into this, and make your own evaluation for price and performance. Just don't assume that journal SSD will be a panacea, or that it's always a good idea to use them. Do create all-flash OSDs One thing your journal SSDs don't help with are reads. So, what can you do to take advantage of SSDs on reads, too? Make them OSDs. That is, not OSD journals, but actual OSDs with a filestore and journal. What this will create are OSDs that don't just write fast, but read fast, too. Do put your all-flash OSDs into a separate CRUSH root Assuming you don't run on all-flash hardware, but operate a cost-effective mixed cluster where some OSDs are spinners and others are SSDs (or NVMe devices or whatever), you obviously want to treat those OSDs separately. The simplest and easiest way to do that is to create a separate CRUSH root in addition to the normally configured default root. For example, you could set up your CRUSH hierarchy as follows: ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY - -1 4.85994 root default -2 1.61998 host elk 0 0.53999 osd.0 up 1.00000 1.00000 1 0.53999 osd.1 up 1.00000 1.00000 2 0.53999 osd.2 up 1.00000 1.00000 -3 1.61998 host moose 3 0.53999 osd.3 up 1.00000 1.00000 4 0.53999 osd.4 up 1.00000 1.00000 5 0.53999 osd.5 up 1.00000 1.00000 -4 1.61998 host reindeer 6 0.53999 osd.6 up 1.00000 1.00000 7 0.53999 osd.7 up 1.00000 1.00000 8 0.53999 osd.8 up 1.00000 1.00000 -5 4.85994 root highperf -6 1.61998 host elk-ssd 9 0.53999 osd.9 up 1.00000 1.00000 10 0.53999 osd.10 up 1.00000 1.00000 11 0.53999 osd.11 up 1.00000 1.00000 -7 1.61998 host moose-ssd 12 0.53999 osd.12 up 1.00000 1.00000 13 0.53999 osd.13 up 1.00000 1.00000 14 0.53999 osd.14 up 1.00000 1.00000 -8 1.61998 host reindeer-ssd 15 0.53999 osd.15 up 1.00000 1.00000 16 0.53999 osd.16 up 1.00000 1.00000 17 0.53999 osd.17 up 1.00000 1.00000 In the example above, OSDs 0-8 are assigned to the default root, whereas OSDs 9-17 (our SSDs) belong to the root highperf . We can now create two separate CRUSH rulesets: rule replicated_ruleset { ruleset 0 type replicated min_size 1 max_size 10 step take default step chooseleaf firstn 0 type host step emit } rule highperf_ruleset { ruleset 1 type replicated min_size 1 max_size 10 step take highperf step chooseleaf firstn 0 type host step emit } The default ruleset, replicated_ruleset , picks OSDs from the default root, whereas step take highperf in highperf_ruleset means it covers only OSDs in the highperf root. Do assign individual pools to your all-flash ruleset Assigning individual pools to a new CRUSH ruleset (and hence, to a whole different set of OSDs) is a matter of issuing a single command: ceph osd pool set <name> crush_ruleset <number> … where <name> name of your pool and <number> is the numerical ID of your ruleset as per your CRUSH map. You can do this while the pool is online, and while clients are accessing its data — although of course, there will be a lot of remapping and backfilling so your overall performance may be affected somewhat. Now, the assumption is that you will have more spinner storage than SSD storage. Thus, you will want to select individual pools for your all-flash OSDs. Here are a handful of pools that might come in handy as first candidates to migrate to all-flash. You can interpret the list below as a priority list: as you add more SSD capacity to your cluster, you can move pools over to all-flash storage one by one. Nova ephemeral RBD pools ( vms , nova-compute ) radosgw bucket indexes ( .rgw.buckets.index and friends) — if you're using radosgw as your drop-in OpenStack Swift replacement Cinder volume pools ( cinder , volumes ) radosgw data pools ( .rgw.buckets and friends) — if you need low-latency reads and writes on Swift storage Glance image pools ( glance , images ) Cinder backup pools ( cinder-backup ) — usually the last pool to convert to all-flash OSDs. Do designate some non-Ceph compute hosts with low-latency local storage Now, there will undoubtedly be some applications where Ceph does not produce the latency you desire. Or, for that matter, any network-based storage. That's just a direct consequence of recent developments in storage and network technology. Just a few years ago, the average latency of a single-sector uncached write to a block device was on the order of a millisecond, or 1,000 microseconds (µs). In contrast, the latency incurred on a TCP packet carrying a 512-byte (1-sector) payload was about 50 µs, which makes for a 100-µs round trip. All in all, the additional latency incurred from writing to a device over the network, as opposed to locally, was approximately 10%. In the interim, a single-sector write for a device of the same price is itself about 100 µs, tops, with some reasonably-priced devices down to about 40 µs. Network latency, in contrast, hasn't changed all that much — going down about 20% from Gigabit Ethernet to 10 GbE. So even going to a single, un-replicated SSD device over the network will now be 40 + 80 = 120 µs latency, vs. just 40 µs locally. That's not a 10% overhead anymore, that's a whopping factor of 3. With Ceph, that gets worse. Ceph writes data multiple times, first to the primary OSD, then (in parallel) to all replicas. So in contrast to a single-sector write at 40 µs, we now incur a latency of at least two writes, plus two network round-trips, to that's 40 x 2 + 80 x 2 = 240 µs, six times the local write latency. The good news is, most applications don't care about this sort of latency overhead, because they're not latency-critical at all. The bad news is, some will. So, should you ditch Ceph because of that? Nope. But do consider adding a handful of compute nodes that are not configured with libvirt/images_type=rbd , but that use local disk images instead. Roll those hosts into a host aggregate, and map them to a specific flavor. Recommend to your users that they use that flavor for low-latency applications. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/dos-donts-ceph-openstack/","loc":"resources/hints-and-kinks/dos-donts-ceph-openstack/"},{"title":"High Availability and Disaster Recovery in OpenStack","text":"From the 2016 International Industry-Academia Workshop on Cloud Reliability and Resilience in Berlin. An OpenStack primer followed by a closer focus on OpenStack's HA & DR feature set. About 35 minutes. Slides: GitHub Use the arrow keys to navigate through the presentation, hit Esc to zoom out for an overview, or just advance by hitting the spacebar. This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/high-availability-and-disaster-recovery-in-openstack/","loc":"resources/presentations/high-availability-and-disaster-recovery-in-openstack/"},{"title":"Heat and its Alternatives: Application Deployment in OpenStack","text":"From the 2016 OpenStack Summit in Barcelona. A comparison of tools for virtual systems orchestration in OpenStack. Heat Juju Ansible Cloudify About 45 minutes. Video: YouTube Slides: GitHub Use the arrow keys to navigate through the presentation, hit Esc to zoom out for an overview, or just advance by hitting the spacebar. This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/heat-and-its-alternatives-application-deployment-in-openstack/","loc":"resources/presentations/heat-and-its-alternatives-application-deployment-in-openstack/"},{"title":"CephFS and LXC: Container High Availability and Scalability, Redefined","text":"An overview of applying CephFS to LXC containers. Slides: GitHub Use the arrow keys to navigate through the presentation, hit Esc to zoom out for an overview, or just advance by hitting the spacebar. This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/cephfs-and-lxc-container-high-availability-and-scalability-redefined/","loc":"resources/presentations/cephfs-and-lxc-container-high-availability-and-scalability-redefined/"},{"title":"Fragile development: Why Scrum sucks, and what you should be doing instead (full talk)","text":"My presentation from FrOSCon 2016. The lengthier version of the Ignite talk I did a few months before, at OpenStack Israel. Video: YouTube , CCC Slides: GitHub Use the arrow keys to navigate through the presentation, hit Esc to zoom out for an overview, or just advance by hitting the spacebar.","tags":"presentations","url":"resources/presentations/fragile-development-why-scrum-sucks-and-what-you-should-be-doing-instead-full-talk/","loc":"resources/presentations/fragile-development-why-scrum-sucks-and-what-you-should-be-doing-instead-full-talk/"},{"title":"Fragile Development: Scrum is terrible, and you should ditch it","text":"This is a writeup of an Ignite talk I gave at OpenStack Israel 2016 . The paragraph headings below approximately correspond to the content of my talk slides; the paragraphs themselves are an approximation of what I said. If you're interested in the exact slide content, you can find that here . Zero flexibility _____ ‘s roles, artifacts, events, and rules are immutable and although implementing only parts of _____ is possible, the result is not _____ . When you see a statement like this and wonder what should be filled in for the blanks, it's rather quite likely that you would guess either a radical political ideology, a very strict religious sect or cult, or something to that effect. You couldn't be further from the truth. Scrum ‘s roles, artifacts, events, and rules are immutable and although implementing only parts of Scrum is possible, the result is not Scrum . Yes, that's a direct quote from the Scrum guide. Scrum, by its own definition, can either be implemented completely — that is, with all its roles, artifacts, events, and rules unchanged — or not at all. This sounds ludicrous enough as it is, and any sane, thinking person should reject or at least resent any such statement outright. But let's give Scrum the benefit of doubt, and let's actually start examining some of its postulates. Teams are self-organizing Scrum hinges on the idea that teams are comprised of capable individuals forming teams, which then self-organize. Now I'm sure nobody would argue that self-organizing teams cannot exist, so this postulate does not invalidate itself outright. However, it is missing an important prerequisite: teams can self-organize if they are stable. And team stability is a precondition that almost never exists in the software industry: our industry is growth-oriented, and driven by quickly-growing startups, so in a successful organization having a new colleague every other month is not unheard of. It is also highly competitive for talent, so having a colleague leave every few months isn't unusual either. The moment a new person joins or leaves, you have a new team. Team stability goes out the window, and with it any reasonable expectation of self-organization. Sprint after sprint after sprint The Scrum Guide explicitly states that every sprint (a time frame of one month or less, in which the team completes objectives agreed to for the sprint backlog) is immediately followed by the next sprint. This is mind-bogglingly ludicrous and outright dangerous to your team's mental health. Software development is a marathon, and running a marathon as an unbroken series of sprints leads to collapse or death. In software development, it's likely to cause burnout. The Daily Scrum One of Scrum's immutable events is the Daily Scrum. The Scrum Guide defines this event as a specific, daily occurrence, time-boxed to 15 minutes and involving the entirety of the development team. This is staggeringly out of place in the modern development team, which may well be spread out over multiple offices and timezones, and may not even physically be in one place more than a handful of times a year. Even in the unlikely event that everyone can get together in one room for precisely fifteen minutes each day, have you ever been in a meeting involving more than 3 people that got anything accomplished in 15 minutes? And remember, 15 minutes. Time-boxed, immutable. If you think your Daily Scrum can be 30 or 45 minutes, or you can do it just every other day or maybe thrice a week, recall: if you do that, you're no longer doing Scrum. No planning beyond the current sprint Scrum is quite emphatic that the only thing developers should be really concerned about in terms of planning is the next 24 hours (the plan for which is ostensibly being laid out in the Daily Scrum), and beyond that, the current sprint at a maximum. Now, while the idea of freeing people's minds and allowing them to focus on a single task at hand is certainly laudable, the practical implications of having no medium to long-term planning is insane. I'd venture a guess that an approach where no planning is for more than a month out is viable, under one condition: having exactly zero users and/or customers for the product you are developing. I leave it to you to decide how valuable it is, then, to develop the product in the first place. Permanent emergency mode Arguably, some of the methods proposed in Scrum are quite suitable for emergency situations. In a situation where you need to come up with a solution that requires creativity, hustle, and speed, you may well sit down, put down a requirements list, elect a coordinator and spokesperson for your team, and just start hacking. I'd fully agree that such situations can be extremely challenging, and quite satisfying to come out of with flying colors. But if your organization is permanently operating in this mode, quit. It doesn't matter which role you're in: as a developer, you're headed for burnout. As a manager, you're herding your team into burnout. Either way, you shouldn't be doing this job, either in your own interest or in that of others. Novelty? Scrum proponents frequently argue in its favor as the antithesis of the obsolete waterfall model, where all deliverables are defined from the outset and there is no room for deviation, leading to products that are either broken, or outdated, or both the moment they are completed. If you think we only found out recently that waterfall is bad, you've been asleep at the switch for over 30 years. In his seminal Mythical Man-Month essay collection from 1975, Fred Brooks pointed out some weaknesses of this model, and in his 1986 follow-up No Silver Bullet, he proposes organic, incremental software development as an alternative. Your team can't work with Scrum? Scrum advocates frequently argue that if Scrum doesn't work with your team, chances are that your team is the problem. This means that you should either replace them, or at least educate them in the ways and means of Scrum, so they can become a better-performing team. At this point, it should be fairly obvious that if Scrum doesn't work for your team, the problem is not your team. The problem is Scrum. What if Scrum doesn't deliver? And finally, Scrum proponents usually argue that if Scrum fails to deliver adequate results in your organization, it's likely because you aren't applying its central tenets correctly. In other words, you must come to your senses, and implement Scrum as designed, and which point results with magically appear, and your team will be in a constant state of flow. This is nonsense. If you were able to actually do Scrum (meaning in its pure, immutable, One True Way), it would surely lead to disaster. But, it's impossible to do so anyway, so go ahead and ditch it — stop being a scrumbag. This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2016/07/05/fragile-development/","loc":"blog/2016/07/05/fragile-development/"},{"title":"Wiping and resetting your SUSE OpenStack Cloud Crowbar configuration","text":"Note: This article was originally written for SUSE OpenStack Cloud 6, and updated for SUSE OpenStack Cloud 7. It may not apply to later SUSE OpenStack Cloud releases. If you're using SUSE OpenStack Cloud , you may want to erase and reinstall your cloud deployment a few times during the testing or proof-of-concept phase. You may also want to experiment with a few permutations of Crowbar network configurations. SUSE's (otherwise excellent) Deployment Guide suggests that the only way to change your Crowbar settings, after install-suse-cloud has been run, is to reinstall your entire admin node . That isn't really true if you know what you're doing. You may be thinking that you could just use snapper to revert to your last Btrfs snapshot created before you ran install-suse-cloud . After all, running yast2 crowbar , like any other YaST module, automatically creates a before-and-after Btrfs snapshot of your root filesystem and all its subvolumes. So, reboot machine, select pre- install-suse-cloud snapshot, complete boot, run snapper rollback , done. Right? Well, not quite. If you followed the Deployment Guide closely, you will have removed your Btrfs subvolume for the /srv directory, and replaced it with a separate, XFS-formatted partition. That means it is excluded from all snapper Btrfs snapshots, and thus, no rollback for you for that directory. Which, of course, Crowbar uses rather extensively. So, here is your checklist for resetting your admin node to a pre- install-suse-cloud state: Reboot your admin node. In the SLES boot menu, select an appropriate snapshot taken immediately prior to running install-suse-cloud . Boot into your snapshot. Run snapper rollback . Reboot again. After rebooting, delete the following and directories: /srv/tftpboot/authorized_keys /srv/tftpboot/validation.pem all subdirectories under /srv/tftpboot/nodes/ Then, you can reconfigure Crowbar ( yast2 crowbar ), run install-suse-cloud , and reboot your OpenStack nodes. They should be discovered anew, and you're then able to redeploy your OpenStack barclamps to them. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/wipe-suse-openstack-cloud-config/","loc":"resources/hints-and-kinks/wipe-suse-openstack-cloud-config/"},{"title":"Why Scrum sucks, and what you ought to be doing instead","text":"An Ignite talk I presented at OpenStack Israel 2016. Video: YouTube Slides: GitHub","tags":"presentations","url":"resources/presentations/why-scrum-sucks-and-what-you-ought-to-be-doing-instead/","loc":"resources/presentations/why-scrum-sucks-and-what-you-ought-to-be-doing-instead/"},{"title":"Containers: Just Because Everyone Else is Doing Them Wrong, Doesn't Mean You Have To","text":"This is a writeup of a presentation I did at LinuxCon Europe in Dublin last year. Since Linux Foundation Events still don't come with video recording for all talks (all they do record and publish are keynotes), I can't point you to a YouTube link, though you're certainly welcome to peruse my slides from that talk. The problem Suppose you're an operator who, in a massively scaled-out and highly automated deployment, is responsible for keeping a few hundred or a few thousand containers up and running. Your developers put those together and then basically throw them over the wall for you to manage. It's your job just to keep them alive, available, and secure; what's in them is your developers' domain. Sure, you have Git repos you build your containers from, and a Docker registry, so you can always check what's in which container. You don't get to call the shots, though. Suppose further that all most of your containers run some form of web service. And let's assume, just for the sake of this discussion, that they're all running Apache, because that's your reference platform. Your developers may be writing applications in Python or Ruby or (shudder) PHP, but what all your apps have in common is that you've settled on Apache as your reference platform. Your developers can assume that with Apache, you, the ops person, know the boldface cold, and you can give them an extremely stable, well-tuned platform to build on. And then Apache is affected by some disturbing security vulnerability that you must now fix in record time. Say, something affecting your core SSL library or maybe even your C library. Sound familiar? Thought so. The fix in a non-containerized world OK, so you must now fix OpenSSL or libc on all your systems in record time before the anticipated exploit barrage rolls in. In a world without containers, you'd rely on your trusted software source (normally, your distro vendor) to provide you with a fixed package or packages for the affected libraries. You would then roll those out via your preferred software management utility, or system automation facility, or unattended upgrade scheme. In short, you'd have a tense time until updated packages are available, but once they are, things get fixed in a matter of minutes. But what now? With the deployment of containers comes, frequently, the notion that packaging, package management, or dependency tracking is somehow a terrible idea. Instead, you put everything you need into one container image, deploy one container per service, and not worry about what a different service running on the same physical hardware might need. At first glance, that simplifies things. Your developer needs MySQL configured a certain way, and some other app needs it differently? Fine, they can put everything in their own separate containers, binaries, libraries and all, problem solved. Storage is dirt cheap, containers are efficient and produce little overhead. If they ever need to change anything, say go from one MySQL point release to another, then they just rebuild the container, you replace the old build with the new one, fine. But now it's not your developer who wants to change things, it's you who needs to deploy a critical fix. 1 so.. using GlibC? How's re-imaging all of your @Docker images going? — Josh Long (龙之春) ( @starbuxman ) February 19, 2016 So you set out to rebuild a few hundred containers, or maybe a couple of thousand, to get the issue fixed. In a perfect environment, you have access to every build chain, know about every version of every container in your area of responsibility, can pinpoint exactly which are affected by the vulnerability, have an automated toolchain to build and deploy them, have perfect documentation so you don't need to check back with any of your developers, so it doesn't matter whether any one is out sick, on vacation, or has left the company since they deployed one of their, now potentially affected, services. And of course, everyone works in such a perfect environment. Right? So now, even after a fix to your issue is already available, you still need to scramble to get it deployed, and deploying is a lot more complicated than in a world without containers. Is this an inherent problem with containers? Of course not. The problem isn't with the fact that you're using containers, or with the specific container technology. The problem is that everyone is telling you to use containers a certain way, and from an operational perspective that way is wrong. And it's not even \"wrong but still better than all other options\", it's just wrong. I guess you could call it the Docker Fallacy. That's the bad news. The good news is that there is a way that is better, saner, and cleaner, and will make your life as an operator much easier, while not being too hard on your developer friends. So what's a better way? You can use containers in a simpler, less flashy, less exciting — in short, better way. Define a core platform, or platforms Any organization worth its salt will select a handful of distributions to build products and services on. Maybe it's even just one, but let's assume you have several, say the latest Ubuntu LTS, 2 the latest CentOS, and the latest Debian. For each of these, you can define an absolute bare-minimal list of packages. I can almost guarantee you that none of your developers will care about a single item on that list. A C library, a shell, an init system, coreutils, NTP… chances are that you'll run up a list of well over 100 core system components that you will be expected to keep secure; your developers will take them all for granted. What you can take for granted, thanks to the tireless work of packagers and distro vendors over years and years, is that you will get timely security updates for all of those. Deploy your core platforms as often as you need Deploy these reference systems across your physical hardware. Deploy as many as you need for all the containers you're expected to run on each platform. Do so in an automated fashion, so that you never have to log into any of these systems by hand. Use OverlayFS for your containers OverlayFS is a union mount filesystem that ships as part of the mainline kernel. With OverlayFS you can do a few clever things: Use a read-only base filesystem with a writable overlay to create a read/write union mount. Write to the union mount and only touch the overlay, leaving the base filesystem pristine. Hide selected content in the base filesystem from the union mount, through the use of opaque directories . Use one base filesystem with multiple overlays to create any number of separate read/write union mounts. Immediately make updates to the base filesystem known to all union mounts, by simply remounting them. This makes OverlayFS extremely powerful when used together with LXC. You define a bunch of overlay directories — one for each of your containers —, and they can all share one base filesystem: your host root filesystem. 3 Then, the union mount becomes your LXC container's root. It automatically has read access to everything that is available on the host, unless specifically hidden, and whatever it writes goes to the overlay. When you discard a container, you delete the overlay. Here is a minimal example configuration for a container like this: # For additional config options, please look at lxc.container.conf(5) # Common configuration lxc.include = /usr/share/lxc/config/ubuntu.common.conf # Container specific configuration lxc.arch = amd64 # Network configuration lxc.network.type = veth lxc.network.link = lxcbr0 lxc.network.flags = up lxc.network.hwaddr = 00:16:3e:76:59:10 # Automatic mounts lxc.mount.auto = proc sys cgroup lxc.rootfs = overlayfs:/var/lib/lxc/host/rootfs:/var/lib/lxc/mytestcontainer/delta0 lxc.utsname = mytestcontainer Note that the LXC userland presently enforces an OverlayFS base directory to be in a subtree of /var/lib/lxc . You can satisfy this requirement by bind-mounting / to /var/lib/lxc/host/rootfs , as shown in the example above. What this creates, among other things, is crystal-clear separation of concerns: whatever is in the overlay is for your developers to decide. They can pull in packages from PyPI, Ruby Gems, NPMs, whatever. What's in the host root is your responsibility. Automate, automate, automate It's obvious and self-evident, but it doesn't hurt to reiterate: you want to automate all of this. You're certainly free to select your own tools to do it, but Ansible specifically has very good LXC container support so it makes this a breeze. Here's a simple Ansible playbook example that creates 100 containers, all based off your host root. 4 - hosts : localhost tasks : - name : Create a local bind mount for the host root filesystem mount : name : /var/lib/lxc/host/rootfs src : / opts : bind fstype : none state : mounted - name : Create a template container using the host root lxc_container : name : host state : stopped directory : /var/lib/lxc/host/rootfs config : /var/lib/lxc/host/config container_config : - \"lxc.mount.auto = proc sys cgroup\" - \"lxc.include = /usr/share/lxc/config/ubuntu.common.conf\" - name : Create 100 OverlayFS based containers lxc_container : name : host backing_store : overlayfs clone_snapshot : true clone_name : \"mytestcontainer{{ item }}\" state : started with_sequence : count=100 Now of course, this will also mean that you'll need to get your developers to define their container config in Ansible. However, that is fundamentally a good thing, because it means that developers and operations people will be reading and writing the same language. Also, if your developers can write a Dockerfile, they won't have a hard time with Ansible YAML either. How does this help? With this approach, think of what you now have to do to make hundreds of containers running on the same box get a new libc. Update your host libc. Restart your containers. That's it. That is literally all you have to do to update hundreds of containers in one fell swoop. LXC will remount your OverlayFS on container restart, and thus all changes to the host will be immediately visible in the container's overlay filesystem. On an Ubuntu platform, you could even go so far as automating this in conjunction with unattended upgrades: # /etc/apt/apt.conf.d/50unattended-upgrades // Automatically upgrade packages from these ( origin:archive ) pairs Unattended - Upgrade:: Allowed - Origins { \"${distro_id}:${distro_codename}-security\" ; }; # /etc/apt/apt.conf.d/05lxc DPkg:: Post - Invoke { \"/sbin/service lxc restart\" ; }; So there you have it. Upgrade loads of containers in minutes. No rebuild, no redeploy, nothing. Packaging actually does work and has merit, regardless of what the hipster crowd is trying to sell you. This article originally appeared on my blog on the hastexo.com website (now defunct). Edit, 2016-02-22: Added Twitter quote from Josh Long. ↩ At the time of writing, the latest Ubuntu LTS is 14.04 \"Trusty Tahr\", which is based on a Linux 3.13 kernel. This Ubuntu stock kernel ships with a pre-release version of OverlayFS which predates the 3.14 mainline merge. I would not recommend using that kernel; instead you'll want to run your hosts with a more recent kernel from the LTS Enablement Stack . Again at the time of writing, this is a Linux 4.2 kernel that ships with the linux-generic-lts-wily package. ↩ LXC containers do present per-container specific content for some directories by default, notably /proc , /dev , and /sys . Other host-filesystem content can be hidden by creating opaque directories in the container overlay; this is what you would commonly do for directories like /root , /home , /tmp and others. ↩ Please note that it's not quite as simple as shown in the Ansible example. You will want to provide some additional tweaks, such as added mounts or opaque directories. I've tried to keep the example brief to illustrate the concept. ↩","tags":"blog","url":"blog/2016/02/21/containers-just-because-everyone-else/","loc":"blog/2016/02/21/containers-just-because-everyone-else/"},{"title":"Dogfooding Dogwood","text":"This week, the Open edX community announced its latest release, Open edX Dogwood . (In case you don't follow the Open edX community closely, its releases are alphabetically named after trees, so on the heels of the Birch and Cypress releases, we now have Dogwood , and Eucalyptus will be next.) Our team got involved in Open edX around the Cypress release timeframe, and we shifted our OpenStack integration work to track the master branch in December, to ensure we would be ready in time for Dogwood. hastexo Academy also tracks master, so if you take one of our self-paced online courses, you'll be running the latest and greatest from Open edX. Checking out the new features There are several new features in Open edX Dogwood, some of which we tested and ran, with somewhat mixed (but generally positive) results. Platform upgrades Open edX now builds upon Django 1.8 and Python 2.7.10. It's great to see some technical debt pay-down by moving beyond the now-unsupported Django 1.4. We hope to see this continue by Eucalyptus hopefully moving to the next Ubuntu LTS, 16.04 \"Xenial Xerus\". It would also be great to see a move to Python 3, but we're not holding our breath on that, for various reasons — including the fact that Ansible, which Open edX uses for deployment, also still requires Python 2. Comprehensive theming Comprehensive theming is a new and improved way to apply theming and branding to Open edX platforms, which will eventually replace the current \"Stanford\" theming engine (named after an Open edX theme developed at Stanford University, which became a popular basis for rebranding the Open edX LMS). In mid-January, we shifted our own Stanford-style Open edX theme to Comprehensive Theming and test-deployed on hastexo Academy, then still in pre-launch. We ran into a critical bug that has been fixed for the release, and will come back to redeploying our new Comprehensive theme at a later date. We're also waiting for a patch to the edx-configuration Ansible repository to land, so we can properly deploy our Comprehensive theme to our Open edX instance. Otto We also looked extensively at the new Open edX ecommerce framework, \"Otto\", for buying and sellling course seats. Sadly, we found multiple issues that prevented us from using it in our infrastructure for the time being, and we pushed Otto off for our Eucalyptus respin. Otto has no support for tax assessment on course seats; this is a show stopper for anyone who wants to sell courses to people in Europe, as course seats are Digital Goods under EU VAT regulations and require VAT assessment. We were admittedly a little dismayed to find that Otto had made some design decisions that made this impossible to fix in the way you would normally do this in the Oscar framework that Otto builds on. Fixing Otto in-place would likely have delayed our Academy launch by several months, so that was a delay we were unwilling to accept. There are other issues with Otto, notably the fact that it comes with its own PayPal integration (as if django-oscar-paypal didn't exist), which made us rather uncomfortable. So we instead integrated hastexo Academy with our own, pure-Oscar web store that makes use of upstream community supported features much more extensively than Otto, and that also enables us to sell other products and services besides hastexo Academy course seats. LTI XBlock With the Dogwood release, the LTI XModule has been refactored into the LTI Consumer XBlock . While we do not currently use this XBlock in production, it comes in very handy as a good reference for XBlock unit tests , which we'll be using to improve the test coverage in our own XBlock. Open edX integration with OpenStack Our OpenStack integration work for Open edX is continuing at its regular, steady pace. Running Open edX Dogwood on OpenStack You're of course still able to deploy Open edX on OpenStack, using the Heat templates we've maintained since Cypress. Running the hastexo XBlock on Open edX Dogwood The hastexo XBlock, enabling course authors to define arbitrarily complex lab environments for courses with OpenStack Heat, is of course fully supported for Open edX Dogwood. That's exactly what you're using when speeding through interactive labs on hastexo Academy. Congrats, and thanks! Congratulations are in order for the entire development community! Our team at hastexo would like to extend a big thank-you to everyone who made a contribution to this release. This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2016/02/12/dogfooding-dogwood/","loc":"blog/2016/02/12/dogfooding-dogwood/"},{"title":"Pacemaker's best-kept secret: crm_report","text":"Whenever things in Pacemaker go wrong (say, for example, resource failover doesn't work as expected, or your cluster didn't properly recover after a node shutdown), you'll want to find out just exactly why that happened. Of course, the actual reason for the malfunction may be buried somewhere deep in your cluster configuration or setup, and so you might need to look at quite a few different sources to pin it down. Sometimes, too, you want to enlist the help of a colleague, or maybe our help even , to get to the bottom of the issue. And sometimes it's not practical to let someone access to system to just trigger the problem and watch what breaks. Thankfully, Pacemaker ships with a utility that helps you collect everything you or someone else might need to look at, in a simple, compact format. Unfortunately few people, including even long-time Pacemaker users, know that it exists: it's called crm_report . Running crm_report crm_report ‘s command syntax is rather quite simple. You just tell it how far in the past you want the report to start, and which directory you want to collect data in: crm_report -f \"2016-01-25 00:00:00\" /tmp/crm_report The directory you specify must not exist. If it does, crm_report will refuse to run, rather than clobber or mess up your existing report data. By analyzing your logs all the way back to a start date you specify, crm_report makes it unnecessary for you to actually try to reproduce the problem. All you need is a rough idea when the issue occurred, and then you give crm_report a timestamp a little earlier than that as its start date. You can also specify the end of the period you're interested in. Suppose you're exactly aware of a 10-minute time window in which the problem occurred. In that case, you could run: crm_report -f \"2016-01-25 01:15:00\" -t \"2016-01-25 01:25:00\" /tmp/crm_report Either way, crm_report will collect relevant log data for the specified time window on the host it is run on, and then connect to the other cluster nodes (via ssh ) and do the same there. The latter behavior can be disabled by adding the -S or --single-node option, but there usually isn't a good reason to do that. In the end, everything will be rolled into one tarball at /tmp/crm_report.tar.bz2 You can then pull the report tarball off the node (with scp , rsync , whatever you prefer), and then share it with whom you need to. Note that the tarball can contain sensitive information such as passwords, so be careful whom you share it with. What's in a crm_report tarball? There's a bunch of truly helpful information in a crm_report generated tarball. Depending on how your cluster is configured and what problems were detected, it will contain, among other things: Your current Pacemaker Cluster Information Base (CIB), Your Corosync configuration, Corosync Blackbox output (if qb-blackbox is installed on your cluster nodes; you can read more about blackbox support here ), drbd.conf and all your DRBD resource configuration files (if your cluster runs DRBD), sysinfo.txt , a text file including your kernel, distro, Pacemaker version, and version information for all your installed packages, your Syslog, filtered for the time period you specified in your crm_report command invocation, diffs for critical system information, if crm_report detected discrepancies between nodes. In other words, it contains pretty much everything that needs to be shared in a critical troubleshooting situation. Why isn't this more widely known? To be perfectly honest, we have no idea. crm_report has been in Pacemaker for years, and even prior to its existence, there was a predecessor named hb_report . It's an extraordinarily useful utility, yet when we ask customers to send a crm_report tarball during a Pacemaker troubleshooting engagement, the usual response is, \"a what?\" We hope this post makes crm_report known to a wider audience, so it gets the love it deserves. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/pacemakers-best-kept-secret/","loc":"resources/hints-and-kinks/pacemakers-best-kept-secret/"},{"title":"Hosting a web site in radosgw","text":"If you're familiar with web site hosting on Amazon S3 , which is a simple and cheap way to host a static web site, you might be wondering whether or not you can do the same in Ceph radosgw. The short answer is you can't. Bucket Website is listed as Not Supported in the radosgw S3 API support matrix , and radosgw doesn't have index document support either. But the longer answer is that you can, provided you use radosgw in combination with a front-end load-balancer — which, as it happens, can add a few more bells and whistles, as well. You could probably do the same thing with nginx, Varnish, or Apache in a mod_proxy_balancer balancer setup, but in this example configuration, we'll use HAProxy. Getting started: the radosgw basics Let's take look at a simple radosgw configuration with virtual host support, such that you can access your buckets as either http://ceph.example.com/bucketname or http://bucketname.ceph.example.com : [client.rgw.radosgw01] rgw_frontends = civetweb port=7480 rgw_dns_name = ceph.example.com rgw_resolve_cname = True Suppose we use s3cmd to upload an HTML file to this bucket, setting a public ACL: s3cmd mb s3://testwebsite s3cmd put --acl-public index.html s3://testwebsite/ Then if you exposed your radosgw to the web, any client (without authentication) would be able to retrieve http://testwebsite.ceph.example.com:7480/index.html with a web browser, or any other HTTP client application (such as curl or wget ): curl -I http://testwebsite.ceph.example.com:7480/index.html Which would then return something like: HTTP / 1.1 200 OK Content-Length : 18050 Accept-Ranges : bytes Last-Modified : Mon, 25 Jan 2016 21:28:47 GMT ETag : \"b03130a4a1fc24df0f9f336f2b6d1d90\" x-amz-request-id : tx000000000000000005a88-0056a7b7eb-312df-default Content-type : text/html Date : Tue, 26 Jan 2016 18:16:11 GMT Introducing HAProxy Now let's start out with putting HAproxy in between. Nothing special there: radosgw listens on the conventional 7480 port, and we simply hand HAproxy traffic through there, and bind HAProxy itself to port 80. global log /dev/log local0 pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemon # turn on stats unix socket stats socket /var/lib/haproxy/stats level admin # Default SSL material locations ca-base /etc/ssl/certs crt-base /etc/haproxy/ssl # Default ciphers to use on SSL-enabled listening sockets. # For more information, see ciphers(1SSL). ssl-default-bind-ciphers HIGH tune.ssl.default-dh-param 2048 defaults log global mode http option httplog option dontlognull retries 3 timeout queue 1000 timeout connect 1000 timeout client 30000 timeout server 30000 option forwardfor frontend ceph_front bind 0.0.0.0 : 80 default_backend ceph_back backend ceph_back balance source server radosgw01 127.0.0.1 : 7480 check Index documents So, the first thing we'll need to add is support for index documents. We'd like to make sure that when we retrieve https://testwebsite.ceph.example.com/ , what's actually fetched from the backend is /index.html . We can do that by adding an HAproxy ACL that matches for the trailing slash in the path, and an http-request set-path directive that appends the index document name: frontend ceph_front bind 0.0.0.0 : 80 acl path_ends_in_slash path_end -i / # Append index document (index.html) to any path # ending in \"/\". http-request set-path %[path]index.html if path_ends_in_slash default_backend ceph_back Now, that's fine in terms of getting the index document correctly: curl -I http://testwebsite.ceph.example.com/index.html HTTP / 1.1 200 OK Content-Length : 18050 Accept-Ranges : bytes Last-Modified : Mon, 25 Jan 2016 21:28:47 GMT ETag : \"b03130a4a1fc24df0f9f336f2b6d1d90\" x-amz-request-id : tx000000000000000005a94-0056a7b9e3-312df-default Content-type : text/html Date : Tue, 26 Jan 2016 18:24:35 GMT However, it of course breaks uploads and even bucket listings, or in other words, anything that uses the S3 API. Now you could test for some S3-specific headers in the request, but really, you should just check whether the request is authorized, and only apply the index document logic if it isn't, like so: frontend ceph_front bind 0.0.0.0 : 80 acl path_ends_in_slash path_end -i / acl auth_header hdr(Authorization) -m found # Append index document (index.html) to any path # ending in \"/\", unless the request has an auth header http-request set-path %[path]index.html if path_ends_in_slash !auth_header default_backend ceph_back Great. Now we can upload using full paths without mangling, and on any un-authenticated requests, we substitute /index.html for any trailing / . In case you're wondering: yes, this works for any path, not just the root path. Directory paths However, you may also want something else, which is the ability to correctly handle a request like http://testwebsite.ceph.example.com/my/sub/directory , where of course you want the path /my/sub/directory translated into /my/sub/directory/index.html , which means we want to append a slash and an index document name to the request path. So let's do that: frontend ceph_front bind 0.0.0.0 : 80 acl path_has_dot path_sub -i . acl path_ends_in_slash path_end -i / acl auth_header hdr(Authorization) -m found http-request set-path %[path]index.html if path_ends_in_slash !auth_header # Append trailing slash if necessary. http-request set-path %[path]/index.html if !path_has_dot !path_ends_in_slash !auth_header default_backend ceph_back Note that what we're doing here is somewhat crude. We're assuming that any actual file that we want to retrieve looks like name.ext , meaning it has a dot (period, full stop) character in it. The path_sub -i . expression in the path_has_dot ACL simply matches any path with . in it, and we're assuming that if a path has a dot then it points to a file, if it doesn't then it points to a directory. You could be a little more clever here and use path_regex instead of path_sub for a full regular expression match. But regex lookups are slower than simple substring matches, so if the substring match works for you, go for it. So now, we can do this: s3cmd put --acl-public index.html s3://testwebsite/my/sub/directory/ And then: # Note omitted trailing slash curl -I http://testwebsite.ceph.example.com/my/sub/directory HTTP / 1.1 200 OK Content-Length : 24235 Accept-Ranges : bytes Last-Modified : Mon, 25 Jan 2016 23:57:04 GMT ETag : \"fecd005b33c0f6bfdee61b787cf54cb0\" x-amz-request-id : tx00000000000000000bc83-0056a7bd25-312cd-default Content-type : text/html Date : Tue, 26 Jan 2016 18:38:29 GMT HTTPS support So, what else might you want to do? One obvious thing that you can use HAproxy for is SSL termination. The radosgw embedded civetweb webserver can do that for you, but that feature is currently mildly broken in a rather curious way . So in order to allow HTTPS access to all your content via HAproxy instead, you would add: frontend ceph_front_ssl bind 0.0.0.0 : 443 ssl crt ceph.pem no-sslv3 no-tls-tickets reqadd X-Forwarded-Proto:\\ https acl path_has_dot path_sub -i . acl path_ends_in_slash path_end -i / acl auth_header hdr(Authorization) -m found http-request set-path %[path]index.html if path_ends_in_slash !auth_header http-request set-path %[path]/index.html if !path_has_dot !path_ends_in_slash !auth_header default_backend ceph_back But maybe you'd like to force, not merely allow, HTTPS access. redirect to the rescue: frontend ceph_front bind 0.0.0.0 : 80 reqadd X-Forwarded-Proto:\\ http redirect scheme https code 301 if ! { ssl_fc } frontend ceph_front_ssl bind 0.0.0.0 : 443 ssl crt ceph.pem no-sslv3 no-tls-tickets reqadd X-Forwarded-Proto:\\ https acl path_has_dot path_sub -i . acl path_ends_in_slash path_end -i / acl auth_header hdr(Authorization) -m found http-request set-path %[path]index.html if path_ends_in_slash !auth_header http-request set-path %[path]/index.html if !path_has_dot !path_ends_in_slash !auth_header default_backend ceph_back And here we go: # Note HTTP curl -IL http://testwebsite.ceph.example.com/my/sub/directory HTTP / 1.1 301 Moved Permanently Content-length : 0 Location : https://testwebsite.ceph.example.com/my/sub/directory Connection : close HTTP/1.1 200 OK Content-Length: 24235 Accept-Ranges: bytes Last-Modified: Mon, 25 Jan 2016 23:57:04 GMT ETag: \"fecd005b33c0f6bfdee61b787cf54cb0\" x-amz-request-id: tx00000000000000000bdeb-0056a7bf9b-312cd-default Content-type: text/html Date: Tue, 26 Jan 2016 18:48:59 GMT Compression And finally, maybe you'd like to speed up access to the stuff on your site. Why not add gzip on-the-fly-compression? It's supported by every browser worth its salt, and will make your users happier. You'll want to restrict compression to specific MIME types though. In the configuration below, we enable compression for plain text, HTML, XML, CSS, JavaScript, and SVG images. frontend ceph_front bind 0.0.0.0 : 80 reqadd X-Forwarded-Proto:\\ http redirect scheme https code 301 if ! { ssl_fc } frontend ceph_front_ssl bind 0.0.0.0 : 443 ssl crt ceph.pem no-sslv3 no-tls-tickets reqadd X-Forwarded-Proto:\\ https acl path_has_dot path_sub -i . acl path_ends_in_slash path_end -i / acl auth_header hdr(Authorization) -m found http-request set-path %[path]index.html if path_ends_in_slash !auth_header http-request set-path %[path]/index.html if !path_has_dot !path_ends_in_slash !auth_header compression algo gzip compression type text/html text/xml text/plain text/css application/javascript image/svg+xml default_backend ceph_back Let's see how that helps us. Do a request without gzip encoding support, and observe that its total download size matches the document's Content-Length : curl https://testwebsite.ceph.example.com/my/sub/directory > /dev/null % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 24235 100 24235 0 0 94565 0 --:--:-- --:--:-- --:--:-- 94299 Now, add an Accept-Encoding header: curl -H 'Accept-Encoding: gzip' https://testwebsite.ceph.example.com/my/sub/directory > /dev/null % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 5237 0 5237 0 0 19243 0 --:--:-- --:--:-- --:--:-- 19324 There. Actual download size goes from 24KB down to just 5KB. Where to go from here There's a few additional features to be added here. You could enable CORS or HSTS, for example, and of course you could add more backends. But if you read this far, you surely get the idea. And you're welcome to examine the headers you can pull from this page you're reading, wink wink. :) This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/hosting-website-radosgw/","loc":"resources/hints-and-kinks/hosting-website-radosgw/"},{"title":"My first Open edX contribution","text":"I've finally submitted my first code contribution to Open edX , a trivial patch for an annoying issue in the LMS start page. The PR is here . The LMS component in Open edX is the stuff that actually provides a learning platform to students, including the courseware itself, a discussion forum, a wiki, and everything else you need for an immersive learning experience. In our own hastexo Academy environment, it of course also loads the hastexo XBlock to interface with arbitrarily complex, on-demand lab environments. If you want to know how the LMS fits into Open edX overall, there's an overview graphic over at open.edx.org for your perusal. Being a new contributor to Open edX, this obviously involves jumping through yet another Contributor Agreement process . Here's to hoping this gets resolved quickly. Update, 2016-02-03 The contributor agreement was squared away really fast ; the patch review did, however, take some time. But it's in now . This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2016/01/05/my-first-open-edx-contribution/","loc":"blog/2016/01/05/my-first-open-edx-contribution/"},{"title":"Removing buckets in radosgw (and their contents)","text":"Every once in a while you'll want to remove a bucket in radosgw, including all the objects contained in that bucket. Now you might use a utility like s3cmd for that purpose: s3cmd rb --recursive s3://mybucket The advantage to this approach is that your users can do it, using just the regular S3 API. But this approach may be slow, particularly if you have previously created your objects with rest-bench , cosbench , or another benchmarking tool. So in the event that you want to remove buckets, and their objects, directly from radosgw , you can do so with radosgw-admin bucket rm --bucket=mybucket --purge-objects This is usually the faster approach. If, at any time, you want to nuke all buckets owned by a particular user, there is a command for that, as well. Use this one with care: radosgw - admin user rm --uid=[username] --purge-data This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/removing-buckets-in-radosgw-and-their-contents/","loc":"resources/hints-and-kinks/removing-buckets-in-radosgw-and-their-contents/"},{"title":"A minimal Ubuntu OpenStack Juju configuration in just four nodes","text":"Juju is Ubuntu's supported and preferred means of deployment automation for an OpenStack cloud. While in Juju, a deployment unit (a Juju charm ) generally expects to fully own the filesystem it is being deployed on, Juju allows you to co-deploy charms on the same physical machines, by way of using LXC containers. Now in general, Juju should allow you to deploy complex service bundles in one swoop, however this works best when deploying to the bare metal (i.e. without containers). Still, it is perfectly possible to automate Juju deployment of an entire OpenStack cloud in just 4 physical nodes: A controller node (running your OpenStack APIs and your dashboard); a compute node (running VMs under libvirt/KVM management); a network gateway node (providing L3 network connectivity); a storage node (providing Cinder volumes via iSCSI and LVM). The assumption for the setup below is that you already have a Juju infrastructure in place. You may have set this up with MAAS, or you may have just bootstrapped a deployment node and then created a Juju manual environment and added your 4 nodes via SSH. Note that the environment described here should not be used for production purposes. However, the same approach is also applicable to a 3-node controller HA cluster, 2-node Neutron gateway cluster with support for HA routers, and as many converged Ceph/ nova-compute nodes as you want. Juju configuration Consider the following Juju configuration YAML example, which you might put into your home directory as juju-config.yaml . keystone : openstack-origin : 'cloud:trusty-liberty' admin-password : 'my very secret password' nova-cloud-controller : openstack-origin : 'cloud:trusty-liberty' network-manager : Neutron neutron-gateway : openstack-origin : 'cloud:trusty-liberty' ext-port : eth2 bridge-mappings : 'external:br-ex' os-data-network : 192.168.133.0/24 instance-mtu : 1400 neutron-api : openstack-origin : 'cloud:trusty-liberty' network-device-mtu : 1400 # Always make sure you enable security groups neutron-security-groups : true overlay-network-type : vxlan rabbitmq-server : # Cinder is deployed in two parts: one for the API and scheduler # (which can live in a container), one for the volume service (which # cannot, at least not for the LVM/iSCSI backend) cinder-api : openstack-origin : 'cloud:trusty-liberty' enabled-services : api,scheduler cinder-volume : openstack-origin : 'cloud:trusty-liberty' enabled-services : volume # Adjust this to match the block device on your volume host block-device : vdb glance : openstack-origin : 'cloud:trusty-liberty' heat : openstack-origin : 'cloud:trusty-liberty' mysql : openstack-dashboard : openstack-origin : 'cloud:trusty-liberty' webroot : / nova-compute : openstack-origin : 'cloud:trusty-liberty' manage-neutron-plugin-legacy-mode : false # Change to qemu if in a nested cloud environment virt-type : kvm neutron-openvswitch : os-data-network : 192.168.133.0/24 Deployment Then, you can run the following shell script to deploy your control services to LXC containers on machine 1, nova-compute (and its subordinate charm, neutron-openvswitch ) to machine 2, neutron-gateway to machine 3, and cinder-volume to machine 4. #!/bin/bash -ex CONFIG = ~/juju-config.yaml juju deploy --config = $CONFIG mysql --to lxc:1 juju deploy --config = $CONFIG rabbitmq-server --to lxc:1 sleep 120s juju deploy --config = $CONFIG keystone --to lxc:1 juju add-relation keystone:shared-db mysql:shared-db juju deploy --config = $CONFIG glance --to lxc:1 juju add-relation glance:identity-service keystone:identity-service juju add-relation glance:shared-db mysql:shared-db juju deploy --config = $CONFIG neutron-api --to lxc:1 juju add-relation neutron-api:amqp rabbitmq-server:amqp juju add-relation neutron-api:identity-service keystone:identity-service juju add-relation neutron-api:shared-db mysql:shared-db juju deploy --config = $CONFIG neutron-gateway --to 3 juju add-relation neutron-gateway:amqp rabbitmq-server:amqp juju add-relation neutron-gateway:neutron-plugin-api neutron-api:neutron-plugin-api juju add-relation neutron-gateway:shared-db mysql:shared-db juju deploy --config = $CONFIG nova-cloud-controller --to lxc:1 juju add-relation nova-cloud-controller:amqp rabbitmq-server:amqp juju add-relation nova-cloud-controller:identity-service keystone:identity-service juju add-relation nova-cloud-controller:image-service glance:image-service juju add-relation nova-cloud-controller:neutron-api neutron-api:neutron-api juju add-relation nova-cloud-controller:shared-db mysql:shared-db juju deploy --config = $CONFIG nova-compute --to 2 juju add-relation nova-compute:amqp rabbitmq-server:amqp juju add-relation nova-compute:cloud-compute nova-cloud-controller:cloud-compute juju add-relation nova-compute:image-service glance:image-service juju add-relation nova-compute:shared-db mysql:shared-db juju deploy --config = $CONFIG neutron-openvswitch juju add-relation neutron-openvswitch:amqp rabbitmq-server:amqp juju add-relation neutron-openvswitch:neutron-plugin-api neutron-api:neutron-plugin-api juju add-relation neutron-openvswitch:neutron-plugin nova-compute:neutron-plugin juju deploy --config = $CONFIG cinder cinder-api --to lxc:1 juju add-relation cinder-api:amqp rabbitmq-server:amqp juju add-relation cinder-api:cinder-volume-service nova-cloud-controller:cinder-volume-service juju add-relation cinder-api:identity-service keystone:identity-service juju add-relation cinder-api:image-service glance:image-service juju add-relation cinder-api:shared-db mysql:shared-db juju deploy --config = $CONFIG cinder cinder-volume --to 4 juju add-relation cinder-volume:amqp rabbitmq-server:amqp juju add-relation cinder-volume:shared-db mysql:shared-db juju add-relation cinder-volume:image-service glance:image-service juju deploy --config = $CONFIG openstack-dashboard --to 1 juju add-relation openstack-dashboard:identity-service keystone:identity-service juju deploy --config = $CONFIG heat --to lxc:1 juju add-relation heat:amqp rabbitmq-server:amqp juju add-relation heat:identity-service keystone:identity-service juju add-relation heat:shared-db mysql:shared-db And you're done! The whole process should give you an OpenStack cloud in about 20-30 minutes. By the way, an exceedingly useful command to watch the installation progress of your Juju environment is: watch \"juju stat --format=tabular\" This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/ubuntu-openstack-juju-4-nodes/","loc":"resources/hints-and-kinks/ubuntu-openstack-juju-4-nodes/"},{"title":"A Python one-liner for pretty-printing radosgw utilization","text":"In case you need a quick overview of how many radosgw objects live in your Ceph cluster, your first step is normally this command: radosgw-admin bucket stats When used without the --bucket=<name> argument, this command lists a bunch of statistics for all your radosgw buckets, in a somewhat convoluted JSON format. If you only want a simple list of all your buckets and the number of objects they contain, you can use the following bit of Python list comprehension magic: radosgw-admin bucket stats | \\ python -c 'import json; import sys; print \"\\n\".join([\"%s: %s\" % (str(x[\"bucket\"]), \", \".join([\"%s: %s\" % (k, v[\"num_objects\"]) for k,v in x[\"usage\"].iteritems()])) for x in json.load(sys.stdin) if isinstance(x,dict)])' And while the above is all on one line so you can easily copy and paste, here are the Python bits in a slightly more legible format: import json import sys data = json . load ( sys . stdin ) print \" \\n \" . join ([ \" %s : %s \" % ( str ( x [ \"bucket\" ]), \", \" . join ([ \" %s : %s \" % ( k , v [ \"num_objects\" ]) for k , v in x [ \"usage\" ] . iteritems ()])) for x in data if isinstance ( x , dict )]) Of course, you'll need to substitute print() for print if your system runs only Python 3. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/radosgw-utilization-one-liner/","loc":"resources/hints-and-kinks/radosgw-utilization-one-liner/"},{"title":"Understanding radosgw benchmarks","text":"We've noticed that there are a few common misconceptions around radosgw performance, and we're hoping that this post can clear up some of those. radosgw is of course Ceph's RESTful object gateway. That means that you can use any client that speaks the Amazon S3 or OpenStack Swift protocol to interact with your Ceph cluster. Since RESTful object access is HTTP based, this also means you can combine radosgw with HTTP load balancers, reverse proxies and the like, which often comes in handy. In general, as any RESTful object storage, you would generally store data in radosgw that you read and write in one chunk, and where bulk storage is more important than online availability (if you need data at your fingertips, you'd use RBD or CephFS or even straight-up RADOS instead, but those are for different use cases). The performance implications of using radosgw (or any RESTful object storage, for that matter) usually apply to one of two different use cases: Either you want to store lots of data in bulk, and come back to it later. This, for example, is why in OpenStack backups of volumes and databases typically go to OpenStack Swift or radosgw speaking the Swift protocol. Or you want to store lots of relatively small data chunks really fast. Suppose you have a monitoring system storing data points in S3. So either you want to write big chunks of data, in which case you're interested in throughput (typically measured in amount of data per unit time, such as MB/s). Or you want to write small chunks, then what's important is completed operations per unit time (typically measured in number of writes per second, which in the RESTful case would be HTTP PUTs per second). Now with radosgw, you can measure this with a handy tool called rest-bench. Sadly rest-bench no longer builds with Ceph for Infernalis and later, because the Ceph developers now favor Intel's COSbench utility. But rest-bench from older Ceph releases will be around for a while and it's handy because unlike COSbench, it doesn't require Java. So let's take a look. The general invocation for rest-bench is like this: rest-bench -t $CONCURRENCY -b $SIZE \\ --seconds = $SECS --api-host = $RGW \\ --bucket = $BUCKET \\ --access-key = $KEY --secret = $SECRET \\ --no-cleanup write What does that mean? $CONCURRENCY is the number of concurrently running PUT operations. Basically, this is how many clients you want to simulate. The default is 16. $SIZE is the size of an individual object being written. The default here is 4MB. $RGW is of course your radosgw host including a port number. $SECS is the number of seconds to run the benchmark. The default is 60, but in order to get a quick idea of your radosgw performance, as little as 10 is often sufficient. $BUCKET is the scratch bucket where you're creating objects during the benchmark run. $ACCESS_KEY and $SECRET are the access and secret keys you created with radosgw-admin user create . write specifies a random write benchmark. --no-cleanup specifies that you don't want the bucket to be cleaned out after the benchmark run. It's normally fine to run several benchmarks in a row and only empty the benchmark bucket when done with all. Object size First, we'll examine how object size affects radosgw throughput and latency. So let's start out with a benchmark run that uses the default settings for concurrency and object sizes: export RGW = localhost:7480 export SECS = 10 export SIZE = $(( 1 << 22 )) # 4MB object size export BUCKET = bench export CONCURRENCY = 16 export KEY = your_radosgw_key export SECRET = your_radosgw_secret rest-bench -t $CONCURRENCY -b $SIZE --seconds = $SECS --api-host = $RGW \\ --bucket = $BUCKET --access-key = $KEY --secret = $SECRET --no-cleanup write host = localhost : 7480 Maintaining 16 concurrent writes of 4194304 bytes for up to 10 seconds or 0 objects [...] Total time run : 10 . 312134 Total writes made : 399 Write size : 4194304 Bandwidth ( MB / sec ) : 154 . 769 [...] So that means we achieved a bandwidth of just under 155 MB/s (which is near the max RADOS bandwidth this particular cluster is capable of; it's by no means a high-end system) and we managed 399 writes, or approx. 40 PUTs/s. Let's see how going even bigger changes things: $ export SIZE = $(( 1 << 26 )) # 64MB object size $ rest-bench -t $CONCURRENCY -b $SIZE --seconds = $SECS --api-host = $RGW \\ --bucket = $BUCKET --access-key = $KEY --secret = $SECRET --no-cleanup write host = localhost : 7480 Maintaining 16 concurrent writes of 67108864 bytes for up to 10 seconds or 0 objects [...] Total time run : 13 . 959088 Total writes made : 35 Write size : 67108864 Bandwidth ( MB / sec ) : 160 . 469 Perfectly logical. Our bandwidth doesn't change much, but of course the number of PUTs we get done per second decreases significantly, to a puny 3 PUTs/s. (Note: radosgw does break down objects into smaller chunks when it talks to RADOS. However, this doesn't change the fact that a client needs to haul a 64MB object across the network and through the radosgw HTTP server.) Let's do the opposite now, and go for smaller objects. Suppose your application is using a typical object size of 32K. export SIZE = $(( 1 << 15 )) # 32KB object size rest-bench -t $CONCURRENCY -b $SIZE --seconds = $SECS --api-host = $RGW \\ --bucket = $BUCKET --access-key = $KEY --secret = $SECRET --no-cleanup write host = localhost : 7480 Maintaining 16 concurrent writes of 32768 bytes for up to 10 seconds or 0 objects [...] Total time run : 10 . 042325 Total writes made : 2965 Write size : 32768 Bandwidth ( MB / sec ) : 9 . 227 [...] Of course if we looked at our bandwidth alone, this would be an abysmal result. But your application is trying to write 32K chunks, and lots of them. And it's succeeding just fine; we're now near 300 PUTs/s. Going even smaller, we'd expect PUTs/s to trend further up and nominal MB/s to go down. Let's try with 4K objects: export SIZE = $(( 1 << 12 )) # 4KB object size rest-bench -t $CONCURRENCY -b $SIZE --seconds = $SECS --api-host = $RGW \\ --bucket = $BUCKET --access-key = $KEY --secret = $SECRET --no-cleanup write host = localhost : 7480 Maintaining 16 concurrent writes of 4096 bytes for up to 10 seconds or 0 objects [...] Total time run : 10 . 052134 Total writes made : 3249 Write size : 4096 Bandwidth ( MB / sec ) : 1 . 263 [...] And sure enough, 325 PUTs/s. So in summary, larger object sizes increase your write bandwidth to your radosgw cluster, while smaller objects enable a higher writes-per-second load. Concurrency Another aspect that influences your radosgw performance is concurrency. Generally, the principle is simple: if you have multiple parallel applications that write to radosgw and that don't have to wait for each other, your aggregate throughput will be higher, and your writes-per-second will be higher as well. If you have a small number (in the worst case, a single one that is single-threaded) and you can only ever issue one PUT at a time, both throughput and writes-per-second will be lower in aggregate. export RGW = localhost:7480 export SECS = 10 export SIZE = $(( 1 << 22 )) # back to 4MB object size export BUCKET = bench export CONCURRENCY = 16 export KEY = <key> export SECRET = <secret> $ rest-bench -t $CONCURRENCY -b $SIZE --seconds = $SECS --api-host = $RGW \\ --bucket = $BUCKET --access-key = $KEY --secret = $SECRET --no-cleanup write host = localhost : 7480 Maintaining 16 concurrent writes of 4194304 bytes for up to 10 seconds or 0 objects [...] Total time run : 10 . 294444 Total writes made : 394 Write size : 4194304 Bandwidth ( MB / sec ) : 153 . 092 [...] export CONCURRENCY = 1 rest-bench -t $CONCURRENCY -b $SIZE --seconds = $SECS --api-host = $RGW \\ --bucket = $BUCKET --access-key = $KEY --secret = $SECRET --no-cleanup write host = localhost : 7480 Maintaining 1 concurrent writes of 4194304 bytes for up to 10 seconds or 0 objects [...] Total time run : 10 . 090768 Total writes made : 147 Write size : 4194304 Bandwidth ( MB / sec ) : 58 . 271 Logical, right? Rather than allowing 16 threads to interact with the cluster in parallel, we now have to wait for every single PUT to complete before we can issue the next. Pretty obvious to see both our writes-per-second and our aggregate bandwidth to drop by more than half. The effect is even slightly less pronounced with smaller objects. Compare the two for 4KB objects: export SIZE = $(( 1 <<12)) # 4KB object size export CONCURRENCY=1 6 rest-bench - t $CONCURRENCY - b $SIZE -- seconds = $SECS -- api-host = $RGW \\ -- bucket = $BUCKET -- access-key = $KEY -- secret = $SECRET -- no-cleanup write host = localhost : 7480 Maintaining 16 concurrent writes of 4096 bytes for up to 10 seconds or 0 objects [...] Total time run : 10 . 053976 Total writes made : 3211 Write size : 4096 Bandwidth ( MB / sec ) : 1 . 248 [...] export CONCURRENCY = 1 rest-bench -t $CONCURRENCY -b $SIZE --seconds = $SECS --api-host = $RGW \\ --bucket = $BUCKET --access-key = $KEY --secret = $SECRET --no-cleanup write host = localhost : 7480 Maintaining 1 concurrent writes of 4096 bytes for up to 10 seconds or 0 objects [...] Total time run : 10 . 007962 Total writes made : 1632 Write size : 4096 Bandwidth ( MB / sec ) : 0 . 637 [...] Both writes-per-second and throughput drop by half. Conclusions Note: If you've dealt with storage performance considerations before, some of these will be blindingly obvious. Apologies for that; it just shows that Ceph is generally a well-behaved system that does what you would normally expect. Larger objects have less overhead, and as such increase your throughput, Smaller objects increase writes-per-second at the expense of aggregate throughput, because they have more overhead, Serialization and contention (both of which mean reduced concurrency) reduce your data throughput and your writes-per-second. What does this mean for your radosgw application? Concurrency is good. If your application can fire a bunch of RESTful objects at radosgw, which don't have to wait for each other, great. If you need to optimize for lots of PUTs per second, make sure that your application sends data in reasonably sized chunks. And again, make sure it is capable of doing so in parallel. If you need to optimize for throughput instead, make sure that your application coalesces data into large objects. There is a big difference between sending one object of 10MB, and 10 objects of 1 MB. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/understanding-radosgw-benchmarks/","loc":"resources/hints-and-kinks/understanding-radosgw-benchmarks/"},{"title":"OpenStack for Open edX: Inside and Out (SWITCH ICT-Focus 2015)","text":"My presentation at SWITCH ICT-Focus 2015. Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/ictfocus2015/","loc":"resources/presentations/ictfocus2015/"},{"title":"Clusters, Routers, Agents and Networks: High Availability in Neutron","text":"Of everything that we can build and deploy in a highly-available fashion in OpenStack, deploying highly available networking has been one of the trickiest, most complex aspects to get right. I team up with Adam Spiers (SUSE) and Assaf Muller (Red Hat) to discuss high availability in OpenStack Neutron. Video: YouTube Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/openstacksummit2015-tokyo-neutron-ha/","loc":"resources/presentations/openstacksummit2015-tokyo-neutron-ha/"},{"title":"Automated OpenStack deployment: A comparison","text":"From the 2015 OpenStack Summit in Tokyo. A comparison of automated deployment tools for OpenStack. OSP Director on RHEL OSP 7 Juju on Ubuntu Trusty Chef/Crowbar on SUSE OpenStack Cloud 5 Ansible on Rackspace Private Cloud Fuel on Mirantis OpenStack About 45 minutes. Video: YouTube Slides: GitHub Use the arrow keys to navigate through the presentation, hit Esc to zoom out for an overview, or just advance by hitting the spacebar. This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/automated-openstack-deployment-a-comparison/","loc":"resources/presentations/automated-openstack-deployment-a-comparison/"},{"title":"Open edX","text":"Open edX is an extensible, highly scalable, open-source learning management platform. Originally conceived at MIT and Harvard University , the edx.org platform it is currently backed by more than 70 organizations world-wide and serves more than 4 million students. Since 2013, the software running edx.org is available under an open source software license, enabling private and public organizations to run and operate the same learning platform under the Open edX initiative. hastexo's contribution to Open edX is twofold: first, we enabled Open edX to run in an automated, distributed fashion on the OpenStack platform. Then, we taught Open edX the ability to automate the orchestration of on-demand OpenStack training environments. Both are now an integral part of our hastexo Academy service offering. Want to find out how you can use Open edX and OpenStack to your advantage? Check out the short animation below (with audio) to find out. Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/open-edx/","loc":"resources/presentations/open-edx/"},{"title":"Manageable Application Containers: Lightning Quick Updates, Scaleable Security, Easy High Availability","text":"From LinuxCon Europe 2015 in Dublin. An alternative approach to managing application containers. Slides: GitHub Use the arrow keys to navigate through the presentation, hit Esc to zoom out for an overview, or just advance by hitting the spacebar. This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/manageable-application-containers/","loc":"resources/presentations/manageable-application-containers/"},{"title":"OpenStack Orchestration and Automation","text":"My presentation from OpenStack Israel 2015. A fast-paced introduction to cloud-init and OpenStack Heat. Video: YouTube Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/osil2015-orchestration/","loc":"resources/presentations/osil2015-orchestration/"},{"title":"Ceph Tech Talk: Placement Groups","text":"A Ceph Tech Talk on the ins and outs of Ceph Placement Groups (PGs). Special thanks to Patrick McGarry for inviting me to speak on a Ceph Tech Talk. For the slide deck, use the PgUp/PgDown keys to navigate, or just advance by hitting the spacebar. Video: YouTube Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/ceph-tech-talk-pg/","loc":"resources/presentations/ceph-tech-talk-pg/"},{"title":"Have Data, Want Scale, Indefinitely: Exploring Ceph","text":"An introduction to Ceph (with audio). For the slide deck, use the PgUp/PgDown keys to navigate, or just advance by hitting the spacebar. For audio narration, just click the icon in the bottom-left corner and the presentation will auto-advance in step with the narration. Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/ceph-intro/","loc":"resources/presentations/ceph-intro/"},{"title":"Ceph Performance Demystified: Benchmarks, Tools, and the Metrics that Matter","text":"Mystified about Ceph performance tuning and benchmarking? Don't despair! This presentation was given at Ceph Day London in 2014. Slides: GitHub Video (courtesy of the Ceph team at Red Hat): YouTube This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/ceph-day-london/","loc":"resources/presentations/ceph-day-london/"},{"title":"OpenStack High Availability: Are We There Yet?","text":"My presentation from LinuxCon Europe 2014, outlining the state of high availability in OpenStack. Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/lceu2014-openstack-ha/","loc":"resources/presentations/lceu2014-openstack-ha/"},{"title":"Speak! How to talk in public and not wreck your voice","text":"An Ignite talk I presented at OSCON 2014. Decidedly non-technical, this covers vocal hygiene and how to take care of your voice. Video: YouTube","tags":"presentations","url":"resources/presentations/speak-how-to-talk-in-public-and-not-wreck-your-voice/","loc":"resources/presentations/speak-how-to-talk-in-public-and-not-wreck-your-voice/"},{"title":"Hands On Trove: Database as a Service in OpenStack","text":"This tutorial covered OpenStack Trove at Percona Live 2014. If you want to recreate the experience, read on! In order to make the most of this tutorial, you can recreate the interactive steps presented. Please note: the process, while simple, is extremely bandwidth intensive and you don't want to be the bandwidth hog that everyone hates in your hotel, or on a conference wifi. Do so in your office (or home) instead. The set-up process is decribed in a brief README . Effectively, it boils down to cloning a Git repo and then running vagrant up, and you'll be good to go. But do pay attention to the system requirements. Slides: SlideShare This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/hands-trove-database-service-openstack/","loc":"resources/presentations/hands-trove-database-service-openstack/"},{"title":"Fun with extended attributes in Ceph Dumpling","text":"This is a rather nasty bug in Ceph OSD, affecting 0.67 \"Dumpling\" and earlier releases. It is fixed in versions later than 0.70, and a simple workaround is available, but when it hits, this issue can be pretty painful. Please read this post to the end. This is by no means a punch being thrown at Ceph, in fact it rather clearly illustrates a very sane choice that the Ceph developers have made. If you run Ceph Emperor or later, you are not affected by this issue, but it will be an interesting read in data integrity in distributed systems anyway. Too much of a good thing: large extended attributes Here is how to reproduce the problem in a very simple bit of Python code, against Ceph Dumpling. Do not run this on a production system. Don't. Ever. #!/usr/bin/python # import rados with rados . Rados ( conffile = '/etc/ceph/ceph.conf' ) as cluster : with cluster . open_ioctx ( 'test' ) as ioctx : o = rados . Object ( ioctx , 'onebyte' ) # Write one byte as the object content o . write ( 'a' ) print ( 'Wrote object' ) # Write an attribute of 8M o . set_xattr ( 'val' , 'a' * 8 * 1024 ** 2 ) print ( 'Set large attribute' ) # Retrieving an attribute by name should succeed a = o . get_xattr ( 'val' ) print ( 'Retrieved large attribute' ) # Walking the attribute list should fail try : alist = [ i for i in o . get_xattrs () ] print ( 'Retrieved whole attribute list' ) except rados . Error : print ( 'Failed to retrieve attribute list. ' 'Congratulations, you probably just ' 'corrupted one of your PGs.' ) raise Removing the disabling comment character is left as an exercise for the daring reader, just in case your cut & paste trigger finger is itchy. Do not run this against a production system. So what are we doing here? We're creating a single RADOS object named onebyte in a pool called test . It is, as the name implies, only one byte long (it contains just the letter a), but it has a very long attribute named val , which is 8 Megabytes' worth of a ‘s. (In case you're wondering: yes, there are applications that set very large attributes on RADOS objects. radosgw is one of them.) Since you've been able to set the attribute, you can also retrieve it, which is why the call to get_xattr('val') succeeds just fine. But if you fetch the entire attribute list (with get_xattrs ), then you run into an E2BIG error. You can confirm that on the Linux command line, using the rados utility, just the same. First, getting the object and getting an xattr by name: $ sudo rados -p test get onebyte - a $ sudo rados -p test getxattr onebyte val - 2 > & 1 | head -c 50 aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa Obviously, you're welcome to omit the head redirection if you prefer to flood your screen. But for proving we can still retrieve the attribute value, 50 characters is quite sufficient. Let's try listing the attributes, though: $ sudo rados -p test listxattr onebyte error getting xattr set test/onebyte: Argument list too long Oops. Argument list too long is bash's way of translating the E2BIG error for you, because that's what it usually means. In this case, though, it's actually what we get from the rados utility, and that gets it from the OSD it's talking to, and that gets it from the filesystem. Digging deeper Now let's take a look where this object is stored. $ sudo ceph osd map test onebyte $ osdmap e191 pool 'test' ( 3 ) object 'onebyte' -> pg 3 .ed47d009 ( 3 .1 ) -> up [ 0 ,2 ] acting [ 0 ,2 ] So it 's PG 3.1, currently mapped to OSDs 0 (primary) and 2 (replica). We happen to be on the very host where OSD 0 is running, so let' s take a closer look: $ sudo getfattr -d /var/lib/ceph/osd/ceph-0/current/3.1_head/onebyte__head_ED47D009__3 /var/lib/ceph/osd/ceph-0/current/3.1_head/onebyte__head_ED47D009__3: Argument list too long Same thing, E2BIG. Sure, if we can't enumerate the attributes ourselves, the OSD can't either. But it's still fairly benign, because we can still retrieve the object, right? Adding daemon failure Well, not so much. Let's see what happens if one of our OSDs gets restarted. This is a perfectly benign operation that Ceph is expected to (and does) handle very gracefully. $ sudo restart ceph-osd id = 0 ceph-osd ( ceph/0 ) start/running, process 7922 $ sudo rados -p test get onebyte - a The object is still there. What if, incidentally, the other OSD also happens to go down some time later, and stays down? $ sudo stop ceph-osd id = 2 ceph-osd ( ceph/2 ) stop/waiting $ sudo ceph osd out 2 marked out osd.2. Remember, \"at scale, something always fails\" . Ceph is built for exactly that, and its algorithms deal with this type of failure in stride. So at this point, we would expect Ceph to remap the PGs that were previously on OSD 2 to OSD 1, and synchronize with OSD 0. And a few minutes later, all hell breaks loose: sudo ceph -s cluster bd70ea39-58fc-4117-ade1-03a4d429cb49 health HEALTH_WARN 200 pgs degraded ; 1 pgs recovering ; 200 pgs stuck unclean ; recovery 2 / 2 degraded ( 100 . 000 %); 1 / 1 unfound ( 100 . 000 %) monmap e4 : 3 mons at { ubuntu-ceph1=192.168.122.201:6789/0,ubuntu-ceph2=192.168.122.202:6789/0,ubuntu-ceph3=192.168.122.203:6789/0 } , election epoch 180 , quorum 0 , 1 , 2 ubuntu-ceph1 , ubuntu-ceph2 , ubuntu-ceph3 osdmap e237 : 3 osds : 1 up , 1 in pgmap v1335 : 200 pgs : 1 active + recovering + degraded , 199 active + degraded ; 1 bytes data , 38684 KB used , 5071 MB / 5108 MB avail ; 2 / 2 degraded ( 100 . 000 %); 1 / 1 unfound ( 100 . 000 %) mdsmap e1 : 0 / 0 / 1 up Fighting a fire Wow. We shut down only one OSD (OSD 2), the other one (OSD 0) was merely restarted, but it has crashed in the interim. Its mon osd down out interval has also expired, so it has been marked out as well. All of our PGs are stuck degraded, one has an unfound object (that's the one whose xattrs can no longer be enumerated). Yikes. We scramble to bring our just-shutdown OSD back in. $ sudo start ceph-osd id = 2 ceph-osd ( ceph/2 ) start/running, process 7426 $ sudo ceph osd in 2 marked in osd.2. Does this make things better? $ sudo ceph - w cluster bd70ea39 - 58 fc - 4117 - ade1 - 03 a4d429cb49 health HEALTH_WARN 200 pgs degraded ; 1 pgs recovering ; 200 pgs stuck unclean ; recovery 2 / 2 degraded ( 100.000 % ) monmap e4 : 3 mons at { ubuntu - ceph1 = 192.168.122.201 : 6789 / 0 , ubuntu - ceph2 = 192.168.122.202 : 6789 / 0 , ubuntu - ceph3 = 192.168.122.203 : 6789 / 0 } , election epoch 180 , quorum 0 , 1 , 2 ubuntu - ceph1 , ubuntu - ceph2 , ubuntu - ceph3 osdmap e243 : 3 osds : 2 up , 2 in pgmap v1343 : 200 pgs : 1 active + recovering + degraded , 199 active + degraded ; 1 bytes data , 38812 KB used , 5071 MB / 5108 MB avail ; 2 / 2 degraded ( 100.000 % ) mdsmap e1 : 0 / 0 / 1 up 2014 - 02 - 11 19 : 09 : 56.868771 mon .0 [ INF ] osdmap e242 : 3 osds : 2 up , 2 in 2014 - 02 - 11 19 : 09 : 56.895559 mon .0 [ INF ] pgmap v1342 : 200 pgs : 1 active + recovering + degraded , 199 active + degraded ; 1 bytes data , 38812 KB used , 5071 MB / 5108 MB avail ; 2 / 2 degraded ( 100.000 % ) 2014 - 02 - 11 19 : 09 : 57.901188 mon .0 [ INF ] osdmap e243 : 3 osds : 2 up , 2 in 2014 - 02 - 11 19 : 09 : 57.918612 mon .0 [ INF ] pgmap v1343 : 200 pgs : 1 active + recovering + degraded , 199 active + degraded ; 1 bytes data , 38812 KB used , 5071 MB / 5108 MB avail ; 2 / 2 degraded ( 100.000 % ) 2014 - 02 - 11 19 : 09 : 59.920149 mon .0 [ INF ] osdmap e244 : 3 osds : 1 up , 2 in 2014 - 02 - 11 19 : 09 : 59.931825 mon .0 [ INF ] pgmap v1344 : 200 pgs : 1 active + recovering + degraded , 199 active + degraded ; 1 bytes data , 38812 KB used , 5071 MB / 5108 MB avail ; 2 / 2 degraded ( 100.000 % ) 2014 - 02 - 11 19 : 10 : 00.940319 mon .0 [ INF ] osd .2 192.168.122.203 : 6800 / 8362 boot 2014 - 02 - 11 19 : 10 : 00.940987 mon .0 [ INF ] osdmap e245 : 3 osds : 2 up , 2 in 2014 - 02 - 11 19 : 10 : 00.954275 mon .0 [ INF ] pgmap v1345 : 200 pgs : 1 active + recovering + degraded , 199 active + degraded ; 1 bytes data , 38812 KB used , 5071 MB / 5108 MB avail ; 2 / 2 degraded ( 100.000 % ) 2014 - 02 - 11 19 : 10 : 01.960942 mon .0 [ INF ] osdmap e246 : 3 osds : 2 up , 2 in 2014 - 02 - 11 19 : 10 : 01.975509 mon .0 [ INF ] pgmap v1346 : 200 pgs : 1 active + recovering + degraded , 199 active + degraded ; 1 bytes data , 38812 KB used , 5071 MB / 5108 MB avail ; 2 / 2 degraded ( 100.000 % ) 2014 - 02 - 11 19 : 10 : 03.982202 mon .0 [ INF ] osdmap e247 : 3 osds : 1 up , 2 in 2014 - 02 - 11 19 : 10 : 03.994963 mon .0 [ INF ] pgmap v1347 : 200 pgs : 1 active + recovering + degraded , 199 active + degraded ; 1 bytes data , 38812 KB used , 5071 MB / 5108 MB avail ; 2 / 2 degraded ( 100.000 % ) 2014 - 02 - 11 19 : 10 : 05.005162 mon .0 [ INF ] osd .2 192.168.122.203 : 6800 / 8483 boot 2014 - 02 - 11 19 : 10 : 05.005386 mon .0 [ INF ] osdmap e248 : 3 osds : 2 up , 2 in Hardly. OSDs flapping right and left. Ouch ouch ouch. Desperation: not your friend OK, let's try to do something really terrible and get rid of that file manually. $ sudo ceph osd map test onebyte osdmap e254 pool 'test' ( 3 ) object 'onebyte' -> pg 3 .ed47d009 ( 3 .1 ) -> up [ 1 ] acting [ 1 ] So it's mapped to OSD 1 now, which is expected. Let's take a look and see if we can find and remove it. ceph @ ubuntu - ceph2 : ~$ ls / var / lib / ceph / osd / ceph - 1 / current / 3.1 _head / ceph @ ubuntu - ceph2 : ~$ An empty directory. Well of course, they could never actually peer, so the data never got synchronized. So there's pretty much one thing left. ceph @ ubuntu - ceph3 : ~$ sudo stop ceph - osd id = 2 stop : Unknown instance : ceph / 2 ceph @ ubuntu - ceph3 : ~$ sudo rm / var / lib / ceph / osd / ceph - 2 / current / 3.1 _head / onebyte__head_ED47D009__3 ceph @ ubuntu - ceph3 : ~$ sudo start ceph - osd id = 2 ceph - osd ( ceph / 2 ) start / running , process 9069 ceph @ ubuntu - ceph1 : ~$ sudo stop ceph - osd id = 0 stop : Unknown instance : ceph / 0 ceph @ ubuntu - ceph1 : ~$ sudo rm / var / lib / ceph / osd / ceph - 0 / current / 3.1 _head / onebyte__head_ED47D009__3 ceph @ ubuntu - ceph1 : ~$ sudo start ceph - osd id = 0 ceph - osd ( ceph / 0 ) start / running , process 9485 There. Shut down the OSDs, nuked the files, brought the OSDs back up. A Fire Contained And after a few more seconds, finally: $ sudo ceph -s cluster bd70ea39-58fc-4117-ade1-03a4d429cb49 health HEALTH_OK monmap e4: 3 mons at { ubuntu-ceph1 = 192 .168.122.201:6789/0,ubuntu-ceph2 = 192 .168.122.202:6789/0,ubuntu-ceph3 = 192 .168.122.203:6789/0 } , election epoch 180 , quorum 0 ,1,2 ubuntu-ceph1,ubuntu-ceph2,ubuntu-ceph3 osdmap e259: 3 osds: 3 up, 3 in pgmap v1367: 200 pgs: 200 active+clean ; 1 bytes data, 122 MB used, 15204 MB / 15326 MB avail mdsmap e1: 0 /0/1 up Whew. $ sudo rados -p test get onebyte - error getting test/onebyte: No such file or directory Now obviously the offending object is gone, which is ugly and we could have manually recreated that file and set some magic user.ceph attributes enabling us to keep the object, but in this case we just didn't care and wanted our cluster back up and running as soon as possible. Prevention So we have a brutal cure for this problem that is roughly akin to performing brain surgery with a fork and spoon. What could we have done better? LevelDB to the rescue. Ceph optionally (and in later versions, by default) stores attributes that would overflow the filesystem xattr store in a separate database called an omap, using Google's embedded LevelDB database. And in Dumpling, this feature is disabled by default – with an exception for ext3/4, which have interesting attribute limitations themselves. This is the all-important option that needs to go in your ceph.conf: filestore xattr use omap = true You can enable this on a running cluster and this will retain and preserve any xattrs previously set on RADOS objects. Attributes mapped to file xattrs will simply be moved to the omap database (note however that the opposite is not true, but you'll never want to disable this option anymore, anyway). As of this Ceph commit (which went into Ceph 0.70), the option is no longer available and is always treated as if set to true, so those versions are not affected by the issue described in this post. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/fun-extended-attributes-ceph-dumpling/","loc":"resources/hints-and-kinks/fun-extended-attributes-ceph-dumpling/"},{"title":"Unrecoverable unfound objects in Ceph 0.67 and earlier","text":"As Ceph author Sage Weil points out frequently, distributed storage solutions for all their goodness have a \"dirty little secret\" : No matter just how redundant and reliable they are by design, a bug in the storage software itself can be a real issue. And occasionally, the bug doesn't have to be in the storage software itself. Every self-respecting Linux file system supports extended file attributes (\"xattrs\") , and XFS (commonly used with Ceph OSDs) is no exception. When OSDs store RADOS objects in the OSD filestore, they make heavy use of key-value pairs. To do so, they can employ two approaches: storing key-value pairs in filesystem xattrs directly (inline xattrs); storing them in a separate key-value store known as an object map or omap (based on Google LevelDB . RADOS generally expects that the maximum xattr size on a file is practically unlimited, so if your filestore is on a filesystem where that is not the case (such as ext4), you would generally use omaps. Enabling the use of omaps is easy enough. This goes in your ceph.conf: [osd] filestore xattr use omap = true Ceph releases since 0.66 will enable this automatically if the filestore is determined to be running on ext4. But for the XFS and BTRFS filesystem, the general recommendation (and default behavior) remained to just use inline xattrs. This is also true for the stable Ceph \"Dumpling\" release (0.67). Since Ceph 0.70, the configuration option has been dropped and Ceph since always behaves as if filestore xattr use omap was set to true . Now there is a reason for that, and it is a bit trickier than you might expect. When manipulating extended attributes, applications (including ceph-osd) make use of the getxattr() , setxattr() , and listxattr() syscalls . Expectedly, these syscalls retrieve, set, and enumerate extended attributes set on a file. Now it is actually possible to set so many keys, or so large values, that while getxattr() and setxattr() executed on a specific file continue to work just fine, listxattr() returns with -E2BIG . Now it turns out that radosgw can actually set attribute lists that large, and ceph-osd will fail if it cannot determine the file attributes for a file under its control. When this happens, the object shows as unfound in ceph health detail , and sadly, the documented operation to recover unfound objects fails. The affected Placement Group (PG) also remains stuck, again being reported as such in ceph health detail. If you actually have run into this problem, you should really call Inktank for support. (You can also give us a call, of course, and we'll be happy to help you confirm the problem. But we will refer you to Inktank for the actual fix – we don't fiddle and mess around with RADOS object internals, and neither should you.) How to avoid this in the first place? If you're on Ceph 0.70 or later, congratulations. You should be safe, as omaps are enabled and anything that would overflow your xattrs instead gets stored in an omap. If you're on any earlier version, including the currently stable 0.67.x \"Dumpling\" series, enable filestore xattr use omap. Do it now, regardless of what filesystem your OSDs run on. Then restart your OSDs one by one; your existing xattrs won't get lost. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/unrecoverable-unfound-objects-ceph-067-and-earlier/","loc":"resources/hints-and-kinks/unrecoverable-unfound-objects-ceph-067-and-earlier/"},{"title":"linux.conf.au 2014, or My Annual Journey To Awesome","text":"Earlier this month, I got on a flight to Perth, WA to make my annual trek to linux.conf.au , the largest open-source conference in the Southern Hemisphere and one of my favorite conferences on the circuit. LCA is an excellent, volunteer-run, hugely insightful conference and well worth the 26-hour trip. I arrived in Perth in time to catch part of the Systems Administration mini-conference on day 1. In particular, I caught a systemd talk by Rodger Donaldson which was both informative and entertaining, so I encourage you to watch that when you get the chance. Day 2 had the OpenStack miniconf, which was good but a bit too focused on OpenStack governance and organizational issues in its first half. In the afternoon, talks got more technical, which was a clear improvement. Wednesday talks included a highly entertaining look at MySQL's history from Stewart Smith , Dave Chinner ‘s musings on where Linux filesystems came from, and an insightful RADOS deep dive from Sage Weil. There was also a DRBD talk that I was not so fond of, but I've already shared my thoughts about that one on Google+, and you're certainly welcome to take a look over there. My own Rapid OpenStack Deployment for Novices and Experts Alike tutorial was on Thursday, had a nice turnout, and my hands-on stuff all worked! What more could I possibly ask for? Besides my own talk, you should also totally watch Matthew Garrett ‘s Thursday keynote and Lana Brindley ‘s agile documentation tutorial with Lego goodness . And of course, my clear favorite among all LCA talks this year, Bdale Garbee's reflections on losing his house in a wildfire last year. As for Friday, again a stellar keynote from Jonathan Oxer ( Arduino satellites in space , geek overload), Lennart's kdbus talk which you've probably already read on LWN about, and a brilliant lightning talk from Tim Serong about building a DIY bookscanner. All in all, linux.conf.au in Perth was terrific, as usual, and I am absolutely planning to be there again next year. The trip will be even more atrociuous as next year's venue is Auckland , but it will be totally worth it. No doubt in my mind at all. I'd like to extend a huge thank-you to all LCA attendees, fellow speakers, organizers and volunteers who make this conference a fantastic event year after year. See you in 2015! This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2014/01/20/linuxconfau-2014-or-my-annual-journey-to-awesome/","loc":"blog/2014/01/20/linuxconfau-2014-or-my-annual-journey-to-awesome/"},{"title":"Greetings from Havana: A fresh perspective on globally distributed OpenStack","text":"My second appearance at OpenStack Israel, this time with news on distributed OpenStack environments in the Havana release. The video (courtesy of OpenStack Israel) and slides are below. For the introduction given in Hebrew, the slides contain a transcript in English. Video: YouTube Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/greetings-from-havana/","loc":"resources/presentations/greetings-from-havana/"},{"title":"Ceph: object storage, block storage, file system, replication, massive scalability and then some!","text":"This is one of my most popular talks, co-presented with intrepid cartoonist-turned-engineer Tim Serong from SUSE . Video: YouTube This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/ceph-object-storage-block-storage-file-system-replication-massive-scalabilit/","loc":"resources/presentations/ceph-object-storage-block-storage-file-system-replication-massive-scalabilit/"},{"title":"Ceph: The Storage Stack for OpenStack","text":"My presentation from OpenStack Israel , May 27, 2013. After a brief introduction into Ceph, I dive into OpenStack specific Ceph features and outlines RBD integration with Glance and Cinder, and explains RadosGW Swift compatibility. Use the PgUp/PgDown keys to navigate through the presentation, or just advance by hitting the spacebar. Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/ceph-storage-stack-openstack/","loc":"resources/presentations/ceph-storage-stack-openstack/"},{"title":"Enter the cuttlefish!","text":"Today, the developers released Ceph 0.61, codenamed cuttlefish. There are some interesting features in this new release, take a look. One thing that will undoubtedly make Ceph a lot more palatable to RHEL/CentOS users is the availability of Ceph in EPEL . This was originally announced in late March , but 0.61 is the first supported release that comes with Red Hat compatible RPMs. Note that at the time of writing, EPEL is obviously still stuck on the 0.56 bobtail release , but it is expected that cuttlefish support will follow shortly. In the interim, cuttlefish packages are available outside EPEL, on the ceph.com yum repo . This allows you to run a Ceph cluster on RHEL/CentOS. It does, however come with a few limitations: You can't use RBD from a kvm/libvirt box that is running RHEL. RHEL does not ship with librados support enabled in the qemu-kvm builds, and removing this limitation would mean for third parties to provide their own libvirt/kvm build. As of today, tough, no RBD-support libvirt/kvm lives in CentOS Plus . You can't use the kernel rbd or ceph modules from a client that is running RHEL. RBD and Ceph filesystem support is absent from RHEL kernels. I'm curious to see if and when that will change, given Red Hat's focus on GlusterFS as their preferred distributed storage solution. It will be interesting to see what happens there. Another neat little new feature is the ability to set quotas on pools, which is something that we've frequently had customers ask for in our consulting practice. Then there are incremental snapshots for RBD, another really handy feature for RBD management in cloud solutions like OpenStack . There's more, and you may head over to the press release and the Inktank blog for more details. And then you might want to mark your calendars for one of the following events: At the OpenStack DACH Day at LinuxTag in Berlin on May 24 , Wolfgang Schulze from Inktank gives an overview about Ceph (in German, register here ). At OpenStack Israel on May 27, I'll be speaking about Ceph integration with OpenStack (in English, register here ). And at OpenStack CEE on May 29 in Budapest, Martin speaks about Scale-out Made Easy: Petabyte Storage with Ceph (in English, register here ). All these events are expected to sell out beforehand, and they are only a couple of weeks away. So make sure you grab your seat, and we'll see you there! This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2013/05/07/enter-the-cuttlefish/","loc":"blog/2013/05/07/enter-the-cuttlefish/"},{"title":"More Reliable, More Resilient, More Redundant","text":"Another update on OpenStack's progress in high availability, for the Grizzly and Havana releases. Presented at the OpenStack Summit in Portland, on April 17, 2013. I give an overview of infrastructure, compute and networking high availability development in the April 2013 OpenStack Grizzly release, and an outlook for OpenStack Havana. Use the PgUp/PgDown keys to navigate through the presentation, or just advance by hitting the spacebar. Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/more-reliable-more-resilient-more-redundant/","loc":"resources/presentations/more-reliable-more-resilient-more-redundant/"},{"title":"High Availability Update: You can now vote our talk into the OpenStack summit!","text":"For the upcoming OpenStack Summit in Portland, Syed Armani and I have submitted a talk on OpenStack high availability. Here's how you can make sure it makes it into the program. Our talk, More reliable, more resilient, more redundant: High Availability Update for Grizzly and beyond, is an extended overview about current and future high availability features in OpenStack. It covers infrastructure high availability, HA features in Nova, Quantum, and several other topics. I've given a shorter version of this talk just this week, at the 2nd Swiss OpenStack User Group meetup, where apparently around 55 people liked it a lot. You can take a look at the slides here, and there will also be a video that should be available later this week. So, to make sure that this talk makes it into the Summit, we need your help! Voting for Summit sessions is up, and you can vote for our talk here. Please note, you must be an OpenStack Foundation member to vote. If you're not, and you're into OpenStack, you can join (for free!) as an Individual Member. Then, you can immediately proceed to the voting page to cast your vote. Thanks for your support, and we hope to see you in Portland! This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2013/02/21/high-availability-update-you-can-now-vote-our-talk-into-the-openstack-summit/","loc":"blog/2013/02/21/high-availability-update-you-can-now-vote-our-talk-into-the-openstack-summit/"},{"title":"High Availability Update (Grizzly and Havana)","text":"Another update on OpenStack's progress in high availability, for the Grizzly and Havana releases. Presented at the Swiss OpenStack User Group meetup in Zurich, on February 19, 2013. I give an overview of infrastructure, compute and networking high availability development in the run-up to the Grizzly feature freeze. Use the PgUp/PgDown keys to navigate through the presentation, or just advance by hitting the spacebar. Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/high-availability-update-grizzly-and-havana/","loc":"resources/presentations/high-availability-update-grizzly-and-havana/"},{"title":"My week at linux.conf.au 2013","text":"linux.conf.au 2013 has kicked off this morning, and here is a brief preview of my talks and related activities this week. This afternoon, as part of the Cloud, Distributed Storage and High Availability miniconf, I am moderating the Grand Distributed Storage Debate . In this debate, we'll have Sage Weil (for Ceph) and John Mark Walker (for GlusterFS) go head-to-head about the merits of their respective projects. On Tuesday, during the OpenStack miniconf, I'm doing a talk on integrating Ceph with OpenStack. Finally, on Friday afternoon, Tim Serong and I are scheduled to do a full hands-on Ceph tutorial. Watch this space, we'll announce where to download your VM images no later than Wednesday (as soon as we've figured out just where in the conference network we can upload it). This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2013/01/28/my-week-at-linuxconfau-2013/","loc":"blog/2013/01/28/my-week-at-linuxconfau-2013/"},{"title":"Solid-state drives and Ceph OSD journals","text":"Object Storage Daemons ( OSDs ) are the Ceph stack's workhorses for data storage. They're significantly smarter than many of their counterparts in distributed block-storage solutions (open source or not), and their design is instrumental in securing the stack's reliability and scalability. Among other things, OSDs are responsible for the decentralized replication — which is highly configurable — of objects in the store. They do so in a primary-copy fashion: every Ceph object (more precisely, the Placement Group it is a part of) is written to the primary OSD first, and from there replicates to one or several replica OSDs to ensure redundancy. This replication is synchronous, such that a new or updated object guarantees its availability (in the way configured by the cluster administrator) before an application is notified that the write has completed. More specifically, in order for an OSD to acknowledge a write as completed, the new object must have been written to the OSD's journal. OSDs use a write-ahead mode for local operations: a write hits the journal first, and from there is then being copied into the backing filestore. (Note: if your filestore is using btrfs, the journal is applied in parallel with the filestore write instead. Btrfs still being experimental, however, this is not a configuration often used in production.) Thus, for best cluster performance it is crucial that the journal is fast, whereas the filestore can be comparatively slow. This, in turn, leads to a common design principle for Ceph clusters that are both fast and cost-effective: Put your filestores on slow, cheap drives (such as SATA spinners), put your journals on fast drives (SSDs, Fusion-IO cards, whatever you can afford). Another common design principle is that you create one OSD per spinning disk that you have in the system. Many contemporary systems come with only two SSD slots, and then as many spinners as you want. That is not a problem for journal capacity — a single OSD's journal is usually no larger than about 6 GB, so even for a 16-spinner system (approx. 96GB journal space) appropriate SSDs are available at reasonable expense. Many operators are scared of an SSD suddenly dying a horrible death, so they put their SSDs in a RAID-1. Many are also tempted to put their OSD journal partitions onto the same RAID. Another option is to use, say, one partition on each of your SSD in a RAID for the operating system installation, and then chop up the rest of your SSDs as non-RAIDed Ceph OSD journals. This creates an interesting situation when you get to more than about 10-or-so OSDs (the exact number is hard to give). Now you have your OS and several OSD journals on the same physical SSD. SSDs are much faster than spinners, but they have neither infinite throughput nor zero latency. Eventually, you might hit your SSD's physical limits for random I/O all over the place. For example, if one of your hosts dies and the rest now reshuffles data to restore the desired level of redundancy, you may see relatively intensive I/O all over the other OSDs — this is exacerbated in a system where you have few OSD hosts which host many OSD disks. Putting your journal SSDs in a RAID set looks like a good idea at first. Specifically, Ceph OSDs currently cannot recover from a broken SSD journal without reinitializing and recovering the entire filestore. This means that as soon as SSD acting as journal backing storage burns up, you've effectively lost those OSDs completely and need to recover them from scratch. 1 Put them in a RAID-1, problem solved? Well, not quite, because you've now duplicated all of your journal writes and you're hitting two SSDs all over the place. Thus it's generally a much better idea to put half of your journals on one SSD, and half on the other. If one of your SSDs burns up you'll still lose the OSDs whose journals it hosts — but it'll only be half of the OSDs hosted on that node altogether. Any such performance issues get worse if some of your OSDs are also MONs: your OSD journals now compete with your operating system and your MONs for I/O on the same SSDs. Once your SSDs get hit so hard that your MONs can't do I/O, those MONs eventually die. This might not harm your operations if you have sufficient backup MONs available, and everything will be fine again once your recovery is complete, but it's still a nuisance. This is remarkably common specifically in POCs, by the way, where people often try to repurpose three of their old, two-SSDs-plus-dozens-of-disks storage servers for a 3-node Ceph cluster. So, as you are considering your OSD journal and filestore layout, take note of the following general guidelines: By and large, try to go for a relatively small number of OSDs per node, ideally not more than 8. This combined with SSD journals is likely to give you the best overall performance. If you do go with OSD nodes with a very high number of disks, consider dropping the idea of an SSD-based journal. Yes, in this kind of setup you might actually do better with journals on the spinners. Alternatively in the same scenario, consider putting your operating system install on one or a couple of the spinners (presumably smaller ones than the others), and use the (un-RAIDed) SSDs for OSD journals exclusively. Consider having a few dedicated MONs (MONs that are not also OSDs). Note on ceph-osd --mkjournal This article originally appeared on the hastexo.com website (now defunct). Since this article was originally published, a --mkjournal option was added to the ceph-osd command, allowing you to recreate a journal for an existing OSD. This mitigates the issue in that you don't need to recreate OSDs from scratch when a journal device breaks — but the OSDs will still be temporarily unavailable. ↩","tags":"hints-and-kinks","url":"resources/hints-and-kinks/solid-state-drives-and-ceph-osd-journals/","loc":"resources/hints-and-kinks/solid-state-drives-and-ceph-osd-journals/"},{"title":"Thoughts on \"ecosystems\"","text":"Over the past couple of years, it seems that the term ecosystem is being broadly applied to what we previously called a community. I don't like that, and here's why. The origin of the term ecosystem, when applied to the environment in which a software project is being developed, used and promoted, is unknown, at least to the best of my knowledge. Some say that it was Brian Aker who first spoke of \"the MySQL ecosystem\", and it seemed rather fitting at the time. Presently though, it seems there's ecosystems everywhere: the Linux ecosystem, the OpenStack ecosystem, the Python ecosystem, you name it. And it annoys me. It annoys me not in the way marketing drone babbling annoys me, like when someone waxes lyrical about synergies or paradigm shifts — that's the kind of fluff you automatically filter out and disregard, a bit like page numbers in the slide decks of presenters stuck in the 20th century. But the ecosystem thing is frequently used also by developers and users, the actual movers and shakers, in the way we would previously use community. Now let's look for a moment at how a community works. A community is governed by rules and morals. Those can be explicit, as written-down laws, covenants or contracts. Much more commonly though, they are implicit: everybody understands them, everybody is expected to abide by them, and if you break them, you're being shunned — but there's no requirement to write these rules down. When we think about communities, most will naturally associate this with a large group of people, like a clan or tribe, maybe a few hundred or even a few thousand individuals. Puny, right? We need something grander, something that alludes to hundreds or thousands of species with maybe millions of individuals playing a part. Let's pick a term: ecosystem. Yay! Problem solved. Waaaay bigger than a community. So much more awe-inspiring. But guess what: an ecosystem is fundamentally amoral. In an ecosystem, there is no right or wrong — other than survival being right, and if it happens to be at everyone else's expense, that doesn't make it wrong. From the inside perspective of an ecosystem, if an invasive species intrudes and steamrolls the entire habitat, so be it: it just changed the ecosystem. Nature shrugs. Nature also shrugs at parasites, disease, deception, camouflage, poison, and gangs of predators collaborating with swift and deadly force to mercilessly kill a defenseless herbivore. Now you're welcome to call me out on my naïveté, and point out that it is precisely those things that happen in business every day. I am acutely aware of that. I believe, however, we ought to consider them evils, and some may consider them necessary evils at times. They shouldn't the foundations on which we build our communities. Words matter. I think we should use them wisely. This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/12/16/thoughts-on-ecosystems/","loc":"blog/2012/12/16/thoughts-on-ecosystems/"},{"title":"On the merits of working from home, in a distributed virtual team","text":"During lunch at the EMEA OpenStack day in London this week, I had a brief but excellent conversation with fellow OpenStacker Adam Spiers from SUSE. Our chat turned to the merits of working from home, and he encouraged me to write up a blog post about some of the ideas of mine and of my co-founders' which we have since made hastexo policy, however informal or unwritten. Note that much of what follows aren't necessarily original ideas of ours. Many of my thoughts I owe to some very insightful chats I've had over the past few months with the delightful Sarah Novotny , original co-founder of Blue Gecko , seasoned OSCON conference chair and now CIO at Meteor Entertainment . If you get a chance to talk to Sarah at a conference and poll her views on this, I can highly recommend you seize that chance. It all starts with the observation that the separation of the place you live in, and the place you work at, is a fairly recent concept in human history. Prior to the Industrial Revolution , which originated in late 18th century England and steamrolled first Europe and then the rest of the world, no such separation was common: the blacksmith would live upstairs in his shop, so would the bakerman or the butcher. The teacher would dwell, with his family, in the local school. The farmer, and the farmhands, would live on that farm. In such a setting it follows naturally that the work day spans essentially your entire waking time: you would start your day's work as soon as you got up, and finished it when you retired for the night. It would be equally natural to close the shop and interrupt your work for perhaps an hour at a time, in order to consume a meal with your family or run an errand, or to hold the siesta common in the Mediterranean to pass the hottest hours of the day. Then with the Industrial Revolution, everything changed. In the name of efficiency and progress, we decided that we had to pool workers in one place — called a factory, or perhaps a shipyard — because now we needed collaboration: one person could no longer fulfill the task alone, so we had to get many people to one place to fulfill it together. And as a natural continuation of our pre-industrial routine, people would work ten to twelve hours a day, six days a week — until we realized that it started messing up our lives, inflicting misery on our families and social ties. And we invented a new concept called spare time: time we could spend by ourselves, or with our families and friends, something we didn't have to ask for in the pre-industrial age when our work and life would naturally have been integrated. And we gradually got to \"advances\" like first the 10-hour workday, then the 8-hour workday, then the 40-hour work week when we decided it would be better to have a rest of two days a week rather than one. Then we invented white-collar, office jobs, and we gradually moved from an industry-dominated to a service-dominated economy. And because by this time we were all well trained in the rules of industrial life, and because it had brought us progress and prosperity, we applied the same concepts to offices that we previously had applied to factories and shipyards: we would gather everyone in the same place, removed from home and families, and we would get everyone to accept fixed \"office hours\" when all hands would have to be present. Of course, we still needed to collaborate, it's just that the tasks differed from the ones we faced in factories and shipyards. Fast forward to the early 21st century, where we are suddenly endowed with an abundance of readily available, cheap technology that allows us to communicate and collaborate instantly, from almost anywhere. And it is at this point that the unnatural split between work and non-work life, which we inflicted upon ourselves during the Industrial Revolution and which we have managed to rationalize with the brainwash that a \"clean separation\" of \"work and private life\" is \"essential\" to well-being — that has become a complete anachronism. It is no longer vital for the people making up a company to physically be in the same place to collaborate, to serve customers, and to be productive and make a difference to communities. In fact, I consider it counterproductive. We've finally arrived in a position where we can restore the very natural way for humans to live and work, namely integrated with our families, from home, connected through technology that enables us to communicate just as effectively as sitting at the same desk. It also enables us to live healthier, better lives. I'm fully aware that this style of work is probably not for everyone. But if you're thinking it's not for you (and I was one of those, until a little over a year ago) it's worth asking yourself why you're thinking that. Is it really because you want to work in an office, or because everyone has told you for most of your career that you want to work in an office? Here at hastexo, it took us some time — several months — to figure out all-electronic collaboration, but the machinery is working extremely well now. The Google Apps stack has been enormously useful for us in that regard. We practically live in Google Hangouts and documents shared on Google Drive . We jot down ideas in Google Docs and sketch architectures in Google Drawings. We do our weekly standups that way, and increasingly customer meetings, too. We collaboratively draft and edit slide decks for training. And we rehearse conference talks via video call. It just works, and it's huge fun that way. And it enables us to close our laptops and go read bedtime stories to our kids when we're done. This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/12/06/on-the-merits-of-working-from-home-in-a-distributed-virtual-team/","loc":"blog/2012/12/06/on-the-merits-of-working-from-home-in-a-distributed-virtual-team/"},{"title":"Adding MySQL/Galera resources to Pacemaker","text":"Once you have one instance of Galera running, and it is running on the same node that holds the temporarily-configured cluster IP (192.168.122.99 in our example), you can add your resources to the Pacemaker cluster configuration. Create a temporary file, such as /tmp/galera.crm , with the following contents: primitive p_ip_mysql_galera ocf : heartbeat : IPaddr2 \\ params nic = \"eth1\" iflabel = \"galera\" \\ ip = \"192.168.122.99\" cidr_netmask = \"24\" primitive p_mysql ocf : heartbeat : mysql \\ params config = \"/etc/mysql/my.cnf\" \\ pid = \"/var/run/mysqld/mysqld.pid\" \\ socket = \"/var/run/mysqld/mysqld.sock\" \\ binary = \"/usr/sbin/mysqld\" \\ op monitor interval = \"30s\" \\ op start interval = \"0\" timeout = \"60s\" \\ op stop interval = \"0\" timeout = \"60s\" clone cl_mysql p_mysql \\ meta interleave = \"true\" colocation c_ip_galera_on_mysql \\ inf : p_ip_mysql_galera cl_mysql property stonith - enabled = \"false\" Then, import this into your Pacemaker configuration: crm configure load update /tmp/galera.crm What this creates are a couple of Pacemaker resources: The cluster IP address, 192.168.122.99 ( p_ip_mysql_galera ). Throughout the lifetime of the cluster, this will always be available on one of the nodes where any MySQL/Galera instance is running. This is the IP address new Galera nodes use when joining the cluster. The MySQL server itself ( cl_mysql ), which will be automatically recovered in-place if it ever fails. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/mysqlgalera-pacemaker-adding-mysqlgalera-resources-pacemaker/","loc":"resources/hints-and-kinks/mysqlgalera-pacemaker-adding-mysqlgalera-resources-pacemaker/"},{"title":"Bootstrapping the Galera cluster","text":"In order to bootstrap your Galera cluster, manually bring up the cluster IP address on the desired interface. In this example, we'll use 192.168.122.99 and eth1: ip address add 192 .168.122.99/24 dev eth1 label eth1:galera And initialize the Galera cluster: mysqld --wsrep_cluster_address = gcomm:// & Note the empty gcomm:// address. An avalanche of output is likely to follow. Near the end, you should see entries similar to these: [ Note ] WSREP : Synchronized with group , ready for connections [ Note ] mysqld : ready for connections . At this point, your MySQL/Galera cluster is properly initialized. It only has one node, and it is not under cluster management yet, but it's already a working Galera installation. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/mysqlgalera-pacemaker-bootstrapping-galera-cluster/","loc":"resources/hints-and-kinks/mysqlgalera-pacemaker-bootstrapping-galera-cluster/"},{"title":"Configuring Corosync","text":"You now need configure Corosync. The following example configuration file assumes that your cluster nodes have two network interfaces, using the 192.168.122.0/24 and 192.168.133.0/24 networks. You will need to adjust this to your own network configuration. Set the contents of /etc/corosync/corosync.conf as follows: compatibility : whitetank totem { version : 2 secauth : on threads : 0 rrp_mode : active token : 10000 interface { ringnumber : 0 bindnetaddr : 192.168 . 122.0 mcastaddr : 239.255 . 42.0 mcastport : 5405 ttl : 1 } interface { ringnumber : 1 bindnetaddr : 192.168 . 133.0 mcastaddr : 239.255 . 42.1 mcastport : 5405 ttl : 1 } } logging { fileline : off to_stderr : no to_logfile : no to_syslog : yes debug : off timestamp : on logger_subsys { subsys : AMF debug : off } } Also, create an authkey file for node authentication: dd if = /dev/urandom of = /etc/corosync/authkey bs = 128 count = 1 chmod 0400 /etc/corosync/authkey And create /etc/corosync/service.d/pacemaker with the following content: service { name : pacemaker ver ; 1 } Finally, distribute the configuration across your cluster: for n in bob charlie ; do rsync -av /etc/corosync/* $n :/etc/corosync done And start Corosync on all cluster nodes: service corosync start Once Corosync has started on all nodes, you should be able to check its status with the corosync-cfgtool and corosync-objctl commands: # corosync-cfgtool -s Printing ring status. Local node ID 1870309568 RING ID 0 id = 192 .168.122.111 status = ring 0 active with no faults RING ID 1 id = 192 .168.133.111 status = ring 1 active with no faults Both rings should be in the active with no faults state. # corosync-objctl runtime.totem.pg.mrp.srp.members runtime.totem.pg.mrp.srp.1870309568.ip = r ( 0 ) ip ( 192 .168.122.111 ) r ( 1 ) ip ( 192 .168.133.111 ) runtime.totem.pg.mrp.srp.1870309568.join_count = 1 runtime.totem.pg.mrp.srp.1870309568.status = joined runtime.totem.pg.mrp.srp.1887086784.ip = r ( 0 ) ip ( 192 .168.122.112 ) r ( 1 ) ip ( 192 .168.133.112 ) runtime.totem.pg.mrp.srp.1887086784.join_count = 1 runtime.totem.pg.mrp.srp.1887086784.status = joined runtime.totem.pg.mrp.srp.1903864000.ip = r ( 0 ) ip ( 192 .168.122.113 ) r ( 1 ) ip ( 192 .168.133.113 ) runtime.totem.pg.mrp.srp.1903864000.join_count = 1 runtime.totem.pg.mrp.srp.1903864000.status = joined All three nodes members should be in the membership with both of their interfaces, and their status should be joined . This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/mysqlgalera-pacemaker-configuring-corosync/","loc":"resources/hints-and-kinks/mysqlgalera-pacemaker-configuring-corosync/"},{"title":"Dealing with node failure","text":"If an entire node happens to get killed, and that node currently does not hold the Galera IP (192.168.122.99 in our example), then the other nodes simply continue to function normally, and you can connect to and use them without interruption. In the example below, alice has left the cluster: ============ Last updated : Mon Dec 3 22 : 24 : 55 2012 Last change : Mon Dec 3 22 : 23 : 19 2012 via crmd on charlie Stack : openais Current DC : charlie - partition with quorum Version : 1.1.7 - ee0730e13d124c3d58f00016c3376a1de5323cff 3 Nodes configured , 3 expected votes 4 Resources configured . ============ Online : [ bob charlie ] OFFLINE : [ alice ] Full list of resources : p_ip_mysql_galera ( ocf :: heartbeat : IPaddr2 ) : Started bob Clone Set : cl_mysql [ p_mysql ] Started : [ bob charlie ] Stopped : [ p_mysql : 0 ] If the node dies that does currently hold the Galera IP (192.168.122.99 in our example), then the cluster IP shifts to a different node, and when the failed node returns, it can re-fetch the cluster state from the node that took over the IP address. In the example below, in a healthy cluster the IP happens to be running on bob : ============ Last updated : Mon Dec 3 22 : 32 : 35 2012 Last change : Mon Dec 3 22 : 23 : 19 2012 via crmd on charlie Stack : openais Current DC : charlie - partition with quorum Version : 1.1.7 - ee0730e13d124c3d58f00016c3376a1de5323cff 3 Nodes configured , 3 expected votes 4 Resources configured . ============ Online : [ bob alice charlie ] Full list of resources : p_ip_mysql_galera ( ocf : : heartbeat : IPaddr2 ) : Started bob Clone Set : cl_mysql [ p_mysql ] Started : [ alice bob charlie ] Subsequently, bob is affected by a failure, and the IP address shifts to alice : ============ Last updated : Mon Dec 3 22 : 33 : 33 2012 Last change : Mon Dec 3 22 : 23 : 19 2012 via crmd on charlie Stack : openais Current DC : charlie - partition with quorum Version : 1.1.7 - ee0730e13d124c3d58f00016c3376a1de5323cff 3 Nodes configured , 3 expected votes 4 Resources configured . ============ Online : [ alice charlie ] OFFLINE : [ bob ] Full list of resources : p_ip_mysql_galera ( ocf :: heartbeat : IPaddr2 ) : Started alice Clone Set : cl_mysql [ p_mysql ] Started : [ alice charlie ] Stopped : [ p_mysql : 1 ] When bob returns, it simply connects to alice (which now hosts the cluster IP), fetches the database state from there, and continues to run: ============ Last updated : Mon Dec 3 22 : 35 : 46 2012 Last change : Mon Dec 3 22 : 23 : 19 2012 via crmd on charlie Stack : openais Current DC : charlie - partition with quorum Version : 1.1.7 - ee0730e13d124c3d58f00016c3376a1de5323cff 3 Nodes configured , 3 expected votes 4 Resources configured . ============ Online : [ bob alice charlie ] Full list of resources : p_ip_mysql_galera ( ocf : : heartbeat : IPaddr2 ) : Started alice Clone Set : cl_mysql [ p_mysql ] Started : [ alice bob charlie ] This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/mysqlgalera-pacemaker-dealing-node-failure/","loc":"resources/hints-and-kinks/mysqlgalera-pacemaker-dealing-node-failure/"},{"title":"MySQL/Galera in Pacemaker High Availability Clusters","text":"In this walkthrough, you will create a Pacemaker managed MySQL/Galera cluster. It assumes that you are running on a Debian 6.0 (squeeze) box, but the concepts should be equally applicable to other platforms with minimal modifications. It also assumes that your Galera cluster will consist of three nodes, named alice, bob and charlie. Furthermore, all cluster nodes can resolve each other's hostnames. Please note: All commands in this walkthrough require that you are logged into your system as root. First, make sure you have the required packages installed. One of the easiest ways to get your hands on MySQL/Galera binaries is to install Percona XtraDB Cluster, which our friends at Percona make available in their public software repository. Create /etc/apt/sources.list.d/percona.list with the following content: deb http://repo.percona.com/apt squeeze main Fetch the Percona repository signing key: apt-key adv --keyserver hkp://keys.gnupg.net --recv-keys 1C4CBDCDCD2EFD2A You also require Pacemaker packages from the Debian backports repository. Do do so, create /etc/apt/sources.list.d/backports.list with the following content: deb http://backports.debian.org/debian-backports squeeze-backports main Now, update your package lists: apt-get update Once that is completed, you are able to install the percona-xtradb-cluster-server-5.5 package: apt-get -y install percona-xtradb-cluster-server-5.5 Note that percona-xtradb-cluster-server-5.5 conflicts with the standard Debian mysql-server packages, so if you have any of those installed, they will be removed in the process of installing XtraDB Cluster. Stop the MySQL server services for the time being: service mysql stop Also required is the pacemaker package (and its dependencies) from squeeze-backports: apt-get -t squeeze-backports install pacemaker And finally rsync is required for one of the supported Snapshot State Transfer (SST) methods for Galera: apt-get install rsync Now, all required packages are installed and you're ready to configure XtraDB Cluster. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/mysqlgalera-pacemaker-high-availability-clusters/","loc":"resources/hints-and-kinks/mysqlgalera-pacemaker-high-availability-clusters/"},{"title":"Recovering from full cluster shutdown","text":"If at any time all of the nodes in your cluster have been taken down, it is necessary to re-initialize the Galera replication state. In effect, this is identical to bootstrapping the cluster. Start by manually bringing up the cluster IP on one of your nodes: ip address add 192 .168.122.99/24 dev eth1 label eth1:galera Re-initialize the Galera cluster: mysqld --wsrep_cluster_address = gcomm:// & Note the empty gcomm:// address. Finally, clear your resource state with crm resource cleanup cl_mysql . Pacemaker will leave the running IP address and MySQL instance untouched, and bring up the additional MySQL instances. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/mysqlgalera-pacemaker-recovering-full-cluster-shutdown/","loc":"resources/hints-and-kinks/mysqlgalera-pacemaker-recovering-full-cluster-shutdown/"},{"title":"Setting Galera-specific MySQL options","text":"Now you can proceed with setting Galera specifics in your MySQL configurations. Create a configuration file, identical on all cluster nodes, named /etc/mysql/conf.d/galera.cnf with the following content: [mysqld] bind_address = 0.0.0.0 binlog_format = ROW default_storage_engine = InnoDB innodb_autoinc_lock_mode = 2 innodb_locks_unsafe_for_binlog = 1 Create another configuration file, specific to each cluster node, named /etc/mysql/conf.d/wsrep.cnf with the following content: [mysqld] # node alice has address 192.168.122.111 wsrep_node_address = 192.168.122.111 wsrep_provider = /usr/lib/libgalera_smm.so wsrep_slave_threads = 8 wsrep_sst_method = rsync wsrep_cluster_address = gcomm://192.168.122.99 [mysqld] # node bob has address 192.168.122.112 wsrep_node_address = 192.168.122.112 wsrep_provider = /usr/lib/libgalera_smm.so wsrep_slave_threads = 8 wsrep_sst_method = rsync wsrep_cluster_address = gcomm://192.168.122.99 [mysqld] # node charlie has address 192.168.122.111 wsrep_node_address = 192.168.122.113 wsrep_provider = /usr/lib/libgalera_smm.so wsrep_slave_threads = 8 wsrep_sst_method = rsync wsrep_cluster_address = gcomm://192.168.122.99 You can now proceed with bootstrapping your cluster. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/mysqlgalera-pacemaker-setting-galera-specific-mysql-options/","loc":"resources/hints-and-kinks/mysqlgalera-pacemaker-setting-galera-specific-mysql-options/"},{"title":"Starting Pacemaker","text":"Once Corosync is running, you are able to start the Pacemaker cluster resource manager on all cluster nodes: service pacemaker start Once cluster startup is completed, you should see output similar to the following when invoking the crm_mon utility: ============ Last updated: Mon Dec 3 15:37:59 2012 Last change: Mon Dec 3 15:37:58 2012 via crmd on alice Stack: openais Current DC: alice - partition with quorum Version: 1.1.7-ee0730e13d124c3d58f00016c3376a1de5323cff 3 Nodes configured, 3 expected votes 0 Resources configured. ============ Online: [ bob alice charlie ] This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/mysqlgalera-pacemaker-starting-pacemaker/","loc":"resources/hints-and-kinks/mysqlgalera-pacemaker-starting-pacemaker/"},{"title":"Testing resource recovery","text":"If MySQL happens to die in your cluster, Pacemaker will automatically recover the service in place. To test this, select any node on your cluster and send the mysqld process a KILL signal: killall -KILL mysqld Then, monitor your cluster status with crm_mon -rf . After a few seconds, you should see one of your p_mysql clones entering the FAILED state: ============ Last updated : Mon Dec 3 19 : 03 : 25 2012 Last change : Mon Dec 3 18 : 54 : 44 2012 via crmd on bob Stack : openais Current DC : charlie - partition with quorum Version : 1.1.7 - ee0730e13d124c3d58f00016c3376a1de5323cff 3 Nodes configured , 3 expected votes 4 Resources configured . ============ Online : [ bob alice charlie ] Full list of resources : p_ip_mysql_galera ( ocf :: heartbeat : IPaddr2 ) : Started alice Clone Set : cl_mysql [ p_mysql ] p_mysql : 1 ( ocf :: heartbeat : mysql ) : Started bob FAILED Started : [ alice charlie ] Migration summary : * Node alice : * Node bob : * Node charlie : Failed actions : p_mysql : 1 _monitor_30000 ( node = bob , call = 30 , rc = 7 , status = complete ) : not running Then, after a few seconds, the resource will automatically recover: ============ Last updated : Mon Dec 3 19 : 03 : 35 2012 Last change : Mon Dec 3 18 : 54 : 44 2012 via crmd on bob Stack : openais Current DC : charlie - partition with quorum Version : 1.1.7 - ee0730e13d124c3d58f00016c3376a1de5323cff 3 Nodes configured , 3 expected votes 4 Resources configured . ============ Online : [ bob alice charlie ] Full list of resources : p_ip_mysql_galera ( ocf : : heartbeat : IPaddr2 ) : Started alice Clone Set : cl_mysql [ p_mysql ] Started : [ alice bob charlie ] Migration summary : * Node alice : * Node bob : p_mysql : 1 : migration - threshold = 1000000 fail - count = 1 * Node charlie : Failed actions : p_mysql : 1 _monitor_30000 ( node = bob , call = 30 , rc = 7 , status = complete ) : not running To subsequently get rid of the entry in the Failed actions list, use crm resource cleanup cl_mysql . This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/mysqlgalera-pacemaker-testing-resource-recovery/","loc":"resources/hints-and-kinks/mysqlgalera-pacemaker-testing-resource-recovery/"},{"title":"MySQL High Availability Deep Dive","text":"This is a tutorial that Yves Trudeau and I presented at the Percona Live UK 2012 conference in London. It covers Pacemaker integration with DRBD, MySQL Replication, and Galera. Slides: Google Slides This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/mysql-high-availability-deep-dive/","loc":"resources/presentations/mysql-high-availability-deep-dive/"},{"title":"GlusterFS in High Availability Clusters","text":"My Pacemaker presentation from the GlusterFS Workshop at LinuxCon Europe 2012. Presented in Barcelona in November of 2012, this is a overview of integrating GlusterFS with the Pacemaker cluster stack. This tutorial gives an overview of The Pacemaker stack, Using GlusterFS for Pacemaker storage, Managing GlusterFS volumes from Pacemaker. My original presentation included several live demos. In this version, they have been replaced by placeholders. Use the PgUp/PgDown keys to navigate through the presentation, or just advance by hitting the spacebar. Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/glusterfs-high-availability-clusters/","loc":"resources/presentations/glusterfs-high-availability-clusters/"},{"title":"Hands-On With Ceph","text":"My Ceph tutorial from LinuxCon Europe 2012. Presented in Barcelona in November of 2012, this is a dense summary of the features of the Ceph distributed storage stack. This tutorial gives an overview of Native RADOS object storage, The RBD block device, ReSTful object storage with radosgw, the Ceph distributed filesystem. My original presentation included several live demos. In this version, they have been replaced by placeholders. Slides: GitHub This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/hands-ceph/","loc":"resources/presentations/hands-ceph/"},{"title":"Talking Ceph and GlusterFS at LinuxCon Europe","text":"Early next month, I'll be off to Barcelona for speaking at LinuxCon Europe. Here's an overview of my talks. November 5-7, the Linux Foundation is holding the annual LinuxCon Europe in one of Europe's most beautiful cities — some say the most beautiful — Barcelona. I will be attending the full conference, and presenting two talks. Wednesday, November 7, is a day full of tutorials at LinuxCon Europe. I am presenting Hands-On with Ceph: Object Storage, Block Storage, Filesystem & More , a deep dive into the Ceph stack. This is a double-slot tutorial, scheduled for 2:45 - 4:25pm in the Verdi room. Then on Thursday, the GlusterFS community team has invited me to speak at the Gluster Workshop. This workshop is complimentary to registered LinuxCon Europe attendees , but you can also register separately just for the workshop. In that talk, I'll speak about GlusterFS in High Availability Clusters: Integration with the Pacemaker HA Stack . It's also a 1-hour slot, from 10 - 11am in Vivaldi. I'm pretty excited about this trip: it's close to home, it's a great conference, and I've never been to Barcelona before. So, if you're headed there, please drop me a note and let me know so we can catch up. Thanks — see you there! This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/10/24/talking-ceph-and-glusterfs-at-linuxcon-europe/","loc":"blog/2012/10/24/talking-ceph-and-glusterfs-at-linuxcon-europe/"},{"title":"Migrating virtual machines from block-based storage to RADOS/Ceph","text":"Ceph allows you to replace existing SAN storage (or SAN drop-in substitutes) with a flexible storage solution with real scale-out capabilities. Here is how you migrate existing virtual machines managed by libvirt from block-based storage to a Ceph based storage solution. Prerequisites What you'll need in order to successfully manage the migration from block-based storage to a working Ceph cluster is this: A working Ceph cluster. You probably guessed this one. More specifically, you should have access to the client.admin key of your RADOS installation. Usually, the key will be stored in /etc/ceph/keyring on nodes running RADOS. a RADOS pool in which you can create RBD images. You can either use the standard rbd pool or create your own pool. We'll use the libvirt pool throughout the following example. a set of credentials for a client to connect to the cluster and create and use RBD devices. If you use a libvirt version < 0.9.7, you will have to use the default client.admin credentials for this purpose. If you run libvirt 0.9.7 or later, you should use a separate set of credentials (i.e. create a user called e.g. client.rbd and use that one). That user should have at least the allow r permission on your mons, and allow rw on your osds (the latter you can restrict to the rbd pool used if you wish). qemu in version 0.14 or higher libvirt in version 0.8.7 or higher (0.9.7 or higher if you want to use a separate user for this) Ceph 0.48 (\"argonaut\") or higher Getting Started When migrating a VM from block-based storage to a Ceph cluster, you unfortunately can't avoid a period of downtime (after all, you won't be able to reliably copy a filesystem from place A to B while it's still changing on the go). So the first thing to do is shut down a currently running virtual machine, like we will do with the ubuntu-amd64-alice VM in this example: virsh shutdown ubuntu-amd64-alice Then you need to create an RBD image within that pool. Suppose you would like to create one that is 100GB in size (recall, all RBD images are thin-provisioned, so it won't actually use 100GB in the Ceph cluster right from the start). qemu-img create -f rbd rbd:libvirt/ubuntu-amd64-alice 100G This means you are connecting to the Ceph mon servers (defined in the default configuration file, /etc/ceph/ceph.conf) using the client.admin identity, whose authentication key should be stored in /etc/ceph/keyring. The nominal image size is 102400MB, it's part of the libvirt pool and its name is a hardly creative ubuntu-amd64-alice. You can run this command from any node inside or outside your Ceph cluster, as long as the configuration file and authentication credentials are stored in the appropriate location. The next step, however, is one that you must complete on the node where you can currently access your block-based storage. This could either be the machine that you have your VM's device currently connected to via iSCSI or - if you are using a SAN drop-in replacement based on DRBD - the machine that currently has the VM's DRBD resource in Primary mode. If you are unsure what your VM's block device is, take a look at the VM's configuration with virsh dumpxml ubuntu-amd64-alice to find out the actual device name (look out for paragraphs including a statement). In our case, the actual device is /dev/drbd/by-res/vm-ubuntu-amd64-alice. Now let's go ahead and do the actual conversion. Please note: For the following command to work, you need a properly populated /etc/ceph directory because that is where qemu-img gets its information from. This is the command that initiates the conversion: qemu-img convert -f raw -O rbd \\ /dev/drbd/by-res/vm-ubuntu-amd64-alice \\ rbd:libvirt/ubuntu-amd64-alice Once the qemu-img command has completed, the actual conversion of your data is already done. That was easy, wasn't it? The final step is to change your libvirt VM configuration file to reflect the changes. Adapting the VM's libvirt configuration (libvirt < 0.9.7) If we want our VM to run on top of a Ceph object store, we need to tell libvirt how to start the VM appropriately. Luckily, current versions of libvirt support Ceph-based RBD backing devices out of the box. Please note: All following steps assume that you have your /etc/ceph set up properly. This means that a working ceph.conf and a keyring file containing the authentication key for client.admin is present. Open up your VM's configuration for editing with virsh edit ubuntu-amd64-alice and scroll down to the VM's disk definition. In our example, that part of the configuration looks like this: <disk type= 'block' device= 'disk' > <driver name= 'qemu' type= 'raw' cache= 'none' /> <source dev= '/dev/drbd/by-res/vm-ubuntu-amd64-alice' /> <target dev= 'vda' bus= 'virtio' /> <address type= 'pci' domain= '0x0000' bus= '0x00' slot= '0x05' function= '0x0' /> </disk> Replace it with an entry using our RBD image: <disk type= 'network' device= 'disk' > <driver name= 'qemu' type= 'raw' /> <source protocol= 'rbd' name= 'libvirt/ubuntu-amd64-alice' > <host name= '192.168.133.111' port= '6789' /> <host name= '192.168.133.112' port= '6789' /> <host name= '192.168.133.113' port= '6789' /> </source> <target dev= 'vda' bus= 'virtio' /> <address type= 'pci' domain= '0x0000' bus= '0x00' slot= '0x05' function= '0x0' /> </disk> Be sure to replace the three IPs in the above example with the actual IPs of your MON servers. Finally, start your virtual machine: virsh start ubuntu-amd64-alice Adapting the VM's libvirt configuration (libvirt >= 0.9.7) Starting with libvirt 0.9.7, you can use a user other than client.admin to access RBD images via libvirt. We recommend to do this. Creating such a setup works very similar to the one without a separate user; the main difference is that it requires you to define a secret in libvirt for the VM. First of all, figure out what user you will be using from within libvirt and where that user's authentication key is stored. For this example, we will assume that the user is called client.rbd and that this user's key is stored in /etc/ceph/keyring.client.rbd. Now, create a new UUID by calling uuidgen on the command line. The UUID for our example will be 5cddc503-9c29-4aa8-943a-c097f87677cf. Then, open /etc/libvirt/secrets/ubuntu-amd64-alice.xml and define a secret block in there: <secret ephemeral= \"no\" private= \"no\" > <uuid> 5cddc503-9c29-4aa8-943a-c097f87677cf </uuid> <usage type= \"ceph\" > <name> client.rbd secret </name> </usage> </secret> Be sure to replace the example's UUID with your own, self-generated value. Make libvirt add this secret to its internal keyring: virsh secret-define \\ /etc/libvirt/secrets/ubuntu-amd64-alice.xml Now find out your user's secret key. Do ceph auth get-or-create client.rbd and take note of the key. In our example, AQB0Q4ZQYDB2MBAAYzWmHvpg7t1MzV1E0jkBww== is the key that will allow us access as client.rbd. Then define the actual password for our secret definition: virsh secret-set-value \\ 5cddc503-9c29-4aa8-943a-c097f87677cf \\ AQB0Q4ZQYDB2MBAAYzWmHvpg7t1MzV1E0jkBww== Again, be sure to use your self-generated UUID instead of the one in this example. Also replace the example key with your real key. Finally, go ahead and adapt your VM settings. Open your VM configuration with virsh edit ubuntu-amd64-alice and scroll down to the VM's disk definition. In our example, that part of the configuration looks like this: <disk type= 'block' device= 'disk' > <driver name= 'qemu' type= 'raw' cache= 'none' /> <source dev= '/dev/drbd/by-res/vm-ubuntu-amd64-alice' /> <target dev= 'vda' bus= 'virtio' /> <address type= 'pci' domain= '0x0000' bus= '0x00' slot= '0x05' function= '0x0' /> </disk> Replace it with an entry using our RBD image: <disk type= 'network' device= 'disk' > <driver name= 'qemu' type= 'raw' /> <auth username= 'rbd' > <secret type= 'ceph' usage= 'client.rbd secret' /> </auth> <source protocol= 'rbd' name= 'libvirt/ubuntu-amd64-alice' > <host name= '192.168.133.111' port= '6789' /> <host name= '192.168.133.112' port= '6789' /> <host name= '192.168.133.113' port= '6789' /> </source> <target dev= 'vda' bus= 'virtio' /> <address type= 'pci' domain= '0x0000' bus= '0x00' slot= '0x05' function= '0x0' /> </disk> Be sure to replace the three IPs in the above example with the actual IPs of your MON servers. Finally, start your virtual machine: virsh start ubuntu-amd64-alice That's it. Your VM should now boot up and use its RBD image from Ceph instead of its original block-based storage backing device. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/migrating-virtual-machines-block-based-storage-radosceph/","loc":"resources/hints-and-kinks/migrating-virtual-machines-block-based-storage-radosceph/"},{"title":"Pacemaker and the recent GitHub service interruption","text":"It never fails. Someone manages to break their Pacemaker cluster, and Henrik starts preaching his usual sermon of why Pacemaker is terrible and why you should never-ever use it. And when that someone is GitHub , which we all know, use and love, then that sermon gets a bit of excess attention. Let's take a quick look at the facts. The week of September 10, GitHub suffered a couple of outages which caused a total downtime of 1 hour and 46 minutes, as Jesse precisely pointed out in a blog post . Exhibiting the excellent transparency that GitHub always offers at any time its infrastructure is affected by issues (remember their role-model behavior in an SSH security incident a few months back), Jesse explains, in a very detailed way, what happened on one of their Pacemaker clusters. Now, all of what follows is based exclusively on the information in that blog post of Jesse's. I have no inside knowledge of the incident, so my picture may be incomplete or skewed. But here's my take on it anyway. I do encourage you to read Jesse's post full-length, as the rest of this post otherwise won't make much sense. I'll just quote certain pieces of it and comment on them here. Please note: nothing in this post should be construed as a put-down of GitHub's excellent staff. They run a fantastic service and do an awesome job. It's just that their post-mortem seems to have created some misconceptions in the MySQL community about the Pacemaker stack as a whole, and those I'd like to help rectify. Also, I'm posting this in the hope that it provides useful insight to both the GitHub folks, and to anyone else facing similar issues. Enable Maintenance Mode when you should From the original post: Monday's migration caused higher load on the database than our operations team has previously seen during these sorts of migrations. So high, in fact, that they caused Percona Replication Manager's health checks to fail on the master. In response to the failed master health check, Percona Replication manager moved the ‘active' role and the master database to another server in the cluster and stopped MySQL on the node it perceived as failed. At the time of this failover, the new database selected for the ‘active' role had a cold InnoDB buffer pool and performed rather poorly. The system load generated by the site's query load on a cold cache soon caused Percona Replication Manager's health checks to fail again, and the ‘active' role failed back to the server it was on originally. At this point, I decided to disable all health checks by enabling Pacemaker's maintenance-mode ; an operating mode in which no health checks or automatic failover actions are performed. Performance on the site slowly recovered as the buffer pool slowly reached normal levels. Now there's actually several issues in there even in this early stage. Maintenance mode is generally the right thing to do here, but you enable it before making large changes to the configuration, and you disable it when done. If you're uncomfortable with the cluster manager taking its hands off the entire cluster, and you know what you're doing, you could also just disable cluster management and monitoring on a specific resource. Both approaches are explained here . Also, as far as \"health checks failing\" on the master is concerned, pretty much the only thing that is likely to cause such a failure in this instance is a timeout, and you can adjust those even on a per-operation basis in Pacemaker. But even that is unnecessary if you enable maintenance mode at the right time. \"Maintenance mode\" really means maintenance mode The following morning, our operations team was notified by a developer of incorrect query results returning from the node providing the ‘standby' role. I investigated the situation and determined that when the cluster was placed into maintenance-mode the day before, actions that should have caused the node elected to serve the ‘standby' role to change its replication master and start replicating were prevented from occurring. Well, of course. In maintenance mode, Pacemaker takes its hands off your resources. If you're enabling maintenance mode right in the middle of a failover, then that's not exactly a stellar idea. If you do, then it's your job to complete those actions manually. I determined that the best course of action was to disable maintenance-mode to allow Pacemaker and the Percona Replication Manager to rectify the situation. \"Best\" might be an exaggeration, if I may say so. A segfault and rejected cluster messages Upon attempting to disable maintenance-mode , a Pacemaker segfault occurred that resulted in a cluster state partition. OK, that's bad, but what exactly segfaulted? crmd? attrd? pengine? Or the master Heartbeat process? But the next piece of information would have me believe that the segfault really isn't the root cause of the cluster partition: After this update, two nodes (I'll call them ‘a' and ‘b') rejected most messages from the third node (‘c'), while the third node rejected most messages from the other two. Now it's a pity that we don't have any version information and logs, but this looks very much like the \"not in our membership\" issue present up to Pacemaker 1.1.6. This is a known issue, the fix is to update to a more recent version ( here's the commit, on GitHub of course), and the workaround is to just restart the Pacemaker services on the affected node(s) while in maintenance mode. A non-quorate partition running MySQL? Despite having configured the cluster to require a majority of machines to agree on the state of the cluster before taking action, two simultaneous master election decisions were attempted without proper coordination. In the first cluster, master election was interrupted by messages from the second cluster and MySQL was stopped. Now this is an example of me being tempted to say, \"logs or it didn't happen.\" If you've got the default no-quorum-policy of \"block\", and you're getting a non-quorate partition, and you don't have any resources with operations explicitly configured to ignore quorum, then \"two simultaneous master election decisions\" can only refer to the Designated Coordinator (DC) election, which has no bearing whatsoever on MySQL master status. Luckily, Pacemaker allows us to take a meaningful snapshot of all cluster logs and status after the fact with crm_report. It would be quite interesting to see a tarball from that. In the second, single-node cluster, node ‘c' was elected at 8:19 AM, and any subsequent messages from the other two-node cluster were discarded. As luck would have it, the ‘c' node was the node that our operations team previously determined to be out of date. We detected this fact and powered off this out-of-date node at 8:26 AM to end the partition and prevent further data drift, taking down all production database access and thus all access to github.com. That's obviously a bummer, but really, if that partition is non-quorate, and Pacemaker hasn't explicitly been configured to ignore that, no cluster resources would start there. Needless to say a working fencing configuration would have helped oodles, too. Your cluster has no crystal ball, but it does have a command line I'll skip over most of the rest of the GitHub post, because it's an explanation of how these backend issues affected GitHub users. I'll just hop on down to this piece: The automated failover of our main production database could be described as the root cause of both of these downtime events. In each situation in which that occurred, if any member of our operations team had been asked if the failover should have been performed, the answer would have been a resounding no. Well, you could have told your Pacemaker of that fact beforehand. Enable maintenance mode and you're good to go. There are many situations in which automated failover is an excellent strategy for ensuring the availability of a service. After careful consideration, we've determined that ensuring the availability of our primary production database is not one of these situations. To this end, we've made changes to our Pacemaker configuration to ensure failover of the ‘active' database role will only occur when initiated by a member of our operations team. That splash you just heard was the bath water. The scream was the baby being tossed out with it. Automated failover is a pretty poor strategy in the middle of a large configuration change. And Pacemaker gives you a simple and easy interface to disable it, by changing a single cluster property. Failure to do so may result in problems, and in this case it did. When you put a baby seat on the passenger side of your car, you disable the air bag to prevent major injury. But if you take that baby seat out and an adult passenger rides with you, are you seriously saying you're going to manually initiate the air bag in case of a crash? I hope you're not. Finally, our operations team is performing a full audit of our Pacemaker and Heartbeat stack focusing on the code path that triggered the segfault on Tuesday. That's probably a really good idea. For anyone planning to do the same, we can help. This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/09/26/pacemaker-and-the-recent-github-service-interruption/","loc":"blog/2012/09/26/pacemaker-and-the-recent-github-service-interruption/"},{"title":"Maintenance in active Pacemaker clusters","text":"In a Pacemaker cluster, as in a standalone system, operators must complete maintenance tasks such as software upgrades and configuration changes. Here's what you need to keep Pacemaker's built-in monitoring features from creating unwanted side effects. Maintenance mode This is quite possibly Pacemaker's single most useful feature for cluster maintenance. In maintenance mode, Pacemaker essentially takes a \"hands-off\" approach to your cluster. Enabling Pacemaker maintenance mode is very easy using the Pacemaker crm shell: crm configure property maintenance-mode = true In maintenance mode, you can stop or restart cluster resources at will. Pacemaker will not attempt to restart them. All resources automatically become unmanaged, that is, Pacemaker will cease monitoring them and hence be oblivious about their status. You can even stop all Pacemaker services on a node, and all the daemons and processes originally started as Pacemaker managed cluster resources will continue to run. You should know that when you start Pacemaker services on a node while the cluster in maintenance mode, Pacemaker will initiate a single one-shot monitor operation (a \"probe\") for every resource just so it has an understanding of what resources are currently running on that node. It will, however, take no further action other than determining the resources' status. You disable maintenance mode with the crm shell, as well: crm configure property maintenance-mode = false Maintenance mode is something you enable before running other maintenance actions, not when you're already half-way through them. And unless you're very well versed in the interdependencies of resources running on the cluster you're working on, it's usually the very safest option. In short: when doing maintenance on your Pacemaker cluster, by default, enable maintenance mode before you start, and disable it after you're done. Disabling monitoring and error recovery on specific resources For any configuration changes that take no more than a few minutes, involving an admin that is potentially watching a console window the whole time, maintenance mode is highly recommended. However, enabling maintenance mode can be a bit hard to argue for large configuration changes lasting, say, several hours. Think of a massive database rebuild, for example. In such a case, you may want to put only your database resource in something like maintenance mode, and have Pacemaker continue to monitor other resources like normal. You can do so by switching the resource to unmanaged mode and disable its monitor operation: crm configure edit p_database Then change the is-managed meta attribute and disable the monitor operation: meta is-managed=false op monitor interval=<interval> enabled=false Once you've done that, you'll effectively have enabled something akin to maintenance mode for a single resource. You can reverse this as you would expect: crm configure edit p_database Then change the is-managed meta attribute and re-enable the monitor operation: meta is-managed=true op monitor interval=<interval> enabled=true When using this approach, all other resources will be monitored and automatically recovered as they normally would. Thus, you'll have to be acutely aware of any side effects your maintenance activities have on other resources. If you're unsure, you should use the global maintenance mode instead. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/maintenance-active-pacemaker-clusters/","loc":"resources/hints-and-kinks/maintenance-active-pacemaker-clusters/"},{"title":"High Availability in OpenStack","text":"An update on high-availability development during the OpenStack Folsom development cycle. This presentation was delivered August 30, 2012 in San Diego, California. It was part of the inaugural CloudOpen conference hosted by the Linux Foundation . Following up on my earlier talks at OpenStack Summit and OSCON, I summarized the high-availability features OpenStack gained during the Folsom development cycle. Slides: Prezi This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/high-availability-openstack/","loc":"resources/presentations/high-availability-openstack/"},{"title":"Speaking and BoFing at CloudOpen in San Diego!","text":"Next week, I will be speaking at the inaugural CloudOpen conference in San Diego. This is your chance to learn about OpenStack high availability and Ceph! August 29-31, San Diego hosts the first CloudOpen conference, colocated with LinuxCon North America . CloudOpen is the Linux Foundation ‘s brand new, stack-agnostic cloud conference where OpenStackers can mingle with CloudStackers and Eucalyptus folks to discuss open-source cloud solutions. It's also the conference where I will be giving my fourth (and likely last, at least for the time being) incarnation of the High Availability for OpenStack talk I first delivered at the Folsom design summit back in April. Since then, we've had a lot of community involvement for HA in OpenStack, and have made some excellent progress, and I will be more than happy to report on that. This presentation is on Thursday, 2:25-3:10pm in Executsoemive Center Room 2 , in the Operations track. Also, Sage Weil of Ceph fame is joining me for an birds-of-a-feather (BoF) session on Ceph. Ross Turk and I had such an excellent turnout (and a great time) in the Ceph BoF at OSCON that we just had to do another. And Sage agreed to take part, which is excellent. He has a talk on Ceph in the main conference track as well. The conference organizers do not announce BoF sessions ahead of time on the CloudOpen web site, so I've simply set up a Google+ event for you to check in on. The exact location is still TBD (we will be assigned a room based on availability), but we will definitely be in the conference area at the Sheraton in San Diego. If you're attending CloudOpen and you want to learn more about Ceph, you're more than welcome to join us! My personal CloudOpen schedule is available here , by the way. Feel free to grab me at a talk, or in the hallway. See you in San Diego! This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/08/20/speaking-and-bofing-at-cloudopen-in-san-diego/","loc":"blog/2012/08/20/speaking-and-bofing-at-cloudopen-in-san-diego/"},{"title":"Highly Available Cloud: Pacemaker integration with OpenStack","text":"This presentation was delivered July 17, 2012 at OSCON in Portland, Oregon. I summarize high availability in OpenStack Folsom, particularly OpenStack integration with the Pacemaker high availability cluster stack. I talk about high availability shortcomings in OpenStack Essex, comparing OpenStack to some of its important competitors. I then explain how these shortcomings are being addressed in Folsom, and give an overview of the current progress in view of current OpenStack Folsom development. Slides: Prezi This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/highly-available-cloud-pacemaker-integration-openstack/","loc":"resources/presentations/highly-available-cloud-pacemaker-integration-openstack/"},{"title":"Configuring radosgw to behave like Amazon S3","text":"If you've heard of Ceph, you've surely heard of radosgw, a RESTful gateway interface to the RADOS object store. You've probably also heard that it provides a front-end interface that is compatible with Amazon's S3 API. The question remains, if you have an S3 client that always assumes it can find objects at http://bucket.s3.amazonaws.com, how can you use such a client to interact, unmodified, with your radosgw host (or hosts)? Pulling this off is actually remarkably simple, if you can control what nameserver your clients use to resolve DNS names. Which should be a given in the private cloud space. First, of course, you'll need an installed and configured Ceph cluster with one or several radosgw nodes. The Ceph documentation is an excellent reference for setting up radosgw. Configuring radosgw to support virtual hosts Then, you make sure you have the following entry in your Ceph configuration (normally in /etc/ceph/ceph.conf): [client.radosgw.charlie] rgw dns name = s3.amazonaws.com Substitute charlie with whatever name you want to use for your radosgw client when you interact with Ceph. What the rgw dns name option specifies is that radosgw will answer queries also for URLs like http://bucket.hostname/object, as opposed to just http://hostname/bucket/object. Configuring Apache to respond to S3 host names Also, add a wildcard record to the ServerAlias directive in the web server configuration for your radosgw host. For example: <VirtualHost *:80 > ServerName radosgw.example.com ServerAlias s3.amazonaws.com ServerAlias *.amazonaws.com Configuring your DNS server Then, set up your DNS server with a wildcard record in the s3.amazonaws.com zone, and have nameserver respond to requests in that zone. The zone file (for BIND9, in this case) could look like this: $TTL 604800 @ IN SOA alice.example.com. root.alice.example.com. ( 2 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS alice.example.com. @ IN A 192.168.122.113 * IN CNAME @ In this zone, the A record s3.amazonaws.com resolves to 192.168.122.113, and any sub-domain (like mybucket.s3.amazonaws.com) also resolves to that same address via a CNAME record. Using your RADOS store with S3 clients And then you just configure your client hosts to resolve DNS names via that nameserver, and use your preferred client application to interact with it. For example, for a user that you've created with radosgw-admin, which uses the access key 12345 with a secret of 67890, and Mark Atwood's popular Net::Amazon::S3::Tools toolkit, here's how you can interact with your RADOS objects: # export AWS_ACCESS_KEY_ID=12345 # export AWS_ACCESS_KEY_SECRET=67890 # s3mkbucket mymostawesomebucket # s3ls mymostawesomebucket # s3put mymostawesomebucket/foobar <<< \"hello world\" # s3ls mymostawesomebucket foobar # s3get mymostawesomebucket/foobar hello world Simple enough. You can add one more nifty feature. Adding load balancing radosgw can scale horizontally, and all you need to do to make this work is to duplicate your radosgw and Apache configuration onto a different host, and then add a second record to your DNS zone: $TTL 604800 @ IN SOA alice.example.com. root.alice.example.com. ( 3 ; Serial 604800 ; Refresh 86400 ; Retry 2419200 ; Expire 604800 ) ; Negative Cache TTL ; @ IN NS alice.example.com. @ IN A 192.168.122.112 @ IN A 192.168.122.113 * IN CNAME @ Then, as you access more buckets, you'll hit the A records in a round-robin fashion, meaning your requests will be balanced across the servers. Add as many as you like. HTTPS support Obviously, the above steps will not work for HTTPS connections to the REST API. And really, making that work would amount to some pretty terrible SSL certificate authority and client trust hackery, so just don't do it. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/configuring-radosgw-behave-amazon-s3/","loc":"resources/hints-and-kinks/configuring-radosgw-behave-amazon-s3/"},{"title":"Fencing in VMware virtualized Pacemaker nodes","text":"For users of VMware virtualization, it's becoming increasingly common to deploy Pacemaker clusters within the virtual infrastructure. Doing this requires that you set up fencing via ESX Server or, more commonly, vCenter. Here's how to do that. The cluster-glue package contains node Pacemaker's fencing (STONITH) plugins, one of which is the external/vcenter plugin. It enables Pacemaker to interface with an ESX Server host or vCenter server. When a Pacemaker node needs to be fenced, the fencing node contacts the vCenter host and instructs it to knock out the offending node. For this to work, your configuration needs to satisfy a couple of prerequisites: Your setup needs a reasonably recent cluster-glue package (the one that ships in Debian squeeze-backports and Ubuntu precise is fine). You need to install the vSphere Web Services SDK on your nodes. This itself has a number of Perl prerequisites. On Debian/Ubuntu systems, you should be able to install them with: aptitude install libarchive-zip-perl libcrypt-ssleay-perl \\ libclass-methodmaker-perl libuuid-perl \\ libsoap-lite-perl libxml-libxml-perl Now, create a set of vCenter credentials with the credstore_admin.pl utility that comes bundled with the SDK: /usr/lib/vmware-vcli/apps/general/credstore_admin.pl \\ -s <vCenter server IP or hostname> \\ -u <vCenter username> \\ -p <vCenter password> This creates a credentials file in .vmware/credstore/vicredentials.xml relative to your home directory. Copy this file into a location where Pacemaker can find it, say /etc/vicredentials.xml , and make sure it gets 0600 permissions. Also, remember to copy it to all your cluster nodes. Once your credentials are properly set up, you can test the STONITH agent's functionality by invoking it directly, like so: VI_SERVER = <vCenter server IP or hostname> \\ VI_CREDSTORE = /etc/vicredentials.xml \\ HOSTLIST = \"<pacemaker hostname>=<vCenter virtual machine name>\" \\ RESETPOWERON = 0 \\ /usr/lib/stonith/plugins/external/vcenter gethosts is the name of one of your cluster nodes as per uname -n, and is the corresponding machine name in your vCenter inventory. If everything is working fine, the gethosts command should return the Pacemaker hostname again. Now, on to adding this to the Pacemaker configuration. The example below is for two hosts named alice and bob, which in the inventory happen to be listed by their FQDN in the example.com domain: primitive p_fence_alice stonith:external/vcenter \\ params VI_SERVER=\"vcenter.example.com\" \\ VI_CREDSTORE=\"/etc/vicredentials.xml\" \\ HOSTLIST=\"alice=alice.example.com\" \\ RESETPOWERON=\"0\" \\ pcmk_host_check=\"static-list\" \\ pcmk_host_list=\"alice\" \\ op monitor interval=\"60\" primitive p_fence_bob stonith:external/vcenter \\ params VI_SERVER=\"vcenter.example.com\" \\ VI_CREDSTORE=\"/etc/vicredentials.xml\" \\ HOSTLIST=\"bob=bob.example.com\" \\ RESETPOWERON=\"0\" \\ pcmk_host_check=\"static-list\" \\ pcmk_host_list=\"bob\" \\ op monitor interval=\"60\" location l_fence_alice p_fence_alice -inf: alice location l_fence_bob p_fence_bob -inf: bob property stonith-enabled=\"true\" At this point you should be able to test fencing with stonith_admin -F or crm node fence . Or simulate a node problem with killall -9 corosync . Special thanks for this goes to Nhan Ngo Dinh both for writing the plugin in the first place, and for providing an excellent and straightforward README file for it. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/fencing-vmware-virtualized-pacemaker-nodes/","loc":"resources/hints-and-kinks/fencing-vmware-virtualized-pacemaker-nodes/"},{"title":"An exciting day for the Ceph community","text":"Today, as you've probably noticed if you're following the development of the Ceph stack, something mighty cool has been happening. The ceph.com web site received a major makeover with a slick new design, and the people behind Ceph have announced the launch of a brand new company to drive the Ceph stack, Inktank . As I've previously blogged here, Ceph is one of the most interesting storage technologies out on the market today – and this includes both open-source and commercial offerings. It's exceptionally well designed, extremely scalable, and useful for a frighteningly diverse set of usage scenarios. Up to this point, Ceph development has been driven and funded by New Dream Network , a long-standing hosting provider operating out of Southern California since 1997 under the DreamHost brand. Now, it's being launched into its own company. Inktank is about to offer professional services and training around the Ceph stack. I've had the pleasure to meet with Inktank President & COO Bryan Bogensberger and others at the OpenStack conference in San Francisco. Indeed, meeting with them was one of my motivations for being there – besides high availability in OpenStack , of course. What Inktank enables us to do is to remain involved in the Ceph community even more than we previously were. We're already offering Ceph instruction as part of our High Availability Expert and Cloud Bootcamp for OpenStack training classes. Martin has presented Ceph at CeBIT in Germany this year. He has also just published a well-received article on Ceph in the U.S. edition of ADMIN magazine , and I have another one coming up in next month's Issue 218 of Linux Journal . So, we're excited for Inktank and wish them the best – even though we sadly can't be at their launch party in Las Vegas on May 8 . We appreciate the invitation, guys – have fun! This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/05/03/an-exciting-day-for-the-ceph-community/","loc":"blog/2012/05/03/an-exciting-day-for-the-ceph-community/"},{"title":"A look back at my first OpenStack Design Summit & Conference","text":"I've just returned from the OpenStack Folsom Design Summit and Spring 2012 Conference , and am finally getting rid of my jet lag. Here's a summary of what's been a mind-blowing conference experience for me. This was my first OpenStack Design Summit and Conference. And as anyone who's in open source is acutely aware, some communities can be reluctant to accept newcomers. Some may even seem outright hostile to the timid. Not the OpenStack community. The minute I sat down in the opening session of the Design Summit on Monday, I felt instantly welcome and at home. Even as a relative OpenStack newbie (who was invited to the Design Summit to provide some insights and guidance on high availability), I immediately got the impression that I was in the right place at the right time. I've rarely seen a developer community on such a positive vibe. Sure, we'll blast each other on technical disagreements, but all in a good-natured, fun way. My own Design Summit session clearly wasn't without such disagreements, and expectedly so. But I think we came to some excellent conclusions: Infrastructure high availability will be an overarching design goal in the upcoming OpenStack Folsom release. We will shoot for providing HA solutions for all OpenStack infrastructure services. This includes MySQL, RabbitMQ, Glance, Keystone, Nova and Horizon (Swift already has HA built in). hastexo will play a very active role in this. We will not reinvent the wheel, and instead rely on the Pacemaker stack wherever possible. Most of the challenge is really in the documentation and in the development of reference solutions that deployment solutions ( Juju , Chef , Puppet ) can then build on. We will take a lot of responsibility in that effort, as well. Some services still require some work to become fully HA capable. Cinder (the volume service that's being factored out of Nova for Folsom) is one example, Quantum is another. This work will be tackled. We're currently planning to stop short of providing monitoring and HA for Nova instances (a.k.a. guest HA ). This is on the list for the next release past Folsom. With those issues discussed, voted on and on the record, I had the honor of presenting them to a larger audience at the main conference . It seems to have hit home pretty well, based on feedback from attendees given in-person and on Twitter. I'm hoping the conference organizers will make a video recording available shortly. Meanwhile, my presentation is already available here . It's now available here in the Presentations section. Overall, this has been a wonderful and very well organized conference, and I'm very much looking forward to coming back next time around. This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/04/24/a-look-back-at-my-first-openstack-design-summit-conference/","loc":"blog/2012/04/24/a-look-back-at-my-first-openstack-design-summit-conference/"},{"title":"Reliable, Redundant, Resilient: High Availability in OpenStack","text":"I explain the high availability features in the upcoming OpenStack Folsom release at the OpenStack Conference Spring 2012. This presentation was delivered April 21, 2012 in San Francisco, California. Slides: Prezi This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/reliable-redundant-resilient-high-availability-in-openstack/","loc":"resources/presentations/reliable-redundant-resilient-high-availability-in-openstack/"},{"title":"Speaking at OSCON 2012","text":"I'll be speaking at OSCON 2012 in Portland, on high availability in OpenStack . I learned from O'Reilly yesterday that my presentation proposal for this year's OSCON , which takes place July 16-20, 2012 in Portland, Oregon, has been accepted. As this is my first OSCON speaking slot (actually, it's my first OSCON altogether), this is a thrilling speaking opportunity for me. My talk, Highly Available Cloud: OpenStack integration with Pacemaker is currently (tentatively, I suppose) scheduled for 11:30 on July 18. This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/04/03/speaking-at-oscon-2012/","loc":"blog/2012/04/03/speaking-at-oscon-2012/"},{"title":"Presentation accepted for OpenStack Spring 2012 Conference","text":"I just learned that my presentation is going ahead at the OpenStack Spring 2012 Conference . My presentation, Reliable, Redundant: High Availability in OpenStack, has been accepted for the main conference track. The official schedule isn't yet up pending confirmation from all speakers, but I've been tentatively informed that it's on at 11:30 am on Friday, April 20. This of course means that you should totally register for the conference if you haven't already done so, and I'll be happy to chat with anyone interested in OpenStack HA. See you in San Francisco! This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/03/28/presentation-accepted-for-openstack-spring-2012-conference/","loc":"blog/2012/03/28/presentation-accepted-for-openstack-spring-2012-conference/"},{"title":"Mandatory and advisory ordering in Pacemaker","text":"Ever wonder what's the difference between order <name> inf: <first-resource> <second-resource> and a score of something other than inf ? We'll explain. If you specify an order constraint score of INFINITY ( inf or the keyword mandatory in crm shell syntax), then the order constraint is considered mandatory. If you specify 0 , or the keyword advisory then it's advisory. What does that mean? Firstly, anytime two resources are started in the same cluster transition, order constraints do apply regardless of whether they're mandatory or advisory. So for the two constraints shown here: order o_foo_before_bar inf: foo bar order o_foo_before_bar 0: foo bar … if foo and bar are just starting, foo starts first, and bar starts only when foo ‘s start operation is completed. So what's the difference, really? Mandatory ordering In a mandatory order constraint, the order is enforced under all circumstances. Consider the following example (primitive definitions omitted to keep this short): order o_foo_before_bar inf: foo bar Suppose foo fails. Now foo must be recovered, but before that, bar must also stop. So the sequence of events is: foo fails Pacemaker attempts to stop foo again (to make sure it's cleaned up). bar stops. foo starts bar starts. If foo fails to start back up, then bar will remain stopped. Based on the start-failure-is-fatal and migration-threshold settings both resources can now potentially migrate to other nodes, but if foo can't be started anywhere, bar also remains stopped. Advisory ordering In an advisory order constraint, the order is enforced only if both resources start in the same transition. Otherwise, it's ignored. Consider the following example (primitive definitions again omitted): order o_foo_before_bar 0: foo bar Again, suppose foo fails. foo must be recovered, but now bar can keep running as it's not being started in the same transition. Thus: foo fails Pacemaker attempts to stop foo again (to make sure it's cleaned up). foo starts If foo fails to start back up, then bar can continue to run. Still, based on the start-failure-is-fatal and migration-threshold settings applying to foo , either it or both resources (depending on colocation constraints) can potentially migrate to other nodes. So when do I use which? Advisory ordering is good for when your dependent resource can recover from a brief interruption in the resource it depends on. For example, you'll want to fire up your libvirt daemon before you start your Pacemaker-managed virtual machines, but if libvirtd were ever to crash you can restart it without needing to restart VMs. Mandatory ordering is for stricter dependencies. Filesystems mounted from an iSCSI device will probably want to be remounted if the iSCSI initator has reported an error. Likewise, you'll probably also want to restart the applications working with that filesystem. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/mandatory-and-advisory-ordering-pacemaker/","loc":"resources/hints-and-kinks/mandatory-and-advisory-ordering-pacemaker/"},{"title":"High Availability in OpenStack","text":"A few thoughts on high availability features (or the current absence thereof) in OpenStack. I've just proposed a session for the OpenStack Folsom design summit which Jay Pipes was nice enough to invite me to (thanks!), and I thought I'd write up a few thoughts of mine ahead of time to get the discussion started. A little while back, Tristan van Bokkem started a discussion on high availability for Nova on the OpenStack mailing list. So in Nova specifically, there are a few components where high availability is readily available; you just have to use it. MySQL. That's a no-brainer. MySQL HA with Pacemaker has been done so many times that I won't rehash it here. What's nice in this regard is that Galera (included in Percona XtraDB Cluster ) now promises to do away with the limitations of both DRBD and traditional MySQL replication , and provide multiple-node, multiple-master synchronous replication for MySQL. As I'm sure you're aware, classic MySQL replication isn't synchronous, and DRBD can't do multi-node master-master, but the Galera based solution looks promising, if not as mature as the other two . Of course, I don't understand why the Galera folks had to reinvent not only replication (which makes sense) but also cluster membership and management (which doesn't), but that's a different discussion to be had altogether. RabbitMQ. Has somewhat similar HA considerations as MySQL. A Pacemaker/DRBD-based solution exists, but is considered deprecated by the RabbitMQ maintainers . Enter mirrored queues, where again the developers seemingly threw out the baby with the bath water and rather than just reimplementing replication (sensible), they came up with their own cluster manager (questionable). Their mirrored queues would probably have played very nicely with master/slave sets in Pacemaker. As Tom Ellis pointed out in another email the previously mentioned thread, there are more HA considerations for services in Nova proper. nova-volume still has a lot of work to do. It has an iSCSI driver which can of course be used as an iSCSI proxy pointed at a highly available, potentially DRBD-backed, software iSCSI target. Or at an iSCSI based hardware solution that has HA built-in, such as HP LeftHand. Alternatively, we could just operate on RBD volumes (part of Ceph ) which will also take care of redundancy for us, and add seamless scaleout and remirroring. That being said, there is currently no real HA provision for the nova-volume service itself, and that's something that will be required. Compute nodes can all run their own instance of nova-api. Front-end API servers can all run nova-scheduler, with a load balancer in front of them. The Pacemaker stack has the potential of being a nice fit for most of the above. It comes with iSCSI target support (RBD doesn't need Pacemaker on the server end, as Ceph takes care of its own HA). Pacemaker also ties in directly with upstart, so any upstart job can be monitored as a Pacemaker service. And Pacemaker's clone facility makes it easy to run multiple instances of inherently stateless services with minimal configuration. What's more, Pacemaker comes with full integration for the ldirectord load-balancing service. Of course, Pacemaker adds a reliable communications layer ( Corosync ) and a multi-master, self-replicating configuration facility. As for non-Nova Openstack services, Glance could use some Pacemaker integration (not hard to do; it's just that someone has to do it). Ceph, in my opinion, has the very interesting potential of being a redundant, scalable storage one-stop shop for OpenStack. It serves the purposes of both volume/block storage (with RBD) and object storage (with RADOS/radosgw). And, as already pointed out, it comes with HA, replication, and scalability built-in. Comments and feedback on the above are much appreciated. For OpenStack developers who visit this blog for the first time: you need to login to post comments in our effort to combat comment spam – but you can simply use your Launchpad OpenID to do so. This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/03/21/high-availability-in-openstack/","loc":"blog/2012/03/21/high-availability-in-openstack/"},{"title":"On my (ex-)maintainership of the DRBD User's Guide","text":"Here's a quick summary of my past and current relationship with the DRBD User's Guide. As you probably know, I created the original DRBD User's Guide several years back, and I maintained it throughout my time at Linbit. When I left last year , it was originally mutually understood (or so I thought) that I could continue to maintain it – as a non-employee, in a community capacity, without compensation, just as it's common in many other open source projects. I tend to enjoy technical writing, and it was something I certainly was looking forward to. And things got off to a promising start, the first (trivial) patch to the documentation which I submitted in my new life was quickly merged without issue . When I made my second and third submission to the documentation, the latter of which was a a bit more elaborate, things got a bit strange. This was after hastexo went operational, although whether that is at all related to the sequence of events I don't know. At any rate, I was being served with a \"Documentation Contributor License Agreement\". Which wasn't considered necessary in my earlier patch. Which involved copyright assignment. Which I balked at. I don't necessarily object to copyright assignment if I write on a contract, as I occasionally do for technical magazines – but what I wrote definitely hadn't been contracted out to me. I had simply submitted it unsolicited in the mere hope it was going to be useful, and I wasn't interested in contract work, either. In addition, the documentation was (and is) under a liberal CC-BY-SA license which made any copyright assignment unnecessary for a simple contribution. In my humble opinion, that is. So I raised these points, and my concerns were rejected, and my patches didn't make it in. Note, I have no quarrel with this at all – it may well be a perfectly sane business decision. But that's none of my business anymore, and I respect their decision just fine. They have stuck to their decision, and that is just fine too. It only means that I don't maintain the User's Guide anymore, and I'm evidently also unable to contribute patches, corrections or improvements unless I consent to copyright assignment, which I disagree with in this instance. So unless the policy changes at some point in the future, I won't be contributing to the User's Guide any longer. My name will remain in the authors list pretty much indefinitely (unless someone publishes a complete rewrite) as that is required by law, but you should interpret that as my being the original author – technically a co-author, as I always made a point of crediting Lars' and Phil's earlier work that the User's Guide was based on. My co-authorship doesn't imply, however, that I'm a currently active author or maintainer. This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/03/20/on-my-ex-maintainership-of-the-drbd-users-guide/","loc":"blog/2012/03/20/on-my-ex-maintainership-of-the-drbd-users-guide/"},{"title":"Managing cron jobs with Pacemaker","text":"It's not uncommon in Pacemaker clusters to run specific cron jobs only on a node that currently runs a particular resource. The ocf:heartbeat:symlink resource agent can be exceptionally helpful in this situation. Here's how to use it. Suppose you've got a cron job for Postfix whose definition normally lives in /etc/cron.d/postfix . All your Postfix related data is in a mountpoint /srv/postfix (that filesystem could live on iSCSI, or DRBD, or it could be a GlusterFS mount – that's irrelevant for the purposes of this discussion). And as such, you've moved your cron definition to /srv/postfix/cron . Now you want that cron job to execute only on the node that also is currently the active Postfix host. That's not hard at all: primitive p_postfix ocf:heartbeat:postfix \\ params config_dir=\"/etc/postfix\" \\ op monitor interval=\"10\" primitive p_symlink ocf:heartbeat:symlink \\ params target=\"/srv/postfix/cron\" \\ link=\"/etc/cron.d/postfix\" \\ backup_suffix=\".disabled\" \\ op monitor interval=\"10\" primitive p_cron lsb:cron \\ op monitor interval=10 order o_symlink_before_cron inf: p_symlink p_cron colocation c_cron_on_symlink inf: p_cron p_symlink colocation c_symlink_on_postfix inf: p_symlink p_postfix What this will do for you is this: Check whether a file named postfix already exists in /etc/cron.d If it does, rename it to postfix.disabled (remember, cron ignores job definitions with dots in the filename) (Re-)Create the postfix job definition as a symlink to /srv/postfix/cron Restart cron when it's done. The c_symlink_on_postfix colocation ensures that all of this happens on the node where the p_postfix resource is also active. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/managing-cron-jobs-pacemaker/","loc":"resources/hints-and-kinks/managing-cron-jobs-pacemaker/"},{"title":"Storage Replication in High-Performance High-Availability Environments","text":"At linux.conf.au 2012, Florian I this presentation on the integration of DRBD , Flashcache and Pacemaker in the High Availability and Distributed Storage miniconf. In this 30-minute presentation, I explore the benefits of Flashcache for replicated storage. Flashcache, originally developed at Facebook, is a general purpose, block level cache device implemented in the Linux device-mapper framework. You can use flashcache in two distinct ways in Pacemaker high-availability clusters, which I both explain in his talk. Video: YouTube This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/storage-replication-high-performance-high-availability-environments/","loc":"resources/presentations/storage-replication-high-performance-high-availability-environments/"},{"title":"Roll Your Own Cloud","text":"Tim Serong and I explore the capabilities of KVM, iSCSI, DRBD and Pacemaker to create a fully open-source enterprise cloud. Shot at linux.conf.au 2011 in Brisbane, this is me babbling, and Tim live-cartooning to the delight of the audience. Video: YouTube This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/roll-your-own-cloud/","loc":"resources/presentations/roll-your-own-cloud/"},{"title":"What's a Totem \"Retransmit List\" all about in Corosync?","text":"Occasionally, you may see errors similar to this in your system logs: corosync [TOTEM ] Retransmit List: e4 e5 e7 e8 ea eb ed ee Here's what causes them, and what you can do to fix the issue. Corosync, more specifically its Totem protocol implementation, defines a maximum number of cluster messages that can be sent during one token rotation. By default, that number is 50, but you may modify this value by setting the window_size parameter in your corosync.conf configuration file. When among several fast cluster nodes (\"processors\" in Totem speak) there are one or few slow ones, the kernel receive buffers can't cope, messages get lost, and they then need to be retransmitted. This is what causes the Retransmit List notifications in the syslogs. This doesn't mean you're losing any messages or data. But it does mean that your cluster performance degrades when this happens, and thus you should really fix that problem. There are a few considerations that apply to tuning Corosync's window_size : If you have a small cluster (say, 8 nodes or less), and they all can be expected to perform equally well because they have identical or nearly-identical hardware, then setting a large window_size of up to 300 should be fine. If your cluster is rather heterogeneous, then you should probably stick with the default of 50. Definitely don't go higher than 256000/MTU, where MTU is that of the network interface(s) Corosync communicates over. For a standard Ethernet interface the default MTU is 1500, which would make for a maximum window_size of 170. If you're running on the generally safe default of 50, and you're still getting Retransmit List notifications, then one of your nodes is most likely significantly slower than the others, and you had better find the cause of that and fix it. The node could be under constant excessive load, or have a problem with its network driver, or may be plugged into an incorrectly-configured switch port. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/whats-totem-retransmit-list-all-about-corosync/","loc":"resources/hints-and-kinks/whats-totem-retransmit-list-all-about-corosync/"},{"title":"The Zen of Pacemaker","text":"I team up with Tim Serong and Andrew Beekhof for a tutorial at linux.conf.au 2012. Pacemaker author Andrew Beekhof dropped in as well, to field questions and provide additional insight into Pacemaker development. The tutorial covers the configuration of a MySQL 2-node high availability cluster front to back, diving into the configuration of replicated storage, the cluster communications infrastructure, cluster resource management, and of course the MySQL database itself. Reviews for this tutorial were rather enthusiastic, with bloggers comparing the experience to an enlightenment session from the Jedi grand masters of high availability. Video: YouTube Slides: SlideShare This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/zen-pacemaker/","loc":"resources/presentations/zen-pacemaker/"},{"title":"Finding out which OSDs currently store a specific RADOS object","text":"Ever wanted to know just which of your OSDs a RADOS object is currently stored in? Here's how. Suppose you've got an RBD device, named test . Then you can use the rbd info command to display which name prefix is used by the RADOS objects that make up the RBD: ceph04:~ # rbd info test rbd image 'test': size 1024 MB in 256 objects order 22 (4096 KB objects) block_name_prefix: rb.0.0 parent: (pool -1) In this example, the prefix we're looking for is rb.0.0 . What's the RBD currently made of? ceph04:~ # rados -p rbd ls | grep \"&#94;rb.0.0.\" rb.0.0.000000000000 rb.0.0.000000000020 rb.0.0.000000000021 rb.0.0.000000000040 rb.0.0.000000000042 rb.0.0.000000000060 rb.0.0.000000000063 rb.0.0.000000000080 rb.0.0.000000000081 rb.0.0.000000000082 rb.0.0.000000000083 rb.0.0.000000000084 rb.0.0.000000000085 rb.0.0.000000000086 rb.0.0.000000000087 rb.0.0.000000000088 rb.0.0.0000000000a0 rb.0.0.0000000000a5 rb.0.0.0000000000c0 rb.0.0.0000000000c6 rb.0.0.0000000000e0 rb.0.0.0000000000e7 rb.0.0.0000000000ff Now suppose you're interested in where rb.0.0.0000000000a5 is. You first grab an OSD map: ceph04 : ~ # ceph osd getmap -o /tmp/osdmap 2012 - 03 - 09 21 : 31 : 47.055376 mon <- [ osd , getmap ] 2012 - 03 - 09 21 : 31 : 47.056624 mon . 1 -> 'got osdmap epoch 187' ( 0 ) wrote 2273 byte payload to / tmp / osdmap And now you can use osdmaptool to test an object name against the mapfile: ceph04 : ~ # osdmaptool --test-map-object rb.0.0.0000000000a5 /tmp/osdmap osdmaptool : osdmap file '/tmp/osdmap' object 'rb.0.0.0000000000a5' -> 0.7 ea1 -> [ 2 , 0 ] … meaning the object lives in Placement Group 0.7ea1 , of which replicas currently exist in OSDs 2 and 0. Why do you want to know this? Normally, really, you don't. All the replication and distribution happens under the covers without your intervention. But you can use this rather neatly if you want to watch your data being redistributed as you take out OSDs temporarily, and put them back in. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/which-osd-stores-specific-rados-object/","loc":"resources/hints-and-kinks/which-osd-stores-specific-rados-object/"},{"title":"Ceph: tickling my geek genes","text":"Haven't heard of Ceph , the open-source distributed petascale storage stack? Well, you've really been missing out. It's not just a filesystem. It's a filesystem, and a striped/replicated block device provider, and a virtualization storage backend, and a cloud object store, and then some. Most of you will, by now, probably have heard of the Ceph filesystem, a distributed, replicated, extremely scaleable filesystem that went upstream with the 2.6.34 kernel release. But that filesystem is really just a client to something that happens server side, which is much more than just file storage. RADOS , the reliable autonomic distributed object store is a massively distributed, replicating, rack-aware object store. It organizes storage in objects, where each object has an identifier, a payload, and a number of attributes. Objects are allocated to a Placement Group (PG), and each PG maps to one or several Object Storage Devices or OSDs. OSDs are managed by a userspace daemon – everything server-side in Ceph is in userspace, really – and locally map to a simple directory. For local storage, objects simply map to flat files, so OSDs don't need to muck around with local block storage. And they can take advantage of lots of useful features built into advanced filesystems, like extended attributes, clones/reflinks, copy-on-write (with btrfs). Extra points for the effort to not reinvent wheels. The entire object store uses a deterministic placement algorithm, CRUSH (Controlled Replication Under Scaleable Hashing). There's never a central instance to ask on every access, instead, everything can work out where objects are. That means the store scales out seamlessly, and can expand and contract on the admin's whim. And based on that basic architecture, there's a number of entry points and deployment scenarios for the stack: radosgw provides a RESTful API for dynamic cloud storage. And it includes an S3 and Swift frontend to act as object storage for AWS/Eucalyptus and OpenStack clouds, respectively. Qemu-RBD is a storage driver for the Qemu/KVM hypervisor (fully integrated with libvirt) that allows the hypervisor to access replicated block devices that are also striped across the object store – with a configurable number of replicas, of course. RBD is a Linux block device that, again, is striped and replicated over the object store. librados (C) and libradospp (C++) are APIs to access the object store programmatically, and come with a number of scripting language bindings. As you've probably guessed, Qemu-RBD builds on librados. Ceph (the filesystem) exposes POSIX filesystem semantics built on top of RADOS, where all POSIX-related metadata is again stored in the object store. This is a remarkably thin client layer at just 17,000 LOC (compare to GFS2 at 26,000 and OCFS2 at 68,000). In short: it's cool stuff. And it's 100% open source, it's all under the LGPL 2.1, and the developers have made a point of not creating any closed-source \"enterprise\" features – in short, they're not shipping \"open core\". 1 We've recently started contributing to the Ceph project to improve its high-availability cluster integration: we've submitted Pacemaker agents to monitor the Ceph daemons proper (a pretty trivial wrapper for a script that ships with Ceph, for now). And we've also contributed a resource agent to manage an RBD device as a Pacemaker resource . The latter gives Pacemaker users the ability to use RBD devices as a drop-in replacement for iSCSI devices, MD devices under Pacemaker control, or DRBD. The Ceph community has been exceptionally welcoming and has made contributing a pleasure – there's no copyright assignment nonsense, no CLAs, just a very positive attitude toward outside contributions. And in case you want to use a Ceph filesystem as a generally available file system in your Pacemaker cluster (as you would with NFS, GlusterFS, GFS2, or OCFS2), you can do that now, too . However, please be cautioned that that should be considered an experimental feature: the Ceph devs have made it very clear on numerous occasions that they're currently focusing on making RADOS and RBD rock solid, and then they'll tackle the POSIX filesystem layer to get it out of experimental mode. This article originally appeared on my blog on the hastexo.com website (now defunct). The original version used the term crippleware here, which I now consider highly inappropriate. (The term open core , to the best of my recollection, wasn't particularly current in 2012.) I would like to apologize for my use of the previous term. The article contains no other edits in comparison to the 2012 original. ↩","tags":"blog","url":"blog/2012/03/08/ceph-tickling-my-geek-genes/","loc":"blog/2012/03/08/ceph-tickling-my-geek-genes/"},{"title":"Solve a DRBD split-brain in 4 steps","text":"Whenever a DRBD setup runs into a situation where the replication network is disconnected and fencing policy is set to dont-care (default), there is the potential risk of a split-brain. Even with resource level fencing or STONITH setup, there are corner cases that will end up in a split-brain. When your DRBD resource is in a split-brain situation, don't panic! Split-brain means that the contents of the backing devices of your DRBD resource on both sides of your cluster started to diverge. At some point in time, the DRBD resource on both nodes went into the Primary role while the cluster nodes themselves were disconnected from each other. Different writes happened to both sides of your cluster afterwards. After reconnecting, DRBD doesn't know which set of data is \"right\" and which is \"wrong\". Indications of a Split-Brain The symptoms of a split-brain are that the peers will not reconnect on DRBD startup but stay in connection state StandAlone or WFConnection. The latter will be shown if the remote peer detected the split-brain earlier and was faster at shutdown its connection. In your kernel logs you will see messages like: kernel : block drbd0 : Split - Brain detected , dropping connection ! 4 Steps to solve the Split-Brain Step 1 Manually choose a node which data modifications will be discarded. We call it the split brain victim. Choose wisely, all modifications will be lost! When in doubt run a backup of the victim's data before you continue. When running a Pacemaker cluster, you can enable maintenance mode. If the split brain victim is in Primary role, bring down all applications using this resource. Now switch the victim to Secondary role: victim# drbdadm secondary resource Step 2 Disconnect the resource if it's in connection state WFConnection : victim # drbdadm disconnect resource Step 3 Force discard of all modifications on the split brain victim: victim # drbdadm -- -- discard - my - data connect resource for DRBD 8.4.x: victim # drbdadm connect -- discard - my - data resource Step 4 Resync will start automatically if the survivor was in WFConnection network state. If the split brain survivor is still in Standalone connection state, reconnect it: survivor # drbdadm connect resource At the latest now the resynchronization from the survivor ( SyncSource ) to the victim ( SyncTarget ) starts immediately. There is no full sync initiated but all modifications on the victim will be overwritten by the survivor's data and modifications on the survivor will be applied to the victim. Background: What happens? With the default after-split-brain policies of disconnect this will happen always in dual primary setups. It can happen in single primary setups if one peer changes at least once its role from Secondary to Primary while disconnected from the previous (before network interruption) Primary. There are a variety of automatic policies to solve a split brain but some of them will overwrite (potentially valid) data without further inquiry. Even with theses policies in place a unresolvable split-brain can occur. The split-brain is detected once the peers reconnect and do their DRBD protocol handshake which also includes exchanging of the Generation Identifiers (GIs). This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/solve-drbd-split-brain-4-steps/","loc":"resources/hints-and-kinks/solve-drbd-split-brain-4-steps/"},{"title":"Checking Corosync cluster membership","text":"It's simple and easy to get Pacemaker's view of the status of members in a cluster – just invoke crm_mon . But what if you want to check on the cluster membership when Pacemaker is not running, or you want to make sure whether Corosync's view of the cluster is identical to Pacemaker's? Here's how. Checking ring status with corosync-cfgtool The corosync-cfgtool utility displays the cluster connectivity status when invoked with the -s flag: # corosync-cfgtool -s Printing ring status . Local node ID 303938909 RING ID 0 id = 10.0 . 1.1 status = ring 0 active with no faults RING ID 1 id = 192.168 . 42.1 status = ring 1 active with no faults The above is the status of two healthy rings; a failed ring (one affected by a network interruption, for example) would show a FAULTY status. There's a catch. In a two-node cluster, if both nodes were to start while all cluster communication links are down, then Corosync would form two memberships with healthy, one-member rings. Both of the nodes would show a ring status similar to the above, but your cluster still wouldn't be communicating. So, you can't rely on corosync-cfgtool -s alone. You must also check Corosync's member list. Querying the member list with corosync-cmapctl We can examine Corosync's cluster member list with the corosync-cmapctl command: # corosync-cmapctl | grep member runtime.totem.pg.mrp.srp.members.303938909.ip=r(0) ip(10.0.1.1) r(1) ip(192.168.42.1) runtime.totem.pg.mrp.srp.members.303938909.join_count=1 runtime.totem.pg.mrp.srp.members.303938909.status=joined runtime.totem.pg.mrp.srp.members.320716125.ip=r(0) ip(10.0.1.2) r(1) ip(192.168.42.2) runtime.totem.pg.mrp.srp.members.320716125.join_count=1 runtime.totem.pg.mrp.srp.members.320716125.status=joined In this example, we have two nodes (with node IDs 303938909 and 320716125 ). They are both configured to use two communication rings, r(0) and r(1) , and both of them have successfully joined the cluster. Note: In earlier Corosync releases (pre-2.0), the corosync-cmapctl tool was called corosync-objctl . Its command syntax for querying the member list was identical. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/checking-corosync-cluster-membership/","loc":"resources/hints-and-kinks/checking-corosync-cluster-membership/"},{"title":"Fencing in Libvirt/KVM virtualized cluster nodes","text":"Often, people deploy the Pacemaker stack in virtual environments for purposes of testing and evaluation. In such environments, it's easy to test Pacemaker's fencing capabilities by tying in with the hypervisor. This quick howto illustrates how to configure fencing for two virtual cluster nodes hosted on a libvirt/KVM hypervisor host. libvirt configuration (hypervisor) In order to do libvirt fencing, your hypervisor should have its libvirtd daemon listen on a network socket. libvirtd is capable of doing this, both on an encrypted TLS socket, and on a regular, unencrypted TCP port. Needless to say, for production use you should only use TLS, but for testing and evaluation – and for that purpose only – TCP is fine. In order for your hypervisor to listen on an unauthenticated, insecure, unencrypted network socket (did we mention that's unsuitable for production?), add the following lines to your libvirtd configuration file: listen_tls = 0 listen_tcp = 1 tcp_port = \"16509\" auth_tcp = \"none\" You can also set the listen_addr parameter, for example to have libvirtd listen only on the network that your virtual machines run in. If you don't set listen_addr, libvirtd will simply listen on the wildcard address. You'll also have to add the -l or --listen flag to your libvirtd invocation. On Debian/Ubuntu platforms, you can do so by editing the /etc/default/libvirt-bin configuration file. Once you've done that, you can use netstat -ltp to check whether libvirtd is in fact listening on its configured port, 16509/tcp. Also, make sure that you don't have a firewall blocking that port. libvirt configuration (virtual machines) Inside your virtual machines, you'll also have to install the libvirt client binaries – the fencing mechanism uses the virsh utility under the covers. Some platforms provide a libvirt-client package for that purpose; for other's, you'll simply have to install the full libvirt package. Once that is set up, you should be able to run this command from inside your virtual machines: virsh --connect = qemu+tcp://<IP of your hypervisor>/system \\ list --all … and that command should list all the domains running on that host, including the one you're connecting from. Pacemaker configuration In one of your virtual machines, you can now set up your fencing configuration. This example assumes that you have two nodes named alice and bob, that their corresponding virtual machine domain names are also alice and bob, and that they can reach their hypervisor by TCP at 192.168.0.1: primitive p_fence_alice stonith:external/libvirt \\ params hostlist=\"alice\" \\ hypervisor_uri=\"qemu+tcp://192.168.0.1/system\" \\ op monitor interval=\"60\" primitive p_fence_bob stonith:external/libvirt \\ params hostlist=\"bob\" \\ hypervisor_uri=\"qemu+tcp://192.168.0.1/system\" \\ op monitor interval=\"60\" location l_fence_alice p_fence_alice -inf: alice location l_fence_bob p_fence_bob -inf: bob property stonith-enabled=true Now you can test fencing to the best of your abilities. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/fencing-libvirtkvm-virtualized-cluster-nodes/","loc":"resources/hints-and-kinks/fencing-libvirtkvm-virtualized-cluster-nodes/"},{"title":"Network connectivity check in Pacemaker","text":"If you want a Pacemaker cluster to move resources on changes on the network connectivity of an individual node, there are two major steps involved: Let Pacemaker monitor connectivity; Configure constraints to react on connectivity changes. Prerequisites Be sure to run at least Pacemaker 1.0.11 or 1.1.6 to include some important fixes affecting the ocf:pacemaker:ping resource agent. Preferably, choose more than one reliable ping targets in your network (like a highly available gateway router, a core switch, or DNS server). Pacemaker configuration The following crm shell code snippet configures a cloned ping resource including constraints to run Dummy resources on any node that has connectivity at all. Please note, that the first constraint forbids to run p_dummy1 if all nodes lose connectivity. The second constraint places p_dummy2 on the node that has the best connectivity: primitive p_ping ocf:pacemaker:ping \\ params host_list=\"dns.example.com router.example.com\" \\ multiplier=\"1000\" dampen=\"60s\"\\ op monitor interval=\"10s\" clone cl_ping p_ping primitive p_dummy1 ocf:pacemaker:Dummy primitive p_dummy2 ocf:pacemaker:Dummy location l_dummy1_needs_connectivity p_dummy1 \\ rule -inf: not_defined pingd or pingd lte 0 location l_dummy2_likes_best_connectivity p_dummy2 \\ rule pingd: defined pingd This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/network-connectivity-check-pacemaker/","loc":"resources/hints-and-kinks/network-connectivity-check-pacemaker/"},{"title":"Speaking at the 2012 Percona Live MySQL Conference","text":"This year, I have the pleasure of returning to the MySQL Conference & Expo as a speaker. Percona have picked up the torch that O'Reilly had held as the conference organizers, and they're putting together a 3-day conference this year. I am co-presenting a tutorial with Yves Trudeau from Percona. Our tutorial is called High Availability Deep Dive: Pacemaker, DRBD, MySQL Replication, and more! and it's going to be the only full-day tutorial offered in this year's conference. In it, Yves and I are going to cover An overview of the Pacemaker cluster stack (the classic \"this is Pacemaker\" introduction) - DRBD-backed MySQL replication (another classic and widely deployed scenario) - MySQL replication under Pacemaker management (a new option which Yves has vastly improved through a big patch set to the MySQL RA). Do I expect this talk to be controversial? Definitely. The amount of \"Pacemaker is terrible\" and \"Pacemaker is unsuitable for managing highly available databases\" that has been around the blogosphere lately is pretty mind-boggling. But strangely enough, most of the things brought forward against Pacemaker by its detractors seem like a time-warp back to about 2007. \"We must use XML to manage Pacemaker!\" Nonsense. In fact, that was never true – the release of Pacemaker as a separate project and the release of the crm shell coincided. Ever since, Pacemaker configuration has been as text-based as MySQL itself. - \"All Pacemaker can do is react to node failure!\" Nothing could be further from the truth. Pacemaker has some of the most sophisticated resource monitoring and auto-recovery capabilities under the sun. - \"OK. But all it can do to react to resource failure is kill a daemon!\" Bogus again. It will happily do whatever the resource agent specifies. Or the admin, through the configuration. In our tutorial, we're going to dispel a few of these myths. We certainly make no claims as to Pacemaker being the one and only solution for MySQL HA, but it's one that serves lots of use cases excellently. Needless to say, I'll also hang around for the conference proper, and I'm very much looking forward to seeing lots of familiar faces. I'll also remain in the Bay Area for some time after the MySQL conference – more on that in a day or two. This article originally appeared on my blog on the hastexo.com website (now defunct).","tags":"blog","url":"blog/2012/02/27/speaking-2012-percona-live-mysql-conference/","loc":"blog/2012/02/27/speaking-2012-percona-live-mysql-conference/"},{"title":"GFS2 in Pacemaker (Debian/Ubuntu)","text":"Setting up GFS2 in Pacemaker requires configuring the Pacemaker DLM, the Pacemaker GFS control daemon, and a GFS2 filesystem itself. Prerequisites GFS2 with Pacemaker integration is supported on Debian ( squeeze-backports and up) and Ubuntu (10.04 LTS and up). You'll need the dlm-pcmk , gfs2-tools , and gfs-pcmk packages. Fencing is imperative. Get a proper fencing/STONITH configuration set up and test it thoroughly. Pacemaker configuration The Pacemaker configuration, shown here in crm shell syntax, normally puts all the required resources into one cloned group. Have a look at this configuration snippet: primitive p_dlm_controld ocf:pacemaker:controld \\ params daemon=\"dlm_controld.pcmk\" \\ op start interval=\"0\" timeout=\"90\" \\ op stop interval=\"0\" timeout=\"100\" \\ op monitor interval=\"10\" primitive p_gfs_controld ocf:pacemaker:controld \\ params daemon=\"gfs_controld.pcmk\"\\ op start interval=\"0\" timeout=\"90\" \\ op stop interval=\"0\" timeout=\"100\" \\ op monitor interval=\"10\" primitive p_fs_gfs2 ocf:heartbeat:Filesystem \\ params device=\"<your device path>\" \\ directory=\"<your mount point>\" \\ fstype=\"gfs2\" \\ op monitor interval=\"10\" group g_gfs2 p_dlm_controld p_gfs_controld p_fs_gfs2 clone cl_gfs2 g_gfs2 \\ meta interleave=\"true\" Then when that's done, your filesystem should happily mount on all nodes. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/gfs2-pacemaker-debianubuntu/","loc":"resources/hints-and-kinks/gfs2-pacemaker-debianubuntu/"},{"title":"Interleaving in Pacemaker clones","text":"Ever wonder what meta interleave really means in a Pacemaker clone definition? We'll explain. The interleave meta attribute is only valid on Pacemaker clone definitions – and their extended version of sorts, master/slave sets. It's not available on primitives and groups. Clones are often used in configurations involving cluster filesystems, such as GFS2 ( here's an example ). Consider the following example (primitive definitions omitted to keep this short): clone cl_foo p_foo meta interleave=false clone cl_bar p_bar meta interleave=false order o_foo_before_bar inf: cl_foo cl_bar What this means is for the order constraint to be fulfilled, all instances of cl_foo must start before any instance of cl_bar can. Often, that's not what you want. In contrast, consider this: clone cl_foo p_foo meta interleave=true clone cl_bar p_bar meta interleave=true order o_foo_before_bar inf: cl_foo cl_bar Here, for each node, as soon as the local instance of cl_foo has started, the corresponding local instance of cl_bar can, too. This is what's usually desired – when in doubt, allow interleaving. One thing that often throws people is that interleaving only works when Pacemaker is configured to run the same number of instances of two clones on the same node. Thus, clone cl_foo p_foo\\ meta interleave=true \\ globally-unique=true clone-node-max=2 clone cl_bar p_bar meta interleave=false order o_foo_before_bar inf: cl_foo cl_bar … won't work, as Pacemaker is allowed to run 2 instances of cl_foo on the same node, but only one of cl_bar (the default for clone-node-max is 1). Also, globally-unique=true is a requirement for any clone-node-max >1 – which means that interleaving between a globally-unique and a not globally-unique clone is also not supported. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/interleaving-pacemaker-clones/","loc":"resources/hints-and-kinks/interleaving-pacemaker-clones/"},{"title":"OCFS2 in Pacemaker (Debian/Ubuntu)","text":"Setting up OCFS2 in Pacemaker requires configuring the Pacemaker DLM, the O2CB lock manager for OCFS2, and an OCFS2 filesystem itself. Prerequisites OCFS2 with Pacemaker integration is supported on Debian ( squeeze-backports and up) and Ubuntu (10.04 LTS and up). You'll need the dlm-pcmk , ocfs2-tools , ocfs2-tools-pacemaker and openais packages. Fencing is imperative. Get a proper fencing/STONITH configuration set up and test it thoroughly. Running OCFS2/Pacemaker integration requires that you load Corosync with the openais_ckpt service enabled. The service definition is in the file /etc/corosync/service.d/ckpt-service which the openais package installs by default. Make sure you did not accidentally delete or disable this file. Pacemaker configuration The Pacemaker configuration, shown here in crm shell syntax, normally puts all the required resources into one cloned group. Have a look at this configuration snippet: primitive p_dlm_controld ocf:pacemaker:controld \\ op start interval=\"0\" timeout=\"90\" \\ op stop interval=\"0\" timeout=\"100\" \\ op monitor interval=\"10\" primitive p_o2cb ocf:pacemaker:o2cb \\ op start interval=\"0\" timeout=\"90\" \\ op stop interval=\"0\" timeout=\"100\" \\ op monitor interval=\"10\" primitive p_fs_ocfs2 ocf:heartbeat:Filesystem \\ params device=\"<your device path>\" \\ directory=\"<your mount point>\" \\ fstype=\"ocfs2\" \\ meta target-role=Stopped \\ op monitor interval=\"10\" group g_ocfs2 p_dlm_controld p_o2cb p_fs_ocfs2 clone cl_ocfs2 g_ocfs2 \\ meta interleave=\"true\" Why keep the filesystem stopped? Because you probably either don't have a configured OCFS2 filesystem on your device yet, or your ran mkfs.ocfs2 when the Pacemaker stack wasn't running. In either of those two cases, mount.ocfs2 will refuse to mount the filesystem. Thus, fire up your DLM and the o2cb process like the above configuration does, and then: If you haven't got a filesystem yet, run mkfs.ocfs2 on your device, or If you do already have one, run tunefs.ocfs2 --update-cluster-stack <device> . Then when that's done, run crm resource start p_fs_ocfs2 and your filesystem should happily mount on all nodes. This article originally appeared on the hastexo.com website (now defunct).","tags":"hints-and-kinks","url":"resources/hints-and-kinks/ocfs2-pacemaker-debianubuntu/","loc":"resources/hints-and-kinks/ocfs2-pacemaker-debianubuntu/"},{"title":"Fencing and Maintaining Sanity in High-Availability Clusters","text":"A 45-minute talk I co-presented with Madison Kelly at Linuxcon Europe 2011 in Prague. We explain the purpose of fencing, options for implementing fencing, and common pitfalls. This presentation rounded out a series of high-availability talks at Linuxcon Europe 2011, the first Linuxcon event hosted by the Linux Foundation in Europe. Madison and I presented at the Clarion Congress hotel, in the main conference track. Slides: SlideShare This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/fencing-and-maintaining-sanity-high-availability-clusters/","loc":"resources/presentations/fencing-and-maintaining-sanity-high-availability-clusters/"},{"title":"MySQL High Availability Sprint: Launch the Pacemaker!","text":"This is a very dense tutorial given at Percona Live UK 2011 in London, England. In three hours, I covered the MySQL HA Stack with Pacemaker and DRBD, front to back. Slides: SlideShare This article originally appeared on the hastexo.com website (now defunct).","tags":"presentations","url":"resources/presentations/mysql-high-availability-sprint-launch-pacemaker/","loc":"resources/presentations/mysql-high-availability-sprint-launch-pacemaker/"},{"title":"The Review Review","text":"This is a talk I submitted 1 to DevConf.CZ 2022, which used a non-anonymized CfP process via Red Hat's CfP website . For that conference, it was selected as the lead talk in the Modern Software Development track. I had previously submitted this talk to DevOpsDays Tel Aviv 2021, which used a non-anonymized CfP process via PaperCall . That submission was rejected. Title The Review Review: comparing code review, testing, staging and deployment across development collaboration platforms Elevator Pitch You have 300 characters to sell your talk. This is known as the \"elevator pitch\". Make it as exciting and enticing as possible. GitHub, GitLab, Gerrit — what should I choose? What's the best review process, the best CI/CD integration, the best deployment facility? Which should I select for my startup, or consider migrating to? Which supports good collaboration practices, which bad ones? This talk gives the run-down. Talk Format What format is this talk best suited for? Talk (~25-40 minutes) Audience Level Who is the best target audience for this talk? Intermediate Description The description will be seen by reviewers during the CFP process and may eventually be seen by the attendees of the event. You should make the description of your talk as compelling and exciting as possible. Remember, you're selling both the organizers of the events to select your talk, as well as trying to convince attendees your talk is the one they should see. In DevOps, the process of collaborative review, testing, staging, and deployment to production constitutes a core element of the work we do. And we generally strive to make this process as effective, efficient, smooth, and transparent as possible. Achieving that partly comes from the work culture we shape and inhabit, partly from our selection of tools — and of course, work culture and work tools permanently and closely influence each other. This goes for both the tools that drive review, and the tools that drive CI/CD: the GitHub Pull Request process in combination with GitHub Actions ; the GitLab Merge Request process in combination with GitLab CI ; the Gerrit Review process in combination with Zuul . None of these is perfect, all of them have their advantages and disadvantages under particular circumstances. Some are meant to be used principally as a service, some are fine to self-host. Some are adamant about enforcing specific deployment practices, some follow a more relaxed approach. This talk is a summary of the current state of affairs with all these tools, and contains recommendations on what to use under which circumstances. Notes Notes will only be seen by reviewers during the CFP process. This is where you should explain things such as technical requirements, why you're the best person to speak on this subject, etc… My team and I have worked with all tools mentioned in a professional capacity, and I believe I've got a very good understanding of the relative merits of the systems presented. This does not include a hard-and-fast recommendation for one particular tool or platform. This is a talk that's suitable for both in-person and on-line events. Tags Tag your talk to make it easier for event organizers to be able to find. Examples are \"ruby, javascript, rails\". GitHub, GitLab, Gerrit, Zuul, CI/CD, Development, DevOps If you're curious why this is here, please read this . ↩","tags":"talk-submissions","url":"./talk-submissions/review-review/","loc":"./talk-submissions/review-review/"},{"title":"The Review Review","text":"I wanted to share a few thoughts on something I consider a rather important topic in our industry: code review and CI/CD tools, and how they relate. This means that I'm talking about source code management: where we store our code, and how we manage access to it; code review: how we coordinate changes to our code; testing and gating: how we make sure that those changes don't break anything; deployment: how we push changes and updates out to the consumers of our code. In case it's not obvious, that means I'm talking about a large fraction of the software engineering cycle. Not all of it; the part involving \"fooling around\" ( creative play ) is perhaps excluded — but substantially everything where people can be said to be \"developing\" in a software engineering organization is encompassed in these things. And there's a few things that follow from that: First, whatever tools we use in order to accomplish these four things, they simultaneously influence and are influenced by our collaboration culture. It's ludicrous to presume that tools and culture are independent of each other, or to categorically declare that tools must be made to fit processes, not the other way around. That's not how people work. Culture and tools always have an influence on each other. Second, the scope of these things is continually expanding as the field evolves. To illustrate, a few years ago a CI/CD platform could get away with supporting automated unit tests and kicking off an Ansible playbook to deploy things to VMs. Today, what we expect out of a continuous deployment pipeline includes support for a package registry (for Python packages or Node.js modules, to give just two examples), a container image registry (for Docker/Podman/OCI containers), a secret store, the ability to deploy to a Kubernetes cluster. And that's just a few examples. I might be forgetting others. Third, this is a classic example of where we must apply systems thinking : since substantially everything the organization does is connected to the toolchain, we cannot make changes to one part of the system without considering the consequences for the system as a whole. That is not to say that we cannot make incremental changes, just that we can't pretend that anything in the system stands alone. To illustrate what I mean, consider the example of an automotive engineer implementing a design change for an engine. If the design change makes the engine so much more efficient that it means a range extension by 10% then that's excellent. But if in the process the designer has made it impossible to connect the engine to its battery (or the fuel line, if we're talking about obsolescent technology), then installing the new engine doesn't just not improve anything — it renders the vehicle immobile. Responsibility Now, what does that mean about responsibility? Who is ultimately in charge of the system consisting of source code management and review tools, and your CI/CD pipeline? The answer is hopefully a no-brainer: since everything I talk about including your organizational culture encompasses substantially all of your engineering organization, the responsibility rests with whoever is in charge of your engineering organization (in most companies, that's often the CTO). And if you're a software technology company so your entire enterprise is substantially a software engineering organization, it's your CEO's or MD's responsibility. Of course, that person may delegate some of the tasks and details of running your source code management and code review and CI/CD platform, but responsibility stays with them. And that responsibility requires both an understanding of the technology itself , and an understanding of how it interacts with your engineering culture. A profound understanding. And I'd go so far as to say if you head up a software engineering organization and you don't have a profound understanding of this toolchain and its mutual influence on your culture, you should find another job. And if you work in a software engineering organization and the person in charge lacks precisely that profound understanding, you should also find another job, because you deserve better. So having said all that, we can start talking about tools. And I'm going to talk about three of them, all of which I use in some professional capacity on an at-least-weekly basis. GitHub The first one is the toolchain that — I think — a majority of open source developers will be most familiar with: GitHub, whose collaboration model is based on the Pull Request (PR). Now the GitHub PR model was strongly influenced by the distributed development model of the Linux kernel. The kernel project is what Git was originally written for, so naturally it is also where the original convention for pull requests emerged. In kernel development, during a kernel merge window, subsystem maintainers fix up a publicly accessible Git tree for Linus to pull from. They then send a message that follows a conventional format to the linux-kernel mailing list (the LKML) outlining the purpose of the changes they want merged. This email contains a summary of the changes, and then an enumeration of each commit to be merged. (There's a git subcommand, git request-pull , to format such a message.) The review then proceeds in an email exchange on LKML. Once Linus is happy with the change, he pulls from the subsystem maintainer's branch and informs them that their changes have merged. Individual subsystem maintainers replicate this model, perhaps with small modifications, for contributions to the subsystems they are responsible for. GitHub Pull Requests (PRs) GitHub replicates some features of the kernel's model: The collaboration model is generally, \"fork and pull\". Individuals maintain their own forks of an upstream codebase, and then send pull requests when they are ready to review. (However, the review process then uses a web interface, rather than a mailing list — in principle, a GitHub reviewer can do a complete review within the GitHub web interface and source code browser and would never even need to check out the repository locally.) Each PR generally consists of multiple commits, which however are expected to closely relate and serve a common purpose. That common purpose is enumerated in a summary at the top of the pull request. GitHub calls this the PR description. Submitters can mark a PR as a draft, with which they indicate that the PR is not ready to be merged yet. When drafts became available in 2019, they replaced an emerging convention in which PR descriptions would be prefixed by WIP (work in progress) or DNM (do not merge) . GitHub PRs can be approved, rejected or commented on by maintainers or other contributors, and an approval can be made a mandatory requirement for merging, but by default GitHub will let anyone merge the PR who has write permissions to the repository that the PR targets. This includes the possibility for a maintainer to merge the contributor's remote branch to their own local checkout, and then pushing the merged branch to he target repo of the PR. Such an event will automatically close the PR and mark it as merged. GitHub Actions GitHub has, for a long time, allowed maintainers to require that PRs pass automated testing. However, until rather recently, it relied on them to run (or interface with) a separate testing infrastructure outside of GitHub to do that. Typical examples for this included CircleCI, or Travis, or Jenkins. It was only in 2019 that GitHub announced automated testing via GitHub Actions. At the time of writing however, GitHub Actions workflows are in widespread use for CI/CD, but it is still quite common for GitHub-hosted projects to allow maintainers to circumvent CI/CD tests and merge directly. When this happens, it often creates a rather unpleasant situation in which CI/CD testing is only run for contributions by \"outsiders\" or \"newbies\", whereas maintainers get to break things with impunity. This means that issues are often not detected until a casual contributor sends a PR, at which point the test breaks and leave the contributor confused (and sometimes lead to the change not even being considered because, well, \"it makes the tests break.\") Another thing that comes bundled with GitHub (and GitHub workflow actions) is the ability to maintain your own package registry and push artifacts to it from your workflow . Interestingly, at the time of writing, GitHub's definition of \"packages\" includes container images, Ruby gems, and npm modules among others , but presently does not include Python modules — although you do, of course, have the option to push your packages to PyPI from your workflow . GitLab The equivalent to the GitHub pull request (PR) is the GitLab merge request (MR) . In principle, a GitLab MR is quite similar to a GitHub PR, albeit with a few noticeable differences: The \"fork and pull\" model is less prevalent on GitLab. Instead, it is far more common for collaborators to work on one project, and then create topic branches within that project for each set of changes. Since the project repo is shared, this facilitates collaboration on a single changeset by multiple people: if two or more people wish to collaborate on a change, they simply push additional squash or fixup commits on the topic branch. They can also agree to force-push amended commits to the topic branch, in which the GitLab web interface will helpfully point out differences between individual versions of a commit (something that GitHub presently cannot do in a PR). As in a GitHub PR, a GitLab MR is generally expected to include one or more commits. Also as in a GitHub PR, an MR is expected to contain a summary that outlines its purpose. GitLab MRs have a Draft status just like GitHub PRs do, and they were introduced about the same time in both products, but GitLab had a preceding feature called work-in-progress MRs (WIP MRs). GitLab has the handy feature that MRs are automatically marked as drafts once any commit with squash: or fixup: in the commit message ends up in the topic branch — GitLab rightfully infers that the branch still needs a squash rebase prior to merge. GitLab MRs can be reviewed in full using the web interface alone: the review interface and the source code browser are closely integrated, just like in GitHub. GitLab CI CI/CD has been an intrinsic part of the GitLab review experience for years, since GitLab includes full CI integration via the .gitlab-ci.yml configuration file. Since GitLab CI has been around for quite a while, and it has a multitude of ways to be used, it \"feels\" more intrinsic to the review process than GitHub Actions do, which to me still leave an impression of being bolted on. In addition, GitLab CI comes with multiple options of using the CI runner: You can use shared runners, which GitLab operates for you. These are Docker containers that GitLab spins up on your behalf in the cloud, and which you share with other GitLab subscription customers. You can also host your own runners. You can do that in Docker containers, in Kubernetes clusters, in virtual machines, and even on bare metal. The runners need no incoming network connectivity; they simply connect to a service on your GitLab host and then poll whether jobs wait for them. You can also specify runners that are exclusive to a project, or to a group or subgroup of projects. GitLab also comes with a package registry , to which you can push packages from CI pipelines. This differs from GitHub in such a way that it includes more package different formats , including a private PyPI workalike for Python packages. In addition, there's also a separate container registry for container images. Gerrit/Zuul Now, it feels a bit awkward to call this one \"Gerrit/Zuul\" when I've called the others just \"GitHub\" and \"GitLab\" respectively, and tacitly included the corresponding CI integrations (GitHub Actions and GitLab CI, respectively) in them. There are a couple of reasons for that: Zuul is a CI/CD framework that is, in principle, not tied to Gerrit, whereas GitHub Actions only apply to GitHub, and GitLab CI only to GitLab. Gerrit/Zuul is a particular combination that was largely popularized by the OpenStack community, which is why a lot of people who are or were part of that community intuitively associate Gerrit with Zuul and vice versa. Likewise, Gerrit is not tied to a specific CI/CD framework. It's perfectly feasible to run code reviews in Gerrit and use a different CI/CD pipeline (or even none at all). And Gerrit/Zuul does differ quite notably from GitHub and GitLab, whose features often map quite closely to each other, and I'd like to highlight some of those differences. Gerrit reviews The Gerrit review process differs in a few crucial points from the one we know from GitHub and GitLab: You don't ask someone to pull from a branch or a fork or yours. Instead, you run git review and Gerrit will make a branch for you. Everything else flows from there. Unlike a GitHub PR and GitLab MR, which both typically contain a series of commits to be taken as a whole, a Gerrit change is really just that: one change. Which, of course, also means that we don't need a separate summary for the change: the summary is the commit message. It's still possible to submit a series of commits in the course of a Gerrit review. However, Gerrit simply sees those as a series of changes that all depend on one another. Dependencies between changes can also be expressed explicitly, by including appropriate keywords in commit messages. Crucially, these dependencies can cross project boundaries. That is to say, a change in one Git repository can depend on a change in another Git repository, so long as they both use the same Gerrit instance for review. And we also have the equivalent of a Draft PR/MR; in Gerrit that's called a work-in-progress change. Because of this, when used in combination with CI such as Zuul, a Gerrit-reviewed project generally expects CI tests to pass on every commit, without exceptions. This is in contrast to many GitHub or GitLab managed projects, which typically only expect the head commit of the topic branch associated with a PR/MR to pass CI. In Gerrit/Zuul managed projects, it's also Zuul that merges the commit. This is also in contrast to projects that live in GitHub or GitLab: in those, the pipeline run results are generally advisory in nature, and a successful pipeline run must still be confirmed by a human clicking a Merge button (or running a git merge command locally, and then pushing to the repository). In addition, even a failing CI run can generally be overridden by a \"core committer\" who has the ability to merge the PR/MR anyway. A Gerrit/Zuul project typically has no such shortcuts, meaning the only way to get changes into the repo is to pass both peer review, and the CI pipeline. In my experience, this tends to create a climate of leadership by example, which has a beneficial effect on both experienced developers (\"seniors\" in a corporate setting) and newcomers (\"juniors\"). Speculative merging There is one other property that Gerrit/Zuul has that sets it apart from other review/CI toolchains: speculative merging. This involves the parallel execution of CI jobs for interdependent changes . With speculative merging, even complex, long-running CI/CD pipelines don't hold up the development process — and this massively enhances project scalability. No direct repo browser integration Notably, in Gerrit/Zuul there is no close integration with repository browsing. Gerrit does include the Gitiles plugin for the purpose, but its user experience is rudimentary at best. A popular alternative is to deploy Gerrit with Gitea , but again, that's not built-in and your trusted Gerrit/Zuul admin has to set it up for you. In addition, while source code browsing in GitHub and GitLab is tightly integrated with project permissions, and that is also true for Gitiles, there is a certain amount of administrative duplication to make your Gerrit repository and project permissions apply to Gitea. No built-in package registries There's another difference in the Gerrit/Zuul stack when compared to GitHub and GitLab, and that is its absence of built-in package registries. Zuul has ready-to-use jobs for pushing to a container registry , or to PyPI , but you do have to either push to upstream public registries, or build your own. Zuul does not come bundled with multitenant private registries the way GitHub and GitLab do. Administrative complexity In view of the above, there's another thing that you might want to consider, which in my humble opinion is an important reason why the Gerrit/Zuul combination has less uptake than it deserves on its technical merits. And this may sound overly dramatic, but: people like to be in charge of their own actions, and software developers are people. And here's an issue with Zuul: there are quite a few things a developer can do on their own in GitHub Actions or GitLab CI that they'd need to ask an admin's help for in Zuul. Creating a relatively standard workflow of building a private container image, pushing it to your own registry, and then rolling out that image to a Kubernetes deployment, is something you can do in GitHub or GitLab as a project owner. With Zuul, you'll need an admin at least to set up and manage your container registry. Rerunning a pipeline, a simple click of a button or API call in GitHub or GitLab, is something you trigger via a Gerrit keyword (typically recheck ) for Zuul — but only on the pipelines where your admin has defined that trigger . So, which one's best? So you want to know which one of these you should choose (or advocate for)? That's surprisingly difficult to answer, and greatly depends on your priorities. And I'll give you this from four angles. When it comes to scalability — the ability to adapt to massive organizational sizes, and/or rapid project growth, or an obscenely large number or projects within an organization — the Gerrit/Zuul combination wins hands down if you have a competent, responsive, and dedicated crew to manage it. When it's about getting started quickly — helping a project get off the ground with a good, usable, easily manageable review and fully integrated CI/CD structure — you can't beat GitLab. In terms of beneficial effect on your development culture, Gerrit/Zuul again probably scores best. If you have a team that's great at reviews and commit and CI and doesn't cut corners, or you want to build a team like that, Gerrit/Zuul can really help. And when it's about giving developers the lowest barrier to entry — meaning using tools that they're most likely already familiar with — GitHub is your platform of choice.","tags":"blog","url":"./blog/2022/01/29/review-review/","loc":"./blog/2022/01/29/review-review/"}]};