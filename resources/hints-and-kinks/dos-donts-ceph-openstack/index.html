
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index,follow" />


    <link rel="stylesheet" type="text/css" href="../../../theme/stylesheet/style.min.css">

    <link id="dark-theme-style" rel="stylesheet" type="text/css"
          media="(prefers-color-scheme: dark)"
    href="../../../theme/stylesheet/dark-theme.min.css">

    <link id="pygments-dark-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: dark)"
          href="../../../theme/pygments/monokai.min.css">
    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"
          href="../../../theme/pygments/github.min.css">

    <link rel="stylesheet" 
    href="../../../theme/tipuesearch/tipuesearch.min.css" />


  <link rel="stylesheet" type="text/css" href="../../../theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="../../../theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="../../../theme/font-awesome/css/solid.css">

  <link rel="stylesheet" type="text/css" href="/css/override.css">



  <link href="https://xahteiwi.eu/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="xahteiwi.eu Atom">

  <link href="https://xahteiwi.eu/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="xahteiwi.eu RSS">


<script defer data-domain="xahteiwi.eu" src="https://plausible.io/js/plausible.js"></script>

 

<meta name="author" content="Florian Haas" />
<meta name="description" content="Ceph and OpenStack are an extremely useful and highly popular combination. Still, new Ceph/OpenStack deployments frequently come with easily avoided shortcomings — we’ll help you fix them!" />
<meta name="keywords" content="OpenStack, Ceph">


  <meta property="og:site_name" content="xahteiwi.eu"/>
  <meta property="og:title" content="The Dos and Don&#39;ts for Ceph for OpenStack"/>
  <meta property="og:description" content="Ceph and OpenStack are an extremely useful and highly popular combination. Still, new Ceph/OpenStack deployments frequently come with easily avoided shortcomings — we’ll help you fix them!"/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="../../../resources/hints-and-kinks/dos-donts-ceph-openstack/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2016-11-28 00:00:00+00:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="../../../author/florian-haas.html">
  <meta property="article:section" content="hints-and-kinks"/>
  <meta property="article:tag" content="OpenStack"/>
  <meta property="article:tag" content="Ceph"/>
  <meta property="og:image" content="https://xahteiwi.eu/images/avatar.jpg">

  <title>xahteiwi.eu &ndash; The Dos and Don&#39;ts for Ceph for OpenStack</title>


</head>
<body >

<aside>
  <div>
    <a href="../../../">
      <img src="https://xahteiwi.eu/images/avatar.jpg" alt="" title="">
    </a>

    <h1>
      <a href="../../../"></a>
    </h1>


    <form class="navbar-search" action="../../../search.html" role="search">
      <input type="text" name="q" id="tipue_search_input" placeholder="Search...">
    </form>


    <ul class="social">
      <li>
        <a class="sc-mastodon"
rel="me"           href="https://mastodon.social/@xahteiwi"
           target="_blank">
          <i class="fa-brands fa-mastodon"></i>
        </a>
      </li>
      <li>
        <a class="sc-linkedin"
           href="https://www.linkedin.com/in/fghaas"
           target="_blank">
          <i class="fa-brands fa-linkedin"></i>
        </a>
      </li>
      <li>
        <a class="sc-github"
           href="https://github.com/fghaas"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>

<nav>
  <a href="../../../">Home</a>

  <a href="/category/hints-and-kinks.html">Tech</a>
  <a href="/category/presentations.html">Talks</a>
  <a href="/category/blog.html">Thoughts</a>
  <a href="">​</a>
  <a href="/about/">About</a>
  <a href="/comments/">Comments</a>
  <a href="/legal/">Legal</a>
  <a href="/privacy/">Privacy</a>
  <a href="">​</a>
  <a href="/categories.html">Categories</a>
  <a href="/tags.html">Tags</a>
  <a href="">​</a>

  <a href="https://xahteiwi.eu/feeds/all.atom.xml">Atom</a>

  <a href="https://xahteiwi.eu/feeds/all.rss.xml">RSS</a>
</nav>

<article class="single">
  <header>
      
    <h1 id="dos-donts-ceph-openstack">The Dos and Don'ts for Ceph for OpenStack</h1>
    <p>
      Posted on Mon 28 November 2016 in <a href="../../../category/hints-and-kinks.html">hints-and-kinks</a>

        &#8226; 9 min read
    </p>
  </header>


  <div>
    <p>Ceph and OpenStack are an extremely useful and
<a href="https://www.openstack.org/assets/survey/April-2016-User-Survey-Report.pdf">highly popular</a>
combination. Still, new Ceph/OpenStack deployments frequently come
with easily avoided shortcomings — we’ll help you fix them!</p>

<h2>Do use <code>show_image_direct_url</code> and the Glance v2 API</h2>
<p>With Ceph RBD (RADOS Block Device), you have the ability to create
<strong>clones.</strong> You can think of clones as the writable siblings of
<em>snapshots</em> (which are read-only). A clone creates RADOS objects only
for those parts of your block device which have been modified relative
to its parent snapshot, and this means two things:</p>
<ol>
<li>
<p>You save space. That’s a no-brainer, but in and of itself it’s not
   a very compelling argument as storage space is one of the cheapest
   things in a distributed system.</p>
</li>
<li>
<p>What’s <em>not</em> been modified in the clone can be served from the
   original volume. This is important because, of course, it means you
   are effectively hitting the same RADOS objects — and thus, the
   same OSDs — no matter which clone you’re talking to. And that, in
   turn, means, those objects are likely to be served from the
   respective OSD’s page caches, in other words, from RAM. RAM is way
   faster to access than any persistent storage device, so being able
   to serve lots of reads from the page cache is good. That, in turn,
   means, that serving data from a clone will be faster than serving
   the same data from a full copy of a volume.</p>
</li>
</ol>
<p>Both Cinder (when creating a volume from an image) and Nova (when
serving ephemeral disks from Ceph) will make use of cloning RBD images
in the Ceph backend, and will do so automatically. But they will do so
only if <code>show_image_direct_url=true</code> is set in <code>glance‑api.conf</code>, and
they are configured to connect to Glance using the Glance v2
API. <a href="http://docs.ceph.com/docs/jewel/rbd/rbd-openstack/#any-openstack-version">So do both.</a></p>
<h2>Do set <code>libvirt/images_type = rbd</code> on Nova compute nodes</h2>
<p>In Nova (using the libvirt compute driver with KVM), you have several
options of storing ephemeral disk images, that is, storage for any VM
that is <em>not</em> booted from a Cinder volume. You do so by setting the
<code>images_type</code> option in the <code>[libvirt]</code> section in
<code>nova‑compute.conf</code>:</p>
<div class="highlight"><pre><span></span><code><span class="k">[libvirt]</span>
<span class="na">images_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&lt;type&gt;</span>
</code></pre></div>
<p>The default type is <code>disk</code>, which means that when you fire up a new
VM, the following events occur:</p>
<ul>
<li><code>nova‑compute</code> on your hypervisor node connects to the Glance API,
  looks up the desired image, and downloads the image to your compute
  node (into the <code>/var/lib/nova/instances/_base</code> directory by
  default).</li>
<li>It then creates a new qcow2 file which uses the downloaded image as
  its backing file.</li>
</ul>
<p>This process uses up a fair amount of space on your compute nodes,
and can quite seriously delay spawning a new VM if it has been
scheduled to a host that hasn’t downloaded the desired image
before. It also makes it impossible for such a VM to be live-migrated
to another host without downtime.</p>
<p>Flipping <code>images_type</code> to <code>rbd</code> means the disk lives in the RBD
backend, as an RBD clone of the original image, and can be created
instantaneously. No delay on boot, no wasting space, all the benefits
of
clones. <a href="http://docs.ceph.com/docs/jewel/rbd/rbd-openstack/#id2">Use it.</a></p>
<h2>Do enable RBD caching on Nova compute nodes</h2>
<p><code>librbd</code>, the library that underpins the Qemu/KVM RBD storage driver,
can enable a disk cache that uses the hypervisor host’s RAM for
caching purposes. You should use this.</p>
<p>Yes, it’s a cache that is safe to use. On the one hand, the
combination of <code>virtio-blk</code> with the Qemu RBD storage driver <strong>will</strong>
properly honor disk flushes. That is to say, when an application
inside your VM says “I want this data on disk now,” then <code>virtio‑blk</code>,
Qemu, and Ceph will all work together to only report the write as
complete when it has been</p>
<ul>
<li>written to the primary OSD,</li>
<li>replicated to the available replica OSDs,</li>
<li>acknowledged to have hit at least the persistent journal on all OSDs.</li>
</ul>
<p>In addition, Ceph RBD has an intelligent safeguard in place: even if
it is configured to cache in write-back mode, <em>it will refuse to do
so</em> (meaning, it will operate in write-through mode) until it has
received the first flush request from its user. Thus, if you run a VM
that just never does that — because it has been misconfigured or its
guest OS is just ages old — then RBD will stubbornly refuse to cache
any writes. The corresponding RBD option is called
<a href="http://docs.ceph.com/docs/jewel/rbd/rbd-config-ref/#cache-settings"><code>rbd cache writethrough until flush</code></a>,
it defaults to <code>true</code> and you should never disable it.</p>
<p>You can enable writeback caching for Ceph by setting the following
<code>nova-compute</code> configuration option:</p>
<div class="highlight"><pre><span></span><code><span class="k">[libvirt]</span>
<span class="na">images_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">rbd</span>
<span class="na">...</span>
<span class="na">disk_cachemodes</span><span class="o">=</span><span class="s">"network=writeback"</span>
</code></pre></div>
<p>And you just should.</p>
<h2>Do use separate pools for images, volumes, and ephemeral disks</h2>
<p>Now that you have enabled <code>show_image_direct_url=true</code> in Glance,
configured Cinder and <code>nova-compute</code> to talk to Glance using the v2
API, and configured <code>nova-compute</code> with <code>libvirt/images_type=rbd</code>, all
your VMs and volumes will be using RBD clones. Clones can span
multiple RADOS pools, meaning you can have an RBD image (and its
snapshots) in one pool, and its clones in another.</p>
<p>You should do exactly that, for several reasons:</p>
<ol>
<li>Separate pools means you can lock down access to those pools
   separately. This is just a standard threat mitigation approach: if
   your <code>nova-compute</code> node gets compromised and the attacker can
   corrupt or delete ephemeral disks, then that’s bad — but it would
   be <em>worse</em> if they could also corrupt your Glance images.</li>
<li>Separate pools also means that you can have different pool
   settings, such as the settings for <code>size</code> or <code>pg_num</code>.</li>
<li>Most importantly, separate pools can use separate <code>crush_ruleset</code>
   settings. We’ll get back to this in a second, it’ll come in handy
   shortly.</li>
</ol>
<p>It’s common to have three different pools: one for your Glance images
(usually named <code>glance</code> or <code>images</code>), one for your Cinder volumes
(<code>cinder</code> or <code>volumes</code>), and one for your VMs (<code>nova-compute</code> or
<code>vms</code>).</p>
<h2>Don’t necessarily use SSDs for your Ceph OSD journals</h2>
<p>Of the recommendations in this article, this one will probably be the
one that surprises the most people. Of course, conventional wisdom
holds that you should <em>always</em> put your OSD journals on fast OSDs, and
that you should deploy SSDs and spinners in a 1:4 to 1:6 ratio, right?</p>
<p>Let’s take a look. Suppose you’re following the 1:6 approach, and your
SATA spinners are capable of writing at 100 MB/s. 6 spinners make 6
OSDs, and each OSD uses a journal device that’s on a partition on an
enterprise SSD. Suppose further that the SSD is capable of writing at
500 MB/s.</p>
<p>Congratulations, in that scenario you’ve just made your SSD your
bottleneck. While you would be able to hit your OSDs at 600 MB/s on
aggregate, your SSD limits you to about 83% of that.</p>
<p>In that scenario you <em>would</em> actually be fine with a 1:4 ratio, but
make your spindles just a little faster and the SSD advantage goes out
the window again.</p>
<p>Now, of course, do consider the alternative: if you’re putting your
journals on the same drive as your OSD filestores, then you
effectively get only half the nominal bandwidth of your drive, on
average, because you write everything twice, to the same device. So
that means that <em>without</em> SSDs, your effective spinner bandwidth is
only about 50 MB/s, so the <em>total</em> bandwidth you get out of 6 drives
that way is more like 300 MB/s, against which 500 MB/s is still a
substantial improvement.</p>
<p>So you will need to plug your own numbers into this, and make your own
evaluation for price <em>and</em> performance. Just don’t assume that journal
SSD will be a panacea, or that it’s always a good idea to use them.</p>
<h2>Do create all-flash OSDs</h2>
<p>One thing your journal SSDs don’t help with are reads. So, what can you
do to take advantage of SSDs on reads, too?</p>
<p>Make them OSDs. That is, not OSD <em>journals,</em> but actual OSDs with a
filestore <em>and</em> journal. What this will create are OSDs that don’t
just write fast, but read fast, too.</p>
<h2>Do put your all-flash OSDs into a separate CRUSH root</h2>
<p>Assuming you don’t run on all-flash hardware, but operate a
cost-effective mixed cluster where some OSDs are spinners and others
are SSDs (or NVMe devices or whatever), you obviously want to treat
those OSDs separately. The simplest and easiest way to do that is to
create a separate CRUSH <code>root</code> in addition to the normally configured
<code>default</code> root.</p>
<p>For example, you could set up your CRUSH hierarchy as follows:</p>
<div class="highlight"><pre><span></span><code>ID WEIGHT  TYPE NAME         UP/DOWN REWEIGHT PRIMARY-AFFINITY
- 
-1 4.85994 root default
-2 1.61998     host elk
 0 0.53999         osd.0          up  1.00000          1.00000 
 1 0.53999         osd.1          up  1.00000          1.00000 
 2 0.53999         osd.2          up  1.00000          1.00000 
-3 1.61998     host moose
 3 0.53999         osd.3          up  1.00000          1.00000 
 4 0.53999         osd.4          up  1.00000          1.00000 
 5 0.53999         osd.5          up  1.00000          1.00000 
-4 1.61998     host reindeer
 6 0.53999         osd.6          up  1.00000          1.00000 
 7 0.53999         osd.7          up  1.00000          1.00000 
 8 0.53999         osd.8          up  1.00000          1.00000
-5 4.85994 root highperf
-6 1.61998     host elk-ssd
 9 0.53999         osd.9          up  1.00000          1.00000 
10 0.53999         osd.10         up  1.00000          1.00000 
11 0.53999         osd.11         up  1.00000          1.00000 
-7 1.61998     host moose-ssd
12 0.53999         osd.12         up  1.00000          1.00000 
13 0.53999         osd.13         up  1.00000          1.00000 
14 0.53999         osd.14         up  1.00000          1.00000 
-8 1.61998     host reindeer-ssd
15 0.53999         osd.15         up  1.00000          1.00000 
16 0.53999         osd.16         up  1.00000          1.00000 
17 0.53999         osd.17         up  1.00000          1.00000
</code></pre></div>
<p>In the example above, OSDs 0-8 are assigned to the <code>default</code> root,
whereas OSDs 9-17 (our SSDs) belong to the root <code>highperf</code>. We can now
create two separate CRUSH rulesets:</p>
<div class="highlight"><pre><span></span><code>rule replicated_ruleset {
    ruleset 0
    type replicated
    min_size 1
    max_size 10
    step take default
    step chooseleaf firstn 0 type host
    step emit
}

rule highperf_ruleset {
    ruleset 1
    type replicated
    min_size 1
    max_size 10
    step take highperf
    step chooseleaf firstn 0 type host
    step emit
}
</code></pre></div>
<p>The default ruleset, <code>replicated_ruleset</code>, picks OSDs from the
<code>default</code> root, whereas <code>step take highperf</code> in <code>highperf_ruleset</code>
means it covers only OSDs in the <code>highperf</code> root.</p>
<h2>Do assign individual pools to your all-flash ruleset</h2>
<p>Assigning individual pools to a new CRUSH ruleset (and hence, to a
whole different set of OSDs) is a matter of issuing a single command:</p>
<div class="highlight"><pre><span></span><code>ceph osd pool set &lt;name&gt; crush_ruleset &lt;number&gt;
</code></pre></div>
<p>… where <code>&lt;name&gt;</code> name of your pool and <code>&lt;number&gt;</code> is the numerical
ID of your ruleset as per your CRUSH map. You can do this while the
pool is online, and while clients are accessing its data — although
of course, there will be a lot of remapping and backfilling so your
overall performance may be affected somewhat.</p>
<p>Now, the assumption is that you will have more spinner storage than
SSD storage. Thus, you will want to select individual pools for your
all-flash OSDs. Here are a handful of pools that might come in handy
as first candidates to migrate to all-flash. You can interpret the
list below as a priority list: as you add more SSD capacity to your
cluster, you can move pools over to all-flash storage one by one.</p>
<ol>
<li>Nova ephemeral RBD pools (<code>vms</code>, <code>nova-compute</code>)</li>
<li>radosgw bucket indexes (<code>.rgw.buckets.index</code> and friends)
   — if you’re using radosgw as your drop-in OpenStack Swift
   replacement</li>
<li>Cinder volume pools (<code>cinder</code>, <code>volumes</code>)</li>
<li>radosgw data pools (<code>.rgw.buckets</code> and friends) — if you need
   low-latency reads and writes on Swift storage</li>
<li>Glance image pools (<code>glance</code>, <code>images</code>)</li>
<li>Cinder backup pools (<code>cinder-backup</code>) — usually the last pool to
   convert to all-flash OSDs.</li>
</ol>
<h2>Do designate some non-Ceph compute hosts with low-latency local storage</h2>
<p>Now, there will undoubtedly be some applications where Ceph does not
produce the latency you desire. Or, for that matter, <em>any</em>
network-based storage. That’s just a direct consequence of recent
developments in storage and network technology.</p>
<p>Just a few years ago, the average latency of a single-sector uncached
write to a block device was on the order of a millisecond, or 1,000
microseconds (µs). In contrast, the latency incurred on a TCP packet
carrying a 512-byte (1-sector) payload was about 50 µs, which makes
for a 100-µs round trip. All in all, the <em>additional</em> latency incurred
from writing to a device over the network, as opposed to locally, was
approximately 10%.</p>
<p>In the interim, a single-sector write for a device of the same price
is itself about 100 µs, tops, with some reasonably-priced devices down
to about 40 µs. Network latency, in contrast, hasn’t changed all that
much — going down about 20% from Gigabit Ethernet to 10 GbE.</p>
<p>So even going to a single, un-replicated SSD device over the network
will now be 40 + 80 = 120 µs latency, vs. just 40 µs locally. That’s
not a 10% overhead anymore, that’s a whopping <em>factor</em> of 3.</p>
<p>With Ceph, that gets worse. Ceph writes data multiple times, first to
the primary OSD, then (in parallel) to all replicas. So in contrast to
a single-sector write at 40 µs, we now incur a latency of at least two
writes, <em>plus</em> two network round-trips, to that’s 40 x 2 + 80 x 2 =
240 µs, <em>six times</em> the local write latency.</p>
<p>The good news is, <em>most</em> applications don’t care about this sort of
latency overhead, because they’re not latency-critical at all. The bad
news is, <em>some</em> will.</p>
<p>So, should you ditch Ceph because of that? Nope. But do consider
adding a handful of compute nodes that are <em>not</em> configured with
<code>libvirt/images_type=rbd</code>, but that use local disk images instead. Roll
those hosts into a
<a href="http://docs.openstack.org/admin-guide/dashboard-manage-host-aggregates.html">host aggregate,</a>
and map them to a specific flavor. Recommend to your users that they
use that flavor for low-latency applications.</p>
<hr/>
<p>This article originally appeared on the <code>hastexo.com</code> website (now defunct).</p>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="../../../tag/openstack.html">OpenStack</a>
      <a href="../../../tag/ceph.html">Ceph</a>
    </p>
  </div>




  <div class="related-posts">
    <h4>Part 1 of "Best Practices for Ceph and OpenStack"</h4>
       <h5>Next articles</h5>
       <ul>
           <li><a href="../../../resources/hints-and-kinks/importing-rbd-into-glance/">Importing an existing Ceph RBD image into Glance</a></li>
           <li><a href="../../../resources/hints-and-kinks/more-recommendations-ceph-openstack/">More recommendations for Ceph and OpenStack</a></li>
       </ul>
  </div>


</article>

<footer>
<p>
  &copy; 2023  - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike License</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
  <span class="footer-separator">|</span>
  Switch to the <a href="javascript:void(0)" onclick="theme.switch(`dark`)">dark</a> | <a href="javascript:void(0)" onclick="theme.switch(`light`)">light</a> | <a href="javascript:void(0)" onclick="theme.switch(`browser`)">browser</a> theme
  <script id="dark-theme-script"
          src="../../../theme/dark-theme/dark-theme.min.js"
          data-enable-auto-detect-theme="True"
          data-default-theme="light"
          type="text/javascript">
  </script>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " xahteiwi.eu ",
  "url" : "../../..",
  "image": "https://xahteiwi.eu/images/avatar.jpg",
  "description": ""
}
</script>
</body>
</html>