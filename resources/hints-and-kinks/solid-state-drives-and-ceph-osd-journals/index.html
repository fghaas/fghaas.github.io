
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="robots" content="index,follow" />


    <link rel="stylesheet" type="text/css" href="../../../theme/stylesheet/style.min.css">

    <link id="dark-theme-style" rel="stylesheet" type="text/css"
          media="(prefers-color-scheme: dark)"
    href="../../../theme/stylesheet/dark-theme.min.css">

    <link id="pygments-dark-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: dark)"
          href="../../../theme/pygments/monokai.min.css">
    <link id="pygments-light-theme" rel="stylesheet" type="text/css"
              media="(prefers-color-scheme: light), (prefers-color-scheme: no-preference)"
          href="../../../theme/pygments/github.min.css">

    <link rel="stylesheet" 
    href="../../../theme/tipuesearch/tipuesearch.min.css" />


  <link rel="stylesheet" type="text/css" href="../../../theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="../../../theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="../../../theme/font-awesome/css/solid.css">

  <link rel="stylesheet" type="text/css" href="/css/override.css">

  <link rel="shortcut icon" href="https://xahteiwi.eu/favicon.svg" type="image/x-icon">
  <link rel="icon" href="https://xahteiwi.eu/favicon.svg" type="image/x-icon">


  <link href="https://xahteiwi.eu/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="xahteiwi.eu Atom">

  <link href="https://xahteiwi.eu/feeds/all.rss.xml" type="application/rss+xml" rel="alternate" title="xahteiwi.eu RSS">


 

<meta name="author" content="Florian Haas" />
<meta name="description" content="Considerations for running Ceph OSD journals on SSDs." />
<meta name="keywords" content="Ceph, Performance">


  <meta property="og:site_name" content="xahteiwi.eu"/>
  <meta property="og:title" content="Solid-state drives and Ceph OSD journals"/>
  <meta property="og:description" content="Considerations for running Ceph OSD journals on SSDs."/>
  <meta property="og:locale" content="en_US"/>
  <meta property="og:url" content="../../../resources/hints-and-kinks/solid-state-drives-and-ceph-osd-journals/"/>
  <meta property="og:type" content="article"/>
  <meta property="article:published_time" content="2013-01-13 20:33:58+01:00"/>
  <meta property="article:modified_time" content=""/>
  <meta property="article:author" content="../../../author/florian-haas.html">
  <meta property="article:section" content="hints-and-kinks"/>
  <meta property="article:tag" content="Ceph"/>
  <meta property="article:tag" content="Performance"/>
  <meta property="og:image" content="https://xahteiwi.eu/images/avatar.jpg">

  <title>xahteiwi.eu &ndash; Solid-state drives and Ceph OSD journals</title>


</head>
<body >

<aside>
  <div>
    <a href="../../../">
      <img src="https://xahteiwi.eu/images/avatar.jpg" alt="" title="">
    </a>

    <h1>
      <a href="../../../"></a>
    </h1>


    <form class="navbar-search" action="../../../search.html" role="search">
      <input type="text" name="q" id="tipue_search_input" placeholder="Search...">
    </form>


    <ul class="social">
      <li>
        <a class="sc-mastodon"
rel="me"           href="https://mastodon.social/@xahteiwi"
           target="_blank">
          <i class="fa-brands fa-mastodon"></i>
        </a>
      </li>
      <li>
        <a class="sc-mastodon"
rel="me"           href="https://fedifreu.de/@xahteiwi"
           target="_blank">
          <i class="fa-brands fa-mastodon"></i>
        </a>
      </li>
      <li>
        <a class="sc-linkedin"
           href="https://www.linkedin.com/in/fghaas"
           target="_blank">
          <i class="fa-brands fa-linkedin"></i>
        </a>
      </li>
      <li>
        <a class="sc-xing"
           href="https://www.xing.com/profile/Florian_Haas2/"
           target="_blank">
          <i class="fa-brands fa-xing"></i>
        </a>
      </li>
      <li>
        <a class="sc-github"
           href="https://github.com/fghaas"
           target="_blank">
          <i class="fa-brands fa-github"></i>
        </a>
      </li>
      <li>
        <a class="sc-rss"
           href="/feeds/all.rss.xml"
           target="_blank">
          <i class="fa-solid fa-rss"></i>
        </a>
      </li>
      <li>
        <a class="sc-envelope"
rel="me"           href="mailto:xahteiwi+ifyoudontremovethisyoullgostraighttodevnull@mailbox.org"
           target="_blank">
          <i class="fa-solid fa-envelope"></i>
        </a>
      </li>
    </ul>
  </div>

</aside>
  <main>

<nav>
  <a href="../../../">Home</a>

  <a href="/category/hints-and-kinks.html">Tech</a>
  <a href="/category/presentations.html">Talks</a>
  <a href="/category/blog.html">Thoughts</a>
  <a href="">​</a>
  <a href="/about/">About</a>
  <a href="/comments/">Comments</a>
  <a href="/legal/">Legal</a>
  <a href="/privacy/">Privacy</a>
  <a href="">​</a>
  <a href="/categories.html">Categories</a>
  <a href="/tags.html">Tags</a>
  <a href="">​</a>

  <a href="https://xahteiwi.eu/feeds/all.atom.xml">Atom</a>

  <a href="https://xahteiwi.eu/feeds/all.rss.xml">RSS</a>
</nav>

<article class="single">
  <header>
      
    <h1 id="solid-state-drives-and-ceph-osd-journals">Solid-state drives and Ceph OSD journals</h1>
    <p>
      Posted on Sun 13 January 2013 in <a href="../../../category/hints-and-kinks.html">hints-and-kinks</a>

        &#8226; 4 min read
    </p>
  </header>


  <div>
    <p>Object Storage Daemons
(<a href="http://ceph.com/docs/master/man/8/ceph-osd/">OSDs</a>) are the Ceph
stack’s workhorses for data storage. They’re significantly smarter
than many of their counterparts in distributed block-storage solutions
(open source or not), and their design is instrumental in securing the
stack’s reliability and scalability.</p>
<p>Among other things, OSDs are responsible for the decentralized
replication — which is highly configurable — of objects in the
store. They do so in a primary-copy fashion: every Ceph object (more
precisely, the Placement Group it is a part of) is written to the
primary OSD first, and from there replicates to one or several replica
OSDs to ensure redundancy. This replication is synchronous, such that
a new or updated object guarantees its availability (in the way
configured by the cluster administrator) before an application is
notified that the write has completed.</p>
<p>More specifically, in order for an OSD to acknowledge a write as
completed, the new object must have been written to the OSD’s
journal. OSDs use a write-ahead mode for local operations: a write
hits the journal first, and from there is then being copied into the
backing filestore. (Note: if your filestore is using btrfs, the
journal is applied in parallel with the filestore write instead. Btrfs
still being experimental, however, this is not a configuration often
used in production.) Thus, for best cluster performance it is crucial
that the journal is fast, whereas the filestore can be comparatively
slow.</p>
<p>This, in turn, leads to a common design principle for Ceph clusters
that are both fast and cost-effective:</p>
<ul>
<li>Put your filestores on slow, cheap drives (such as SATA spinners),</li>
<li>put your journals on fast drives (SSDs, Fusion-IO cards, whatever
  you can afford).</li>
</ul>
<p>Another common design principle is that you create one OSD per
spinning disk that you have in the system. Many contemporary systems
come with only two SSD slots, and then as many spinners as you
want. That is not a problem for journal capacity — a single OSD’s
journal is usually no larger than about 6 GB, so even for a 16-spinner
system (approx. 96GB journal space) appropriate SSDs are available at
reasonable expense.</p>
<p>Many operators are scared of an SSD suddenly dying a horrible death,
so they put their SSDs in a RAID-1. Many are also tempted to put their
OSD journal partitions onto the same RAID. Another option is to use,
say, one partition on each of your SSD in a RAID for the operating
system installation, and then chop up the rest of your SSDs as
non-RAIDed Ceph OSD journals.</p>
<p>This creates an interesting situation when you get to more than about
10-or-so OSDs (the exact number is hard to give). Now you have your OS
and several OSD journals on the same physical SSD. SSDs are much
faster than spinners, but they have neither infinite throughput nor
zero latency. Eventually, you might hit your SSD’s physical limits for
random I/O all over the place. For example, if one of your hosts dies
and the rest now reshuffles data to restore the desired level of
redundancy, you may see relatively intensive I/O all over the other
OSDs — this is exacerbated in a system where you have few OSD hosts
which host many OSD disks.</p>
<p>Putting your journal SSDs in a RAID set looks like a good idea at
first. Specifically, Ceph OSDs currently cannot recover from a broken
SSD journal without reinitializing and recovering the entire
filestore. This means that as soon as SSD acting as journal backing
storage burns up, you’ve effectively lost those OSDs completely and
need to recover them from scratch.<sup id="fnref:mkjournal"><a class="footnote-ref" href="#fn:mkjournal">1</a></sup></p>
<p>Put them in a RAID-1, problem solved?  Well, not quite, because you’ve
now duplicated all of your journal writes and you’re hitting two SSDs
all over the place. Thus it’s generally a much better idea to put half
of your journals on one SSD, and half on the other. If one of your
SSDs burns up you’ll still lose the OSDs whose journals it hosts — but
it’ll only be half of the OSDs hosted on that node altogether.</p>
<p>Any such performance issues get worse if some of your OSDs are also
MONs: your OSD journals now compete with your operating system and
your MONs for I/O on the same SSDs. Once your SSDs get hit so hard
that your MONs can’t do I/O, those MONs eventually die. This might not
harm your operations if you have sufficient backup MONs available, and
everything will be fine again once your recovery is complete, but it’s
still a nuisance. This is remarkably common specifically in POCs, by
the way, where people often try to repurpose three of their old,
two-SSDs-plus-dozens-of-disks storage servers for a 3-node Ceph
cluster.</p>
<p>So, as you are considering your OSD journal and filestore layout, take
note of the following general guidelines:</p>
<ul>
<li>
<p>By and large, try to go for a relatively small number of OSDs per
  node, ideally not more than 8. This combined with SSD journals is
  likely to give you the best overall performance.</p>
</li>
<li>
<p>If you do go with OSD nodes with a very high number of disks,
  consider dropping the idea of an SSD-based journal. Yes, in this
  kind of setup you might actually do better with journals on the
  spinners.</p>
</li>
<li>
<p>Alternatively in the same scenario, consider putting your operating
  system install on one or a couple of the spinners (presumably
  smaller ones than the others), and use the (un-RAIDed) SSDs for OSD
  journals exclusively.</p>
</li>
<li>
<p>Consider having a few dedicated MONs (MONs that are not also OSDs).</p>
</li>
</ul>
<h2>Note on <code>ceph-osd --mkjournal</code></h2>
<hr/>
<p>This article originally appeared on the <code>hastexo.com</code> website (now defunct).</p>
<div class="footnote">
<hr/>
<ol>
<li id="fn:mkjournal">
<p>Since this article was originally published, a <code>--mkjournal</code>
option was added to the <code>ceph-osd</code> command, allowing you to
recreate a journal for an existing OSD. This mitigates the issue
in that you don’t need to recreate OSDs from scratch when a
journal device breaks — but the OSDs will still be <strong>temporarily</strong>
unavailable. <a class="footnote-backref" href="#fnref:mkjournal" title="Jump back to footnote 1 in the text">↩</a></p>
</li>
</ol>
</div>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="../../../tag/ceph.html">Ceph</a>
      <a href="../../../tag/performance.html">Performance</a>
    </p>
  </div>






</article>

<footer>
<p>
  &copy; 2025  - This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/deed.en_US" target="_blank">Creative Commons Attribution-ShareAlike License</a>
</p>
<p>
Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
  <span class="footer-separator">|</span>
  Switch to the <a href="javascript:void(0)" onclick="theme.switch(`dark`)">dark</a> | <a href="javascript:void(0)" onclick="theme.switch(`light`)">light</a> | <a href="javascript:void(0)" onclick="theme.switch(`browser`)">browser</a> theme
  <script id="dark-theme-script"
          src="../../../theme/dark-theme/dark-theme.min.js"
          data-enable-auto-detect-theme="True"
          data-default-theme="light"
          type="text/javascript">
  </script>
</p><p>
  <a rel="license"
     href="http://creativecommons.org/licenses/by-sa/4.0/"
     target="_blank">
    <img alt="Creative Commons License"
         title="Creative Commons License"
         style="border-width:0"
           src="https://i.creativecommons.org/l/by-sa/4.0/80x15.png"
         width="80"
         height="15"/>
  </a>
</p></footer>  </main>

<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " xahteiwi.eu ",
  "url" : "../../..",
  "image": "https://xahteiwi.eu/images/avatar.jpg",
  "description": ""
}
</script>
</body>
</html>